{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import gc\n",
    "import glob\n",
    "import shutil\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "from collections import Counter, OrderedDict\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras.layers import *\n",
    "from keras.models import *\n",
    "from keras.initializers import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import *\n",
    "from keras.regularizers import *\n",
    "from keras import backend as K\n",
    "from keras.legacy import interfaces\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers.merge import _Merge\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.generic_utils import serialize_keras_object\n",
    "from keras.utils.generic_utils import deserialize_keras_object\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, StratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(2018)\n",
    "tf.set_random_seed(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "2001765d86ada3a9eef3c505c0cbb63b67aba737"
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Initializing from file failed",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-ef256426e833>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../input/test.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train shape: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3.6/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Initializing from file failed"
     ]
    }
   ],
   "source": [
    "# Whether to remove punctuation\n",
    "drop = False\n",
    "max_features = 90000\n",
    "maxlen = 72\n",
    "cv = True\n",
    "\n",
    "train = pd.read_csv(\"../input/train.csv\")\n",
    "test = pd.read_csv(\"../input/test.csv\")\n",
    "print(\"Train shape: \", train.shape)\n",
    "print(\"Test shape: \", test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "bc227092c8d646aef4955983af765b8cc3337e59"
   },
   "outputs": [],
   "source": [
    "# Update by find_contractions analysis\n",
    "contractions = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\",\n",
    "                \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",\n",
    "                \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\",\n",
    "                \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\",\n",
    "                \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n",
    "                \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \"i'll've\": \"i will have\",\n",
    "                \"i'm\": \"i am\", \"i've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\",\n",
    "                \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\",\n",
    "                \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\",\n",
    "                \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n",
    "                \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\n",
    "                \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\",\n",
    "                \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\",\n",
    "                \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\",\n",
    "                \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\",\n",
    "                \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\",\n",
    "                \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n",
    "                \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\",\n",
    "                \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\",\n",
    "                \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n",
    "                \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\",\n",
    "                \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\",\n",
    "                \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \"what's\": \"what is\",\n",
    "                \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\",\n",
    "                \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\",\n",
    "                \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\",\n",
    "                \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\",\n",
    "                \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n",
    "                \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\",\n",
    "                \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "\n",
    "# Update by miswords analysis\n",
    "mispell = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling',\n",
    "           'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor',\n",
    "           'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "           'qoura': 'quora', 'sallary': 'salary', 'whta': 'what', 'narcisist': 'narcissist',\n",
    "           'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much',\n",
    "           'howmany': 'how many', 'whydo': 'why do', 'doI': 'do i', 'theBest': 'the best', 'howdoes': 'how does',\n",
    "           'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "           'pennis': 'penis', 'etherium': 'ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data',\n",
    "           '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend',\n",
    "           'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "           'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "def sep_punc(x):\n",
    "    # ’标点符号‘变为‘ 标点符号 ’\n",
    "    x = str(x)\n",
    "    for p in puncs:\n",
    "        x = x.replace(p, f' {p} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    \"\"\"\n",
    "    hmm why is \"##\" in there? Simply because as a reprocessing all numbers bigger \n",
    "    tha 9 have been replaced by hashs. I.e. 15 becomes ## while 123 becomes ### or \n",
    "    15.80€ becomes ##.##€. So lets mimic this preprocessing step to further improve our embeddings coverage\n",
    "    \"\"\"\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c1ccaf665f6fbc23e59b4066ef9748a657f9358"
   },
   "outputs": [],
   "source": [
    "tmp = []\n",
    "for x in tqdm(train.question_text):\n",
    "    for c in x:\n",
    "        if not c.isalnum():\n",
    "            tmp.append(c)\n",
    "for x in tqdm(test.question_text):\n",
    "    for c in x:\n",
    "        if not c.isalnum():\n",
    "            tmp.append(c)\n",
    "puncs = set(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b817ca389a641d48238575ad37059a26ead64ac"
   },
   "outputs": [],
   "source": [
    "train[\"question_text\"] = train[\"question_text\"].str.lower()\n",
    "test[\"question_text\"] = test[\"question_text\"].str.lower()\n",
    "print(\"Lower done\")\n",
    "\n",
    "# 两个字典并集\n",
    "mispell_dict = dict(set(contractions.items()) | set(mispell.items()))\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: replace_typical_misspell(x))\n",
    "print(\"Clean speelings done\")\n",
    "\n",
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: sep_punc(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: sep_punc(x))\n",
    "print(\"Sep punc done\")\n",
    "\n",
    "train[\"question_text\"] = train[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "test[\"question_text\"] = test[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "print(\"Clean numbers done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a82dc4ed4c13e3e81f1e7220355468dac4fe1f38"
   },
   "outputs": [],
   "source": [
    "train_X = train['question_text']\n",
    "# filters注意设置，因为标点符号被保留\n",
    "tokenizer = Tokenizer(num_words=max_features, filters='')\n",
    "tokenizer.fit_on_texts(train_X)\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "\n",
    "test_X = test['question_text']\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "train_y = train['target'].values\n",
    "# 通过word_index与Embedding实现一一对应\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff99e492925e6ae4c4a3873406dd0067d5d7617c"
   },
   "outputs": [],
   "source": [
    "glove_path = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "para_path = \"../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt\"\n",
    "fasttext_path = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "def load_embed(max_features, word_index,embed_path,embed_type):\n",
    "    if embed_type == 'para':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embed_path, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "    elif embed_type == 'glove':\n",
    "        embeddings_index = dict(get_coefs(*v.split(\" \")) for v in open(embed_path))\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(embed_path) if len(o)>100)\n",
    "        \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea4c7bb278106ec6e0e9d818c36f242189dd8b47"
   },
   "outputs": [],
   "source": [
    "embed_glove = load_embed(max_features, word_index, glove_path,'glove')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2a8742c1a7cb11d28d3fff26e8677cdafc689f20"
   },
   "outputs": [],
   "source": [
    "embed_para = load_embed(max_features, word_index, para_path,'para')\n",
    "glove_para = np.mean([embed_glove,embed_para], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ef04dfe3dbfec1adbab7211cd3a125f9cf6f67c3"
   },
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    def __init__(self, step_dim,\n",
    "                 W_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        self.W = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "        else:\n",
    "            self.b = None\n",
    "        self.built = True\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "        eij = K.reshape(K.dot(K.reshape(x, (-1, features_dim)),\n",
    "                        K.reshape(self.W, (features_dim, 1))), (-1, step_dim))\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "        eij = K.tanh(eij)\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        if mask is not None:\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0],  self.features_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_kg_hide-output": false,
    "_uuid": "fb4ca537138231fbb623563c8f80b2526316e4bc"
   },
   "outputs": [],
   "source": [
    "class LstmAtn():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "        x = SpatialDropout1D(0.5)(x_emb)\n",
    "        x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "        y = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "\n",
    "        atn_1 = Attention(maxlen)(x)\n",
    "        atn_2 = Attention(maxlen)(y)\n",
    "        avg_pool = GlobalAveragePooling1D()(y)\n",
    "        max_pool = GlobalMaxPooling1D()(y)\n",
    "        x = concatenate([atn_1, atn_2, avg_pool, max_pool])\n",
    "#         x = BatchNormalization()(x)\n",
    "        x = Dense(16, activation=\"relu\")(x)\n",
    "        x = Dropout(0.1)(x)\n",
    "#         x = BatchNormalization()(x)\n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "    \n",
    "def ConvBlock(inp,filters):\n",
    "    x = Conv1D(filters, 3)(inp)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Activation('relu')(x)\n",
    "\n",
    "    x = Conv1D(filters, 3)(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    outp = Activation('relu')(x)\n",
    "    return outp\n",
    "\n",
    "class VDNN():\n",
    "    def model(self, embedding_matrix, maxlen, max_features):\n",
    "        inp = Input(shape=(maxlen,))\n",
    "        emb_size = embedding_matrix.shape[1]\n",
    "        x_emb = Embedding(max_features, emb_size, weights=[embedding_matrix], trainable=True)(inp)\n",
    "        \n",
    "        x = Conv1D(64, 3, activation='relu')(x_emb)\n",
    "        x = ConvBlock(x, 256)\n",
    "        \n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = ConvBlock(x, 256)\n",
    "        \n",
    "        x = MaxPooling1D(2)(x)\n",
    "        x = ConvBlock(x, 256)\n",
    "        \n",
    "        x = Dense(512)(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        \n",
    "        output = Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs=inp, outputs=output)\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "        return model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44d993d4684065dc14041a1491737cebc5e43403"
   },
   "outputs": [],
   "source": [
    "# def train_offline():\n",
    "#     model = Model().BiGRUAtt(embed_glove, maxlen, max_features)\n",
    "# filepath = \"BiGRUAtt.h5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_loss', save_best_only=True, mode='min')\n",
    "keras_f1 = Metrics()\n",
    "# clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n",
    "#                     step_size=2000., mode='exp_range',\n",
    "#                     gamma=0.99994)\n",
    "callbacks = [keras_f1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7699e1dffd9e07729acfad9b450a8f1a0c78a312"
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2,random_state=2018, stratify=train_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b10f61d9872214256eadc0ed38ef0ce0ca60f64"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4c3546a58870ef22f7312abb2adff093a0d69d6b"
   },
   "outputs": [],
   "source": [
    "model = VDNN().model(embed_glove, maxlen, max_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "412c36e0ae63a8ba0e53b282492a97a4cec1edf6"
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, batch_size=512, epochs=5, validation_data=(X_val, y_val),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a2e6aa38ad3398f0b085c89845b8ed7fdd726ea9"
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=512, epochs=5, validation_data=(X_val, y_val),)\n",
    "pred_glove_val_y = model.predict([X_val], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fac2ac98b0517c861075c175e4b9dbabbb75bc83"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "def train_single():\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_X, train_y, test_size=0.2,random_state=2018, stratify=train_y)\n",
    "    model = VDNN().model(embed_glove, maxlen, max_features)\n",
    "    history = model.fit(X_train, y_train, batch_size=512, epochs=5, validation_data=(X_val, y_val),)\n",
    "    pred_glove_val_y = model.predict([X_val], batch_size=1024, verbose=1)\n",
    "    best_threshold = 0\n",
    "    best_score = 0\n",
    "\n",
    "    for threshold in [i * 0.01 for i in range(25,50)]:\n",
    "        score = metrics.f1_score(y_val, (pred_glove_val_y>threshold).astype(int))\n",
    "        if score > best_score:\n",
    "            best_threshold = threshold\n",
    "            best_score = score\n",
    "    print('best score:%f,best threshold:%f'%(best_score, best_threshold))\n",
    "    pred_test_y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "    pred_test_y = (pred_test_y > best_threshold).astype(int)\n",
    "    gc.collect()\n",
    "    K.clear_session()\n",
    "    tf.reset_default_graph()\n",
    "    return pred_test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4b1c75fd164996c18a9e64668153452e9bdf2f39"
   },
   "outputs": [],
   "source": [
    "pred_test_y = train_single()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b0b67e8a3054a4ba9c2cece6e57f6b31b76fa58"
   },
   "outputs": [],
   "source": [
    "def Submit(pred_test_y):\n",
    "    submit = pd.read_csv(\"../input/sample_submission.csv\")\n",
    "    submit['prediction'] = pred_test_y\n",
    "    submit.to_csv('submission.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7ce0a059ca8bddd554360851fddc351a118611d1"
   },
   "outputs": [],
   "source": [
    "KFOLD = 5\n",
    "SEED = 2018\n",
    "KFolds = list(StratifiedKFold(n_splits=KFOLD, shuffle=True, random_state=SEED).split(train_X, train_y))\n",
    "train_meta = np.zeros((train_y.shape[0],1))\n",
    "test_meta = np.zeros((test_X.shape[0],1))\n",
    "for i, (train_idx, valid_idx) in enumerate(KFolds):\n",
    "    X_train, X_val, y_train, y_val = train_X[train_idx], train_X[valid_idx], train_y[train_idx], train_y[valid_idx]\n",
    "    model = LstmAtn().model(embed_glove, maxlen, max_features)\n",
    "    history = model.fit(X_train, y_train, batch_size=1024, epochs=2, validation_data=(X_val, y_val),)\n",
    "    pred_val_y = model.predict([X_val], batch_size=1024, verbose=1)\n",
    "    pred_test_y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "\n",
    "    train_meta[valid_idx] = pred_val_y\n",
    "    test_meta += pred_test_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3cec90c830ebecab287635a19acf387d301551c3"
   },
   "outputs": [],
   "source": [
    "pred_val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "470cf5a4fd57e54bbd80a7ac7219caf407e7e301"
   },
   "outputs": [],
   "source": [
    "test_meta /= KFOLD\n",
    "best_score,best_threshold = find_threshold(train_y, train_meta)\n",
    "pred_test_y = (pred_test_y > best_threshold).astype(int)\n",
    "Submit(pred_test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47dcdd1fb6dadbc1d3ec487cb224ef2418517a82"
   },
   "outputs": [],
   "source": [
    "def train_kfold(kfold):\n",
    "    KFolds = list(StratifiedKFold(n_splits=kfold, shuffle=True, random_state=SEED).split(train_X, train_y))\n",
    "    train_meta = np.zeros(train_y.shape)\n",
    "    test_meta = np.zeros(test_X.shape[0])\n",
    "    for i, (train_idx, valid_idx) in enumerate(KFolds):\n",
    "        X_train, X_val, y_train, y_val = train_X[train_idx], train_X[valid_idx], train_y[train_idx], train_y[valid_idx]\n",
    "        model = LstmAtn().model(embed_glove, maxlen, max_features)\n",
    "        history = model.fit(X_train, y_train, batch_size=512, epochs=5, validation_data=(X_val, y_val),)\n",
    "        pred_val_y = model.predict([X_val], batch_size=1024, verbose=1)\n",
    "        pred_test_y = model.predict(test_X, batch_size=1024, verbose=1)\n",
    "        \n",
    "        train_meta[valid_idx] = pred_val_y\n",
    "        test_meta += pred_test_y\n",
    "    test_meat /= kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4ab8da621a50765fbe3528fab4d4c874b9943335"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
