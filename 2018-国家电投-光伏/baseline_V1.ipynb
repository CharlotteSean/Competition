{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,KFold\n",
    "from sklearn.preprocessing import StandardScaler,PolynomialFeatures\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.linear_model import Ridge,RidgeCV,LinearRegression,Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.utils import shuffle\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from lightgbm import LGBMRegressor\n",
    "import math\n",
    "%matplotlib inline\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "# from keras.layers import Dense, Dropout, Activation\n",
    "from keras.optimizers import SGD\n",
    "from keras import optimizers\n",
    "\n",
    "pd.set_option('display.max_colwidth',1000)\n",
    "pd.set_option('display.height',1000)\n",
    "pd.set_option('display.max_rows',500)\n",
    "pd.set_option('display.max_columns',500)\n",
    "pd.set_option('display.width',1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成数据\n",
    "def generate_train_data(train_data, test_data, poly=False, select=False):\n",
    "    y = train_data['发电量']\n",
    "    X = train_data.drop(['发电量','ID'], axis=1)\n",
    "    # 去除ID后的test_data\n",
    "    sub_data = test_data.drop(['ID'], axis=1)\n",
    "    \n",
    "    polynm = None\n",
    "    if poly:\n",
    "\n",
    "        polynm = PolynomialFeatures(degree=2, interaction_only=False)\n",
    "        X = polynm.fit_transform(X)\n",
    "        sub_data = polynm.transform(sub_data)\n",
    "        \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
    "    \n",
    "    sm = None\n",
    "    if select:\n",
    "        sm = SelectFromModel(GradientBoostingRegressor(random_state=2))\n",
    "        X_train = sm.fit_transform(X_train, y_train)\n",
    "        X_test = sm.transform(X_test)\n",
    "        sub_data = sm.transform(sub_data)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test, sub_data, sm, polynm\n",
    "\n",
    "def cal_score(mse):\n",
    "    if isinstance(mse, float):\n",
    "        return 1 / (1 + math.sqrt(mse))\n",
    "    else:\n",
    "        return np.divide(1, 1 + np.sqrt(mse))\n",
    "\n",
    "#  定义交叉验证函数\n",
    "def cross_validation_test(models, train_X_data, train_y_data, cv=5):\n",
    "    model_name, mse_avg, score_avg = [], [], []\n",
    "    for i, model in enumerate(models):\n",
    "        print(i + 1,'- Model:', str(model).split('(')[0])\n",
    "        model_name.append(str(i + 1) + '.' + str(model).split('(')[0])\n",
    "        nmse = cross_val_score(model, train_X_data[i], train_y_data[i], cv=cv, scoring='neg_mean_squared_error')\n",
    "        avg_mse = np.average(-nmse)\n",
    "        scores = cal_score(-nmse)\n",
    "        avg_score = np.average(scores)\n",
    "        mse_avg.append(avg_mse)\n",
    "        score_avg.append(avg_score)\n",
    "        print('MSE:', -nmse)\n",
    "        print('Score:', scores)\n",
    "        print('Average XGB - MSE:', avg_mse, ' - Score:', avg_score, '\\n')\n",
    "    res = pd.DataFrame()\n",
    "    res['Model'] = model_name\n",
    "    res['Avg MSE'] = mse_avg\n",
    "    res['Avg Score'] = score_avg\n",
    "    return res\n",
    "\n",
    "def add_avg(df):\n",
    "    array = np.array(df[\"平均功率\"])\n",
    "    newarray=[]\n",
    "    num = 0\n",
    "    len_array = len(array)\n",
    "    for i in np.arange(len_array):\n",
    "        try:\n",
    "            if i<10:\n",
    "                num = (array[i-1]+array[i]+array[i+1])/3\n",
    "            else:\n",
    "                num = (array[i-1]+array[i-2]+array[i-3]+array[i-4]+array[i]+array[i+1]+array[i+2]+array[i+3]+array[i+4])/9\n",
    "        except:\n",
    "            num = (array[i-1]+array[i-2]+array[i-3]+array[i-4]+array[i]+array[i+1-len_array]+array[i+2-len_array]+array[i+3-len_array]+array[i+4-len_array])/9\n",
    "        newarray.append(num)\n",
    "    df[\"old平均功率\"] = newarray\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "submit = pd.DataFrame()\n",
    "submit['ID'] = list(test_data['ID'])\n",
    "submit['发电量'] = 0\n",
    "# 缺失值数据\n",
    "special_missing_ID = test_data[test_data[(test_data == 0) | (test_data == 0.)].count(axis=1) > 13]['ID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([train_data, test_data], axis=0).sort_values(by='ID').reset_index().drop(['index'], axis=1)\n",
    "bad_feature = ['功率A', '功率B', '功率C', '平均功率', '现场温度', '电压A', '电压B', '电压C', '电流B', '电流C', '转换效率', '转换效率A', '转换效率B', '转换效率C']\n",
    "\n",
    "bad_index1 = all_data[bad_feature][\n",
    "    (all_data[bad_feature] > all_data[bad_feature].mean() + 2 * all_data[bad_feature].std()) | \n",
    "    (all_data[bad_feature] < all_data[bad_feature].mean() - 2 * all_data[bad_feature].std())\n",
    "].dropna(how='all').index\n",
    "\n",
    "bad_index2 = all_data[\n",
    "    ((all_data['电压A']<500)&(all_data['电压A']!=0))|\n",
    "    ((all_data['电压B']<500)&(all_data['电压B']!=0))|\n",
    "    ((all_data['电压C']<500)&(all_data['电压C']!=0))].index\n",
    "bad_index = pd.Int64Index(list(bad_index1)+list(bad_index2))\n",
    "# all_data.loc[np.concatenate([bad_index -1,bad_index,bad_index+1])].sort_values(by='ID', ascending=True)\n",
    "\n",
    "# 坏样本相邻的index同样提取出来\n",
    "nn_bad_data = all_data.loc[np.concatenate([bad_index - 1, bad_index, bad_index + 1])].sort_values(by='ID', ascending=True).drop_duplicates()\n",
    "bad_data = all_data.loc[bad_index].sort_values(by='ID', ascending=True).drop_duplicates()\n",
    "# 上下记录均值替代异常值\n",
    "for idx, line in bad_data.iterrows():\n",
    "    ID = line['ID']\n",
    "    col_index = line[bad_feature][ \n",
    "        (line[bad_feature] > all_data[bad_feature].mean() + 3 * all_data[bad_feature].std())| \n",
    "        (line[bad_feature] < all_data[bad_feature].mean() - 3 * all_data[bad_feature].std())\n",
    "    ].index\n",
    "    index = all_data[all_data['ID'] == ID].index\n",
    "    \n",
    "    before_offset = 1\n",
    "    while (idx + before_offset)in bad_index:\n",
    "        before_offset += 1\n",
    "\n",
    "    after_offset = 1\n",
    "    while (idx + after_offset) in bad_index:\n",
    "        after_offset += 1\n",
    "     \n",
    "    replace_value = (all_data.loc[index - before_offset, col_index].values + all_data.loc[index + after_offset, col_index].values) / (before_offset+after_offset)\n",
    "    all_data.loc[index, col_index] = replace_value[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#拆分数据\n",
    "train_data = all_data.drop(all_data[all_data['ID'].isin(submit['ID'])].index).reset_index().drop(['index'], axis=1)\n",
    "test_data = all_data[all_data['ID'].isin(submit['ID'])].drop(['发电量'], axis=1).reset_index().drop(['index'], axis=1)\n",
    "len(train_data), len(test_data)\n",
    "# 去除重复值\n",
    "train_data = train_data.drop_duplicates(train_data.columns.drop('ID'), keep='first')\n",
    "train_data = add_avg(train_data)\n",
    "test_data = add_avg(test_data)\n",
    "\n",
    "X_train, X_test, y_train, y_test, sub_data, sm, polynm = generate_train_data(train_data, test_data, poly=True, select=False)\n",
    "all_X_train = np.concatenate([X_train, X_test])\n",
    "all_y_train = np.concatenate([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_cv():\n",
    "    rmse_score = 0\n",
    "    cvFold = 10\n",
    "    result = submit.copy()\n",
    "    result['发电量']=0\n",
    "    for i in range(cvFold):\n",
    "        print('第'+str(i)+'次交叉验证')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(all_X_train, all_y_train, test_size=0.2, random_state=i)\n",
    "\n",
    "        lgb_clf = LGBMRegressor(boosting_type='gbdt', num_leaves=31, max_depth=-1,\n",
    "                            learning_rate=0.1, n_estimators=1500, max_bin=225,\n",
    "                            min_child_weight=0.01, min_child_samples=20, subsample=1, subsample_freq=1,\n",
    "                            reg_alpha=0, reg_lambda=0, random_state=100*i+500, n_jobs=-1,\n",
    "                            )\n",
    "\n",
    "        lgb_clf.fit(X_train, y_train, eval_metric='rmse', eval_set=(X_test, y_test), early_stopping_rounds=100)\n",
    "\n",
    "        y_pred = lgb_clf.predict(X_test, num_iteration=lgb_clf.best_iteration_)\n",
    "        score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        y_pred_test = lgb_clf.predict(sub_data, num_iteration=lgb_clf.best_iteration_)\n",
    "        result['发电量'] += y_pred_test\n",
    "\n",
    "        score += 1/(1 + math.sqrt(score))\n",
    "        rmse_score += score\n",
    "\n",
    "        print('测试集 RMSE：', score)\n",
    "\n",
    "    result['发电量'] = result['发电量']/cvFold\n",
    "\n",
    "    print(rmse_score/cvFold)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第0次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.11044\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.80096\n",
      "[3]\tvalid_0's rmse: 2.52284\n",
      "[4]\tvalid_0's rmse: 2.27318\n",
      "[5]\tvalid_0's rmse: 2.04857\n",
      "[6]\tvalid_0's rmse: 1.84622\n",
      "[7]\tvalid_0's rmse: 1.6649\n",
      "[8]\tvalid_0's rmse: 1.50212\n",
      "[9]\tvalid_0's rmse: 1.35639\n",
      "[10]\tvalid_0's rmse: 1.22528\n",
      "[11]\tvalid_0's rmse: 1.10843\n",
      "[12]\tvalid_0's rmse: 1.00317\n",
      "[13]\tvalid_0's rmse: 0.908812\n",
      "[14]\tvalid_0's rmse: 0.824986\n",
      "[15]\tvalid_0's rmse: 0.750439\n",
      "[16]\tvalid_0's rmse: 0.683722\n",
      "[17]\tvalid_0's rmse: 0.624861\n",
      "[18]\tvalid_0's rmse: 0.572448\n",
      "[19]\tvalid_0's rmse: 0.526239\n",
      "[20]\tvalid_0's rmse: 0.485077\n",
      "[21]\tvalid_0's rmse: 0.449179\n",
      "[22]\tvalid_0's rmse: 0.417876\n",
      "[23]\tvalid_0's rmse: 0.390754\n",
      "[24]\tvalid_0's rmse: 0.367199\n",
      "[25]\tvalid_0's rmse: 0.346804\n",
      "[26]\tvalid_0's rmse: 0.329373\n",
      "[27]\tvalid_0's rmse: 0.314367\n",
      "[28]\tvalid_0's rmse: 0.301756\n",
      "[29]\tvalid_0's rmse: 0.291092\n",
      "[30]\tvalid_0's rmse: 0.282064\n",
      "[31]\tvalid_0's rmse: 0.274529\n",
      "[32]\tvalid_0's rmse: 0.268444\n",
      "[33]\tvalid_0's rmse: 0.263244\n",
      "[34]\tvalid_0's rmse: 0.258888\n",
      "[35]\tvalid_0's rmse: 0.255388\n",
      "[36]\tvalid_0's rmse: 0.252511\n",
      "[37]\tvalid_0's rmse: 0.250102\n",
      "[38]\tvalid_0's rmse: 0.247959\n",
      "[39]\tvalid_0's rmse: 0.246378\n",
      "[40]\tvalid_0's rmse: 0.244813\n",
      "[41]\tvalid_0's rmse: 0.243718\n",
      "[42]\tvalid_0's rmse: 0.242652\n",
      "[43]\tvalid_0's rmse: 0.241737\n",
      "[44]\tvalid_0's rmse: 0.24098\n",
      "[45]\tvalid_0's rmse: 0.240412\n",
      "[46]\tvalid_0's rmse: 0.23987\n",
      "[47]\tvalid_0's rmse: 0.2394\n",
      "[48]\tvalid_0's rmse: 0.238986\n",
      "[49]\tvalid_0's rmse: 0.238617\n",
      "[50]\tvalid_0's rmse: 0.238279\n",
      "[51]\tvalid_0's rmse: 0.238\n",
      "[52]\tvalid_0's rmse: 0.23782\n",
      "[53]\tvalid_0's rmse: 0.237576\n",
      "[54]\tvalid_0's rmse: 0.237271\n",
      "[55]\tvalid_0's rmse: 0.236946\n",
      "[56]\tvalid_0's rmse: 0.236674\n",
      "[57]\tvalid_0's rmse: 0.236515\n",
      "[58]\tvalid_0's rmse: 0.236314\n",
      "[59]\tvalid_0's rmse: 0.236193\n",
      "[60]\tvalid_0's rmse: 0.235948\n",
      "[61]\tvalid_0's rmse: 0.235804\n",
      "[62]\tvalid_0's rmse: 0.235671\n",
      "[63]\tvalid_0's rmse: 0.235561\n",
      "[64]\tvalid_0's rmse: 0.235527\n",
      "[65]\tvalid_0's rmse: 0.235384\n",
      "[66]\tvalid_0's rmse: 0.235321\n",
      "[67]\tvalid_0's rmse: 0.235181\n",
      "[68]\tvalid_0's rmse: 0.235106\n",
      "[69]\tvalid_0's rmse: 0.235\n",
      "[70]\tvalid_0's rmse: 0.234906\n",
      "[71]\tvalid_0's rmse: 0.234853\n",
      "[72]\tvalid_0's rmse: 0.234746\n",
      "[73]\tvalid_0's rmse: 0.234675\n",
      "[74]\tvalid_0's rmse: 0.234614\n",
      "[75]\tvalid_0's rmse: 0.234466\n",
      "[76]\tvalid_0's rmse: 0.234415\n",
      "[77]\tvalid_0's rmse: 0.234383\n",
      "[78]\tvalid_0's rmse: 0.234339\n",
      "[79]\tvalid_0's rmse: 0.234277\n",
      "[80]\tvalid_0's rmse: 0.234255\n",
      "[81]\tvalid_0's rmse: 0.234231\n",
      "[82]\tvalid_0's rmse: 0.234158\n",
      "[83]\tvalid_0's rmse: 0.234101\n",
      "[84]\tvalid_0's rmse: 0.234051\n",
      "[85]\tvalid_0's rmse: 0.23403\n",
      "[86]\tvalid_0's rmse: 0.233946\n",
      "[87]\tvalid_0's rmse: 0.233907\n",
      "[88]\tvalid_0's rmse: 0.233817\n",
      "[89]\tvalid_0's rmse: 0.233801\n",
      "[90]\tvalid_0's rmse: 0.233748\n",
      "[91]\tvalid_0's rmse: 0.233753\n",
      "[92]\tvalid_0's rmse: 0.233738\n",
      "[93]\tvalid_0's rmse: 0.233735\n",
      "[94]\tvalid_0's rmse: 0.233681\n",
      "[95]\tvalid_0's rmse: 0.233659\n",
      "[96]\tvalid_0's rmse: 0.233577\n",
      "[97]\tvalid_0's rmse: 0.233504\n",
      "[98]\tvalid_0's rmse: 0.233523\n",
      "[99]\tvalid_0's rmse: 0.233541\n",
      "[100]\tvalid_0's rmse: 0.233459\n",
      "[101]\tvalid_0's rmse: 0.233432\n",
      "[102]\tvalid_0's rmse: 0.233396\n",
      "[103]\tvalid_0's rmse: 0.233388\n",
      "[104]\tvalid_0's rmse: 0.233364\n",
      "[105]\tvalid_0's rmse: 0.233362\n",
      "[106]\tvalid_0's rmse: 0.233337\n",
      "[107]\tvalid_0's rmse: 0.233297\n",
      "[108]\tvalid_0's rmse: 0.233246\n",
      "[109]\tvalid_0's rmse: 0.233221\n",
      "[110]\tvalid_0's rmse: 0.233245\n",
      "[111]\tvalid_0's rmse: 0.233183\n",
      "[112]\tvalid_0's rmse: 0.233149\n",
      "[113]\tvalid_0's rmse: 0.233111\n",
      "[114]\tvalid_0's rmse: 0.233058\n",
      "[115]\tvalid_0's rmse: 0.233076\n",
      "[116]\tvalid_0's rmse: 0.233039\n",
      "[117]\tvalid_0's rmse: 0.233012\n",
      "[118]\tvalid_0's rmse: 0.233024\n",
      "[119]\tvalid_0's rmse: 0.232989\n",
      "[120]\tvalid_0's rmse: 0.232921\n",
      "[121]\tvalid_0's rmse: 0.232903\n",
      "[122]\tvalid_0's rmse: 0.232891\n",
      "[123]\tvalid_0's rmse: 0.232902\n",
      "[124]\tvalid_0's rmse: 0.232838\n",
      "[125]\tvalid_0's rmse: 0.232845\n",
      "[126]\tvalid_0's rmse: 0.232754\n",
      "[127]\tvalid_0's rmse: 0.23274\n",
      "[128]\tvalid_0's rmse: 0.232717\n",
      "[129]\tvalid_0's rmse: 0.232713\n",
      "[130]\tvalid_0's rmse: 0.232694\n",
      "[131]\tvalid_0's rmse: 0.232686\n",
      "[132]\tvalid_0's rmse: 0.232681\n",
      "[133]\tvalid_0's rmse: 0.232666\n",
      "[134]\tvalid_0's rmse: 0.232685\n",
      "[135]\tvalid_0's rmse: 0.232657\n",
      "[136]\tvalid_0's rmse: 0.232632\n",
      "[137]\tvalid_0's rmse: 0.232627\n",
      "[138]\tvalid_0's rmse: 0.232737\n",
      "[139]\tvalid_0's rmse: 0.232706\n",
      "[140]\tvalid_0's rmse: 0.232707\n",
      "[141]\tvalid_0's rmse: 0.232618\n",
      "[142]\tvalid_0's rmse: 0.232631\n",
      "[143]\tvalid_0's rmse: 0.232604\n",
      "[144]\tvalid_0's rmse: 0.232612\n",
      "[145]\tvalid_0's rmse: 0.23256\n",
      "[146]\tvalid_0's rmse: 0.232547\n",
      "[147]\tvalid_0's rmse: 0.232501\n",
      "[148]\tvalid_0's rmse: 0.232482\n",
      "[149]\tvalid_0's rmse: 0.232461\n",
      "[150]\tvalid_0's rmse: 0.232475\n",
      "[151]\tvalid_0's rmse: 0.232466\n",
      "[152]\tvalid_0's rmse: 0.232365\n",
      "[153]\tvalid_0's rmse: 0.232336\n",
      "[154]\tvalid_0's rmse: 0.232327\n",
      "[155]\tvalid_0's rmse: 0.232323\n",
      "[156]\tvalid_0's rmse: 0.232299\n",
      "[157]\tvalid_0's rmse: 0.232306\n",
      "[158]\tvalid_0's rmse: 0.232273\n",
      "[159]\tvalid_0's rmse: 0.232277\n",
      "[160]\tvalid_0's rmse: 0.232256\n",
      "[161]\tvalid_0's rmse: 0.232243\n",
      "[162]\tvalid_0's rmse: 0.232248\n",
      "[163]\tvalid_0's rmse: 0.232226\n",
      "[164]\tvalid_0's rmse: 0.23219\n",
      "[165]\tvalid_0's rmse: 0.232176\n",
      "[166]\tvalid_0's rmse: 0.232169\n",
      "[167]\tvalid_0's rmse: 0.232164\n",
      "[168]\tvalid_0's rmse: 0.232165\n",
      "[169]\tvalid_0's rmse: 0.232159\n",
      "[170]\tvalid_0's rmse: 0.232129\n",
      "[171]\tvalid_0's rmse: 0.232171\n",
      "[172]\tvalid_0's rmse: 0.232147\n",
      "[173]\tvalid_0's rmse: 0.232141\n",
      "[174]\tvalid_0's rmse: 0.232125\n",
      "[175]\tvalid_0's rmse: 0.232109\n",
      "[176]\tvalid_0's rmse: 0.23211\n",
      "[177]\tvalid_0's rmse: 0.232112\n",
      "[178]\tvalid_0's rmse: 0.232095\n",
      "[179]\tvalid_0's rmse: 0.232079\n",
      "[180]\tvalid_0's rmse: 0.232088\n",
      "[181]\tvalid_0's rmse: 0.232116\n",
      "[182]\tvalid_0's rmse: 0.232096\n",
      "[183]\tvalid_0's rmse: 0.232075\n",
      "[184]\tvalid_0's rmse: 0.232061\n",
      "[185]\tvalid_0's rmse: 0.232068\n",
      "[186]\tvalid_0's rmse: 0.232092\n",
      "[187]\tvalid_0's rmse: 0.232047\n",
      "[188]\tvalid_0's rmse: 0.231981\n",
      "[189]\tvalid_0's rmse: 0.231941\n",
      "[190]\tvalid_0's rmse: 0.232007\n",
      "[191]\tvalid_0's rmse: 0.232017\n",
      "[192]\tvalid_0's rmse: 0.232023\n",
      "[193]\tvalid_0's rmse: 0.231997\n",
      "[194]\tvalid_0's rmse: 0.231982\n",
      "[195]\tvalid_0's rmse: 0.231988\n",
      "[196]\tvalid_0's rmse: 0.232007\n",
      "[197]\tvalid_0's rmse: 0.232024\n",
      "[198]\tvalid_0's rmse: 0.232005\n",
      "[199]\tvalid_0's rmse: 0.232008\n",
      "[200]\tvalid_0's rmse: 0.231999\n",
      "[201]\tvalid_0's rmse: 0.231981\n",
      "[202]\tvalid_0's rmse: 0.231969\n",
      "[203]\tvalid_0's rmse: 0.23196\n",
      "[204]\tvalid_0's rmse: 0.231965\n",
      "[205]\tvalid_0's rmse: 0.231929\n",
      "[206]\tvalid_0's rmse: 0.231932\n",
      "[207]\tvalid_0's rmse: 0.231966\n",
      "[208]\tvalid_0's rmse: 0.23198\n",
      "[209]\tvalid_0's rmse: 0.232002\n",
      "[210]\tvalid_0's rmse: 0.232012\n",
      "[211]\tvalid_0's rmse: 0.232031\n",
      "[212]\tvalid_0's rmse: 0.232018\n",
      "[213]\tvalid_0's rmse: 0.232016\n",
      "[214]\tvalid_0's rmse: 0.232032\n",
      "[215]\tvalid_0's rmse: 0.232042\n",
      "[216]\tvalid_0's rmse: 0.232049\n",
      "[217]\tvalid_0's rmse: 0.232073\n",
      "[218]\tvalid_0's rmse: 0.232067\n",
      "[219]\tvalid_0's rmse: 0.23208\n",
      "[220]\tvalid_0's rmse: 0.232104\n",
      "[221]\tvalid_0's rmse: 0.232096\n",
      "[222]\tvalid_0's rmse: 0.23209\n",
      "[223]\tvalid_0's rmse: 0.232088\n",
      "[224]\tvalid_0's rmse: 0.232073\n",
      "[225]\tvalid_0's rmse: 0.232076\n",
      "[226]\tvalid_0's rmse: 0.232094\n",
      "[227]\tvalid_0's rmse: 0.232095\n",
      "[228]\tvalid_0's rmse: 0.23208\n",
      "[229]\tvalid_0's rmse: 0.232071\n",
      "[230]\tvalid_0's rmse: 0.232119\n",
      "[231]\tvalid_0's rmse: 0.232129\n",
      "[232]\tvalid_0's rmse: 0.232128\n",
      "[233]\tvalid_0's rmse: 0.232121\n",
      "[234]\tvalid_0's rmse: 0.232127\n",
      "[235]\tvalid_0's rmse: 0.232133\n",
      "[236]\tvalid_0's rmse: 0.232118\n",
      "[237]\tvalid_0's rmse: 0.232133\n",
      "[238]\tvalid_0's rmse: 0.232122\n",
      "[239]\tvalid_0's rmse: 0.232114\n",
      "[240]\tvalid_0's rmse: 0.232134\n",
      "[241]\tvalid_0's rmse: 0.232152\n",
      "[242]\tvalid_0's rmse: 0.232149\n",
      "[243]\tvalid_0's rmse: 0.232137\n",
      "[244]\tvalid_0's rmse: 0.232158\n",
      "[245]\tvalid_0's rmse: 0.232181\n",
      "[246]\tvalid_0's rmse: 0.232154\n",
      "[247]\tvalid_0's rmse: 0.232155\n",
      "[248]\tvalid_0's rmse: 0.232168\n",
      "[249]\tvalid_0's rmse: 0.232165\n",
      "[250]\tvalid_0's rmse: 0.232159\n",
      "[251]\tvalid_0's rmse: 0.232164\n",
      "[252]\tvalid_0's rmse: 0.232154\n",
      "[253]\tvalid_0's rmse: 0.232144\n",
      "[254]\tvalid_0's rmse: 0.232182\n",
      "[255]\tvalid_0's rmse: 0.232164\n",
      "[256]\tvalid_0's rmse: 0.232155\n",
      "[257]\tvalid_0's rmse: 0.232163\n",
      "[258]\tvalid_0's rmse: 0.232144\n",
      "[259]\tvalid_0's rmse: 0.232152\n",
      "[260]\tvalid_0's rmse: 0.232153\n",
      "[261]\tvalid_0's rmse: 0.232154\n",
      "[262]\tvalid_0's rmse: 0.232157\n",
      "[263]\tvalid_0's rmse: 0.232134\n",
      "[264]\tvalid_0's rmse: 0.232111\n",
      "[265]\tvalid_0's rmse: 0.232115\n",
      "[266]\tvalid_0's rmse: 0.232125\n",
      "[267]\tvalid_0's rmse: 0.232121\n",
      "[268]\tvalid_0's rmse: 0.232119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[269]\tvalid_0's rmse: 0.232137\n",
      "[270]\tvalid_0's rmse: 0.232148\n",
      "[271]\tvalid_0's rmse: 0.232145\n",
      "[272]\tvalid_0's rmse: 0.232153\n",
      "[273]\tvalid_0's rmse: 0.232159\n",
      "[274]\tvalid_0's rmse: 0.232159\n",
      "[275]\tvalid_0's rmse: 0.232129\n",
      "[276]\tvalid_0's rmse: 0.232132\n",
      "[277]\tvalid_0's rmse: 0.232136\n",
      "[278]\tvalid_0's rmse: 0.232141\n",
      "[279]\tvalid_0's rmse: 0.232136\n",
      "[280]\tvalid_0's rmse: 0.232123\n",
      "[281]\tvalid_0's rmse: 0.232124\n",
      "[282]\tvalid_0's rmse: 0.232127\n",
      "[283]\tvalid_0's rmse: 0.232121\n",
      "[284]\tvalid_0's rmse: 0.232118\n",
      "[285]\tvalid_0's rmse: 0.232111\n",
      "[286]\tvalid_0's rmse: 0.232125\n",
      "[287]\tvalid_0's rmse: 0.232116\n",
      "[288]\tvalid_0's rmse: 0.232099\n",
      "[289]\tvalid_0's rmse: 0.23208\n",
      "[290]\tvalid_0's rmse: 0.232058\n",
      "[291]\tvalid_0's rmse: 0.232049\n",
      "[292]\tvalid_0's rmse: 0.23206\n",
      "[293]\tvalid_0's rmse: 0.232037\n",
      "[294]\tvalid_0's rmse: 0.232046\n",
      "[295]\tvalid_0's rmse: 0.232029\n",
      "[296]\tvalid_0's rmse: 0.232031\n",
      "[297]\tvalid_0's rmse: 0.232046\n",
      "[298]\tvalid_0's rmse: 0.232046\n",
      "[299]\tvalid_0's rmse: 0.232042\n",
      "[300]\tvalid_0's rmse: 0.232033\n",
      "[301]\tvalid_0's rmse: 0.232048\n",
      "[302]\tvalid_0's rmse: 0.232047\n",
      "[303]\tvalid_0's rmse: 0.232044\n",
      "[304]\tvalid_0's rmse: 0.232038\n",
      "[305]\tvalid_0's rmse: 0.232032\n",
      "Early stopping, best iteration is:\n",
      "[205]\tvalid_0's rmse: 0.231929\n",
      "测试集 RMSE： 0.8655260768881372\n",
      "第1次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.13882\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.8266\n",
      "[3]\tvalid_0's rmse: 2.54508\n",
      "[4]\tvalid_0's rmse: 2.29245\n",
      "[5]\tvalid_0's rmse: 2.06508\n",
      "[6]\tvalid_0's rmse: 1.86076\n",
      "[7]\tvalid_0's rmse: 1.67757\n",
      "[8]\tvalid_0's rmse: 1.51279\n",
      "[9]\tvalid_0's rmse: 1.36512\n",
      "[10]\tvalid_0's rmse: 1.23156\n",
      "[11]\tvalid_0's rmse: 1.11238\n",
      "[12]\tvalid_0's rmse: 1.0052\n",
      "[13]\tvalid_0's rmse: 0.90945\n",
      "[14]\tvalid_0's rmse: 0.823439\n",
      "[15]\tvalid_0's rmse: 0.746347\n",
      "[16]\tvalid_0's rmse: 0.677789\n",
      "[17]\tvalid_0's rmse: 0.616697\n",
      "[18]\tvalid_0's rmse: 0.562106\n",
      "[19]\tvalid_0's rmse: 0.513609\n",
      "[20]\tvalid_0's rmse: 0.470405\n",
      "[21]\tvalid_0's rmse: 0.4323\n",
      "[22]\tvalid_0's rmse: 0.398946\n",
      "[23]\tvalid_0's rmse: 0.369264\n",
      "[24]\tvalid_0's rmse: 0.343618\n",
      "[25]\tvalid_0's rmse: 0.321358\n",
      "[26]\tvalid_0's rmse: 0.302008\n",
      "[27]\tvalid_0's rmse: 0.285385\n",
      "[28]\tvalid_0's rmse: 0.27121\n",
      "[29]\tvalid_0's rmse: 0.258783\n",
      "[30]\tvalid_0's rmse: 0.248351\n",
      "[31]\tvalid_0's rmse: 0.23954\n",
      "[32]\tvalid_0's rmse: 0.232172\n",
      "[33]\tvalid_0's rmse: 0.226168\n",
      "[34]\tvalid_0's rmse: 0.221005\n",
      "[35]\tvalid_0's rmse: 0.216706\n",
      "[36]\tvalid_0's rmse: 0.213071\n",
      "[37]\tvalid_0's rmse: 0.210095\n",
      "[38]\tvalid_0's rmse: 0.207467\n",
      "[39]\tvalid_0's rmse: 0.205401\n",
      "[40]\tvalid_0's rmse: 0.203818\n",
      "[41]\tvalid_0's rmse: 0.202218\n",
      "[42]\tvalid_0's rmse: 0.200721\n",
      "[43]\tvalid_0's rmse: 0.199831\n",
      "[44]\tvalid_0's rmse: 0.19917\n",
      "[45]\tvalid_0's rmse: 0.198217\n",
      "[46]\tvalid_0's rmse: 0.197634\n",
      "[47]\tvalid_0's rmse: 0.197172\n",
      "[48]\tvalid_0's rmse: 0.196773\n",
      "[49]\tvalid_0's rmse: 0.196555\n",
      "[50]\tvalid_0's rmse: 0.196232\n",
      "[51]\tvalid_0's rmse: 0.195948\n",
      "[52]\tvalid_0's rmse: 0.195907\n",
      "[53]\tvalid_0's rmse: 0.195508\n",
      "[54]\tvalid_0's rmse: 0.19534\n",
      "[55]\tvalid_0's rmse: 0.195156\n",
      "[56]\tvalid_0's rmse: 0.1951\n",
      "[57]\tvalid_0's rmse: 0.194814\n",
      "[58]\tvalid_0's rmse: 0.194736\n",
      "[59]\tvalid_0's rmse: 0.194165\n",
      "[60]\tvalid_0's rmse: 0.194082\n",
      "[61]\tvalid_0's rmse: 0.193716\n",
      "[62]\tvalid_0's rmse: 0.193518\n",
      "[63]\tvalid_0's rmse: 0.193563\n",
      "[64]\tvalid_0's rmse: 0.193514\n",
      "[65]\tvalid_0's rmse: 0.193749\n",
      "[66]\tvalid_0's rmse: 0.193765\n",
      "[67]\tvalid_0's rmse: 0.193915\n",
      "[68]\tvalid_0's rmse: 0.193825\n",
      "[69]\tvalid_0's rmse: 0.193956\n",
      "[70]\tvalid_0's rmse: 0.193855\n",
      "[71]\tvalid_0's rmse: 0.193846\n",
      "[72]\tvalid_0's rmse: 0.193994\n",
      "[73]\tvalid_0's rmse: 0.193937\n",
      "[74]\tvalid_0's rmse: 0.194014\n",
      "[75]\tvalid_0's rmse: 0.19395\n",
      "[76]\tvalid_0's rmse: 0.193931\n",
      "[77]\tvalid_0's rmse: 0.193881\n",
      "[78]\tvalid_0's rmse: 0.193847\n",
      "[79]\tvalid_0's rmse: 0.193811\n",
      "[80]\tvalid_0's rmse: 0.193841\n",
      "[81]\tvalid_0's rmse: 0.193758\n",
      "[82]\tvalid_0's rmse: 0.19361\n",
      "[83]\tvalid_0's rmse: 0.193567\n",
      "[84]\tvalid_0's rmse: 0.193453\n",
      "[85]\tvalid_0's rmse: 0.193312\n",
      "[86]\tvalid_0's rmse: 0.19332\n",
      "[87]\tvalid_0's rmse: 0.193427\n",
      "[88]\tvalid_0's rmse: 0.19334\n",
      "[89]\tvalid_0's rmse: 0.193316\n",
      "[90]\tvalid_0's rmse: 0.193291\n",
      "[91]\tvalid_0's rmse: 0.193237\n",
      "[92]\tvalid_0's rmse: 0.193316\n",
      "[93]\tvalid_0's rmse: 0.193377\n",
      "[94]\tvalid_0's rmse: 0.193304\n",
      "[95]\tvalid_0's rmse: 0.193318\n",
      "[96]\tvalid_0's rmse: 0.193374\n",
      "[97]\tvalid_0's rmse: 0.193382\n",
      "[98]\tvalid_0's rmse: 0.193393\n",
      "[99]\tvalid_0's rmse: 0.193158\n",
      "[100]\tvalid_0's rmse: 0.193157\n",
      "[101]\tvalid_0's rmse: 0.193248\n",
      "[102]\tvalid_0's rmse: 0.193235\n",
      "[103]\tvalid_0's rmse: 0.193199\n",
      "[104]\tvalid_0's rmse: 0.193189\n",
      "[105]\tvalid_0's rmse: 0.193206\n",
      "[106]\tvalid_0's rmse: 0.193205\n",
      "[107]\tvalid_0's rmse: 0.193168\n",
      "[108]\tvalid_0's rmse: 0.193169\n",
      "[109]\tvalid_0's rmse: 0.193062\n",
      "[110]\tvalid_0's rmse: 0.192944\n",
      "[111]\tvalid_0's rmse: 0.192892\n",
      "[112]\tvalid_0's rmse: 0.192643\n",
      "[113]\tvalid_0's rmse: 0.192599\n",
      "[114]\tvalid_0's rmse: 0.192632\n",
      "[115]\tvalid_0's rmse: 0.192613\n",
      "[116]\tvalid_0's rmse: 0.192489\n",
      "[117]\tvalid_0's rmse: 0.192352\n",
      "[118]\tvalid_0's rmse: 0.192334\n",
      "[119]\tvalid_0's rmse: 0.192325\n",
      "[120]\tvalid_0's rmse: 0.19234\n",
      "[121]\tvalid_0's rmse: 0.192311\n",
      "[122]\tvalid_0's rmse: 0.192335\n",
      "[123]\tvalid_0's rmse: 0.192309\n",
      "[124]\tvalid_0's rmse: 0.192273\n",
      "[125]\tvalid_0's rmse: 0.192247\n",
      "[126]\tvalid_0's rmse: 0.192189\n",
      "[127]\tvalid_0's rmse: 0.192199\n",
      "[128]\tvalid_0's rmse: 0.1921\n",
      "[129]\tvalid_0's rmse: 0.192107\n",
      "[130]\tvalid_0's rmse: 0.192088\n",
      "[131]\tvalid_0's rmse: 0.192026\n",
      "[132]\tvalid_0's rmse: 0.191989\n",
      "[133]\tvalid_0's rmse: 0.192057\n",
      "[134]\tvalid_0's rmse: 0.19208\n",
      "[135]\tvalid_0's rmse: 0.192091\n",
      "[136]\tvalid_0's rmse: 0.192062\n",
      "[137]\tvalid_0's rmse: 0.192081\n",
      "[138]\tvalid_0's rmse: 0.192119\n",
      "[139]\tvalid_0's rmse: 0.192183\n",
      "[140]\tvalid_0's rmse: 0.192113\n",
      "[141]\tvalid_0's rmse: 0.192101\n",
      "[142]\tvalid_0's rmse: 0.192061\n",
      "[143]\tvalid_0's rmse: 0.192056\n",
      "[144]\tvalid_0's rmse: 0.19192\n",
      "[145]\tvalid_0's rmse: 0.191924\n",
      "[146]\tvalid_0's rmse: 0.191981\n",
      "[147]\tvalid_0's rmse: 0.192031\n",
      "[148]\tvalid_0's rmse: 0.192005\n",
      "[149]\tvalid_0's rmse: 0.192031\n",
      "[150]\tvalid_0's rmse: 0.192042\n",
      "[151]\tvalid_0's rmse: 0.192088\n",
      "[152]\tvalid_0's rmse: 0.192089\n",
      "[153]\tvalid_0's rmse: 0.192005\n",
      "[154]\tvalid_0's rmse: 0.192118\n",
      "[155]\tvalid_0's rmse: 0.192155\n",
      "[156]\tvalid_0's rmse: 0.19209\n",
      "[157]\tvalid_0's rmse: 0.192135\n",
      "[158]\tvalid_0's rmse: 0.192064\n",
      "[159]\tvalid_0's rmse: 0.192063\n",
      "[160]\tvalid_0's rmse: 0.192051\n",
      "[161]\tvalid_0's rmse: 0.19204\n",
      "[162]\tvalid_0's rmse: 0.19207\n",
      "[163]\tvalid_0's rmse: 0.192083\n",
      "[164]\tvalid_0's rmse: 0.192074\n",
      "[165]\tvalid_0's rmse: 0.192178\n",
      "[166]\tvalid_0's rmse: 0.192162\n",
      "[167]\tvalid_0's rmse: 0.192187\n",
      "[168]\tvalid_0's rmse: 0.192082\n",
      "[169]\tvalid_0's rmse: 0.192155\n",
      "[170]\tvalid_0's rmse: 0.192197\n",
      "[171]\tvalid_0's rmse: 0.192233\n",
      "[172]\tvalid_0's rmse: 0.192231\n",
      "[173]\tvalid_0's rmse: 0.192229\n",
      "[174]\tvalid_0's rmse: 0.1922\n",
      "[175]\tvalid_0's rmse: 0.192235\n",
      "[176]\tvalid_0's rmse: 0.192246\n",
      "[177]\tvalid_0's rmse: 0.192291\n",
      "[178]\tvalid_0's rmse: 0.192301\n",
      "[179]\tvalid_0's rmse: 0.192337\n",
      "[180]\tvalid_0's rmse: 0.19234\n",
      "[181]\tvalid_0's rmse: 0.192245\n",
      "[182]\tvalid_0's rmse: 0.192209\n",
      "[183]\tvalid_0's rmse: 0.192263\n",
      "[184]\tvalid_0's rmse: 0.192228\n",
      "[185]\tvalid_0's rmse: 0.192152\n",
      "[186]\tvalid_0's rmse: 0.192208\n",
      "[187]\tvalid_0's rmse: 0.192199\n",
      "[188]\tvalid_0's rmse: 0.192173\n",
      "[189]\tvalid_0's rmse: 0.192139\n",
      "[190]\tvalid_0's rmse: 0.19211\n",
      "[191]\tvalid_0's rmse: 0.192123\n",
      "[192]\tvalid_0's rmse: 0.192034\n",
      "[193]\tvalid_0's rmse: 0.192068\n",
      "[194]\tvalid_0's rmse: 0.19207\n",
      "[195]\tvalid_0's rmse: 0.192016\n",
      "[196]\tvalid_0's rmse: 0.192016\n",
      "[197]\tvalid_0's rmse: 0.191983\n",
      "[198]\tvalid_0's rmse: 0.191979\n",
      "[199]\tvalid_0's rmse: 0.192006\n",
      "[200]\tvalid_0's rmse: 0.191999\n",
      "[201]\tvalid_0's rmse: 0.192015\n",
      "[202]\tvalid_0's rmse: 0.191992\n",
      "[203]\tvalid_0's rmse: 0.191984\n",
      "[204]\tvalid_0's rmse: 0.191994\n",
      "[205]\tvalid_0's rmse: 0.192001\n",
      "[206]\tvalid_0's rmse: 0.191938\n",
      "[207]\tvalid_0's rmse: 0.191864\n",
      "[208]\tvalid_0's rmse: 0.191851\n",
      "[209]\tvalid_0's rmse: 0.191922\n",
      "[210]\tvalid_0's rmse: 0.191941\n",
      "[211]\tvalid_0's rmse: 0.19196\n",
      "[212]\tvalid_0's rmse: 0.191975\n",
      "[213]\tvalid_0's rmse: 0.191961\n",
      "[214]\tvalid_0's rmse: 0.191876\n",
      "[215]\tvalid_0's rmse: 0.191872\n",
      "[216]\tvalid_0's rmse: 0.1919\n",
      "[217]\tvalid_0's rmse: 0.191914\n",
      "[218]\tvalid_0's rmse: 0.191922\n",
      "[219]\tvalid_0's rmse: 0.191936\n",
      "[220]\tvalid_0's rmse: 0.191964\n",
      "[221]\tvalid_0's rmse: 0.191944\n",
      "[222]\tvalid_0's rmse: 0.191954\n",
      "[223]\tvalid_0's rmse: 0.191913\n",
      "[224]\tvalid_0's rmse: 0.191913\n",
      "[225]\tvalid_0's rmse: 0.191921\n",
      "[226]\tvalid_0's rmse: 0.191912\n",
      "[227]\tvalid_0's rmse: 0.191893\n",
      "[228]\tvalid_0's rmse: 0.19188\n",
      "[229]\tvalid_0's rmse: 0.191903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[230]\tvalid_0's rmse: 0.191874\n",
      "[231]\tvalid_0's rmse: 0.191833\n",
      "[232]\tvalid_0's rmse: 0.191844\n",
      "[233]\tvalid_0's rmse: 0.191852\n",
      "[234]\tvalid_0's rmse: 0.191828\n",
      "[235]\tvalid_0's rmse: 0.191895\n",
      "[236]\tvalid_0's rmse: 0.191861\n",
      "[237]\tvalid_0's rmse: 0.191794\n",
      "[238]\tvalid_0's rmse: 0.191808\n",
      "[239]\tvalid_0's rmse: 0.191808\n",
      "[240]\tvalid_0's rmse: 0.191824\n",
      "[241]\tvalid_0's rmse: 0.191828\n",
      "[242]\tvalid_0's rmse: 0.191831\n",
      "[243]\tvalid_0's rmse: 0.19183\n",
      "[244]\tvalid_0's rmse: 0.191844\n",
      "[245]\tvalid_0's rmse: 0.191871\n",
      "[246]\tvalid_0's rmse: 0.191867\n",
      "[247]\tvalid_0's rmse: 0.191893\n",
      "[248]\tvalid_0's rmse: 0.191865\n",
      "[249]\tvalid_0's rmse: 0.191878\n",
      "[250]\tvalid_0's rmse: 0.191842\n",
      "[251]\tvalid_0's rmse: 0.191831\n",
      "[252]\tvalid_0's rmse: 0.191823\n",
      "[253]\tvalid_0's rmse: 0.191853\n",
      "[254]\tvalid_0's rmse: 0.191815\n",
      "[255]\tvalid_0's rmse: 0.191838\n",
      "[256]\tvalid_0's rmse: 0.191815\n",
      "[257]\tvalid_0's rmse: 0.191837\n",
      "[258]\tvalid_0's rmse: 0.191856\n",
      "[259]\tvalid_0's rmse: 0.191823\n",
      "[260]\tvalid_0's rmse: 0.19184\n",
      "[261]\tvalid_0's rmse: 0.191837\n",
      "[262]\tvalid_0's rmse: 0.191855\n",
      "[263]\tvalid_0's rmse: 0.191855\n",
      "[264]\tvalid_0's rmse: 0.191852\n",
      "[265]\tvalid_0's rmse: 0.191871\n",
      "[266]\tvalid_0's rmse: 0.191862\n",
      "[267]\tvalid_0's rmse: 0.191834\n",
      "[268]\tvalid_0's rmse: 0.191818\n",
      "[269]\tvalid_0's rmse: 0.191799\n",
      "[270]\tvalid_0's rmse: 0.191719\n",
      "[271]\tvalid_0's rmse: 0.191845\n",
      "[272]\tvalid_0's rmse: 0.191842\n",
      "[273]\tvalid_0's rmse: 0.19181\n",
      "[274]\tvalid_0's rmse: 0.191812\n",
      "[275]\tvalid_0's rmse: 0.191831\n",
      "[276]\tvalid_0's rmse: 0.191816\n",
      "[277]\tvalid_0's rmse: 0.191763\n",
      "[278]\tvalid_0's rmse: 0.191751\n",
      "[279]\tvalid_0's rmse: 0.191786\n",
      "[280]\tvalid_0's rmse: 0.191773\n",
      "[281]\tvalid_0's rmse: 0.191782\n",
      "[282]\tvalid_0's rmse: 0.191784\n",
      "[283]\tvalid_0's rmse: 0.191798\n",
      "[284]\tvalid_0's rmse: 0.191802\n",
      "[285]\tvalid_0's rmse: 0.191803\n",
      "[286]\tvalid_0's rmse: 0.191723\n",
      "[287]\tvalid_0's rmse: 0.191754\n",
      "[288]\tvalid_0's rmse: 0.191746\n",
      "[289]\tvalid_0's rmse: 0.191742\n",
      "[290]\tvalid_0's rmse: 0.191702\n",
      "[291]\tvalid_0's rmse: 0.191707\n",
      "[292]\tvalid_0's rmse: 0.191705\n",
      "[293]\tvalid_0's rmse: 0.191719\n",
      "[294]\tvalid_0's rmse: 0.191729\n",
      "[295]\tvalid_0's rmse: 0.191737\n",
      "[296]\tvalid_0's rmse: 0.191671\n",
      "[297]\tvalid_0's rmse: 0.191659\n",
      "[298]\tvalid_0's rmse: 0.191657\n",
      "[299]\tvalid_0's rmse: 0.191653\n",
      "[300]\tvalid_0's rmse: 0.191682\n",
      "[301]\tvalid_0's rmse: 0.191642\n",
      "[302]\tvalid_0's rmse: 0.191645\n",
      "[303]\tvalid_0's rmse: 0.191693\n",
      "[304]\tvalid_0's rmse: 0.191707\n",
      "[305]\tvalid_0's rmse: 0.191749\n",
      "[306]\tvalid_0's rmse: 0.191763\n",
      "[307]\tvalid_0's rmse: 0.191759\n",
      "[308]\tvalid_0's rmse: 0.191677\n",
      "[309]\tvalid_0's rmse: 0.191539\n",
      "[310]\tvalid_0's rmse: 0.19156\n",
      "[311]\tvalid_0's rmse: 0.191528\n",
      "[312]\tvalid_0's rmse: 0.191543\n",
      "[313]\tvalid_0's rmse: 0.191572\n",
      "[314]\tvalid_0's rmse: 0.191629\n",
      "[315]\tvalid_0's rmse: 0.191591\n",
      "[316]\tvalid_0's rmse: 0.191604\n",
      "[317]\tvalid_0's rmse: 0.191589\n",
      "[318]\tvalid_0's rmse: 0.191588\n",
      "[319]\tvalid_0's rmse: 0.191569\n",
      "[320]\tvalid_0's rmse: 0.191574\n",
      "[321]\tvalid_0's rmse: 0.191594\n",
      "[322]\tvalid_0's rmse: 0.191591\n",
      "[323]\tvalid_0's rmse: 0.191633\n",
      "[324]\tvalid_0's rmse: 0.191572\n",
      "[325]\tvalid_0's rmse: 0.191467\n",
      "[326]\tvalid_0's rmse: 0.191485\n",
      "[327]\tvalid_0's rmse: 0.191553\n",
      "[328]\tvalid_0's rmse: 0.191544\n",
      "[329]\tvalid_0's rmse: 0.191544\n",
      "[330]\tvalid_0's rmse: 0.191577\n",
      "[331]\tvalid_0's rmse: 0.191565\n",
      "[332]\tvalid_0's rmse: 0.19156\n",
      "[333]\tvalid_0's rmse: 0.19157\n",
      "[334]\tvalid_0's rmse: 0.191489\n",
      "[335]\tvalid_0's rmse: 0.191587\n",
      "[336]\tvalid_0's rmse: 0.191606\n",
      "[337]\tvalid_0's rmse: 0.191621\n",
      "[338]\tvalid_0's rmse: 0.191623\n",
      "[339]\tvalid_0's rmse: 0.191624\n",
      "[340]\tvalid_0's rmse: 0.19163\n",
      "[341]\tvalid_0's rmse: 0.191659\n",
      "[342]\tvalid_0's rmse: 0.191588\n",
      "[343]\tvalid_0's rmse: 0.191568\n",
      "[344]\tvalid_0's rmse: 0.191608\n",
      "[345]\tvalid_0's rmse: 0.191605\n",
      "[346]\tvalid_0's rmse: 0.191596\n",
      "[347]\tvalid_0's rmse: 0.191604\n",
      "[348]\tvalid_0's rmse: 0.191603\n",
      "[349]\tvalid_0's rmse: 0.191581\n",
      "[350]\tvalid_0's rmse: 0.191576\n",
      "[351]\tvalid_0's rmse: 0.191597\n",
      "[352]\tvalid_0's rmse: 0.191601\n",
      "[353]\tvalid_0's rmse: 0.19162\n",
      "[354]\tvalid_0's rmse: 0.191594\n",
      "[355]\tvalid_0's rmse: 0.191584\n",
      "[356]\tvalid_0's rmse: 0.191598\n",
      "[357]\tvalid_0's rmse: 0.191581\n",
      "[358]\tvalid_0's rmse: 0.191597\n",
      "[359]\tvalid_0's rmse: 0.191578\n",
      "[360]\tvalid_0's rmse: 0.191606\n",
      "[361]\tvalid_0's rmse: 0.1916\n",
      "[362]\tvalid_0's rmse: 0.191595\n",
      "[363]\tvalid_0's rmse: 0.191581\n",
      "[364]\tvalid_0's rmse: 0.1916\n",
      "[365]\tvalid_0's rmse: 0.191589\n",
      "[366]\tvalid_0's rmse: 0.191604\n",
      "[367]\tvalid_0's rmse: 0.191556\n",
      "[368]\tvalid_0's rmse: 0.19148\n",
      "[369]\tvalid_0's rmse: 0.19147\n",
      "[370]\tvalid_0's rmse: 0.191482\n",
      "[371]\tvalid_0's rmse: 0.191428\n",
      "[372]\tvalid_0's rmse: 0.191437\n",
      "[373]\tvalid_0's rmse: 0.191434\n",
      "[374]\tvalid_0's rmse: 0.191477\n",
      "[375]\tvalid_0's rmse: 0.191492\n",
      "[376]\tvalid_0's rmse: 0.191483\n",
      "[377]\tvalid_0's rmse: 0.191499\n",
      "[378]\tvalid_0's rmse: 0.191507\n",
      "[379]\tvalid_0's rmse: 0.191507\n",
      "[380]\tvalid_0's rmse: 0.191508\n",
      "[381]\tvalid_0's rmse: 0.191493\n",
      "[382]\tvalid_0's rmse: 0.191504\n",
      "[383]\tvalid_0's rmse: 0.191516\n",
      "[384]\tvalid_0's rmse: 0.191549\n",
      "[385]\tvalid_0's rmse: 0.191546\n",
      "[386]\tvalid_0's rmse: 0.191539\n",
      "[387]\tvalid_0's rmse: 0.191536\n",
      "[388]\tvalid_0's rmse: 0.191546\n",
      "[389]\tvalid_0's rmse: 0.191535\n",
      "[390]\tvalid_0's rmse: 0.19156\n",
      "[391]\tvalid_0's rmse: 0.191549\n",
      "[392]\tvalid_0's rmse: 0.191563\n",
      "[393]\tvalid_0's rmse: 0.191572\n",
      "[394]\tvalid_0's rmse: 0.191593\n",
      "[395]\tvalid_0's rmse: 0.191587\n",
      "[396]\tvalid_0's rmse: 0.191596\n",
      "[397]\tvalid_0's rmse: 0.191598\n",
      "[398]\tvalid_0's rmse: 0.191603\n",
      "[399]\tvalid_0's rmse: 0.191594\n",
      "[400]\tvalid_0's rmse: 0.191642\n",
      "[401]\tvalid_0's rmse: 0.191645\n",
      "[402]\tvalid_0's rmse: 0.191659\n",
      "[403]\tvalid_0's rmse: 0.191646\n",
      "[404]\tvalid_0's rmse: 0.191662\n",
      "[405]\tvalid_0's rmse: 0.191669\n",
      "[406]\tvalid_0's rmse: 0.191683\n",
      "[407]\tvalid_0's rmse: 0.19169\n",
      "[408]\tvalid_0's rmse: 0.191668\n",
      "[409]\tvalid_0's rmse: 0.191655\n",
      "[410]\tvalid_0's rmse: 0.191605\n",
      "[411]\tvalid_0's rmse: 0.191581\n",
      "[412]\tvalid_0's rmse: 0.191599\n",
      "[413]\tvalid_0's rmse: 0.191587\n",
      "[414]\tvalid_0's rmse: 0.191566\n",
      "[415]\tvalid_0's rmse: 0.191564\n",
      "[416]\tvalid_0's rmse: 0.191564\n",
      "[417]\tvalid_0's rmse: 0.191589\n",
      "[418]\tvalid_0's rmse: 0.191615\n",
      "[419]\tvalid_0's rmse: 0.191611\n",
      "[420]\tvalid_0's rmse: 0.19159\n",
      "[421]\tvalid_0's rmse: 0.191605\n",
      "[422]\tvalid_0's rmse: 0.191593\n",
      "[423]\tvalid_0's rmse: 0.191646\n",
      "[424]\tvalid_0's rmse: 0.191648\n",
      "[425]\tvalid_0's rmse: 0.191651\n",
      "[426]\tvalid_0's rmse: 0.191668\n",
      "[427]\tvalid_0's rmse: 0.191682\n",
      "[428]\tvalid_0's rmse: 0.191688\n",
      "[429]\tvalid_0's rmse: 0.191708\n",
      "[430]\tvalid_0's rmse: 0.191693\n",
      "[431]\tvalid_0's rmse: 0.191688\n",
      "[432]\tvalid_0's rmse: 0.191688\n",
      "[433]\tvalid_0's rmse: 0.191685\n",
      "[434]\tvalid_0's rmse: 0.191681\n",
      "[435]\tvalid_0's rmse: 0.191688\n",
      "[436]\tvalid_0's rmse: 0.191689\n",
      "[437]\tvalid_0's rmse: 0.191684\n",
      "[438]\tvalid_0's rmse: 0.191694\n",
      "[439]\tvalid_0's rmse: 0.19169\n",
      "[440]\tvalid_0's rmse: 0.191688\n",
      "[441]\tvalid_0's rmse: 0.191679\n",
      "[442]\tvalid_0's rmse: 0.191675\n",
      "[443]\tvalid_0's rmse: 0.191675\n",
      "[444]\tvalid_0's rmse: 0.19168\n",
      "[445]\tvalid_0's rmse: 0.191683\n",
      "[446]\tvalid_0's rmse: 0.191671\n",
      "[447]\tvalid_0's rmse: 0.191668\n",
      "[448]\tvalid_0's rmse: 0.191652\n",
      "[449]\tvalid_0's rmse: 0.191752\n",
      "[450]\tvalid_0's rmse: 0.191765\n",
      "[451]\tvalid_0's rmse: 0.191781\n",
      "[452]\tvalid_0's rmse: 0.19178\n",
      "[453]\tvalid_0's rmse: 0.191736\n",
      "[454]\tvalid_0's rmse: 0.191733\n",
      "[455]\tvalid_0's rmse: 0.191725\n",
      "[456]\tvalid_0's rmse: 0.191745\n",
      "[457]\tvalid_0's rmse: 0.191758\n",
      "[458]\tvalid_0's rmse: 0.191737\n",
      "[459]\tvalid_0's rmse: 0.19175\n",
      "[460]\tvalid_0's rmse: 0.191781\n",
      "[461]\tvalid_0's rmse: 0.191786\n",
      "[462]\tvalid_0's rmse: 0.191809\n",
      "[463]\tvalid_0's rmse: 0.191801\n",
      "[464]\tvalid_0's rmse: 0.191787\n",
      "[465]\tvalid_0's rmse: 0.191836\n",
      "[466]\tvalid_0's rmse: 0.191865\n",
      "[467]\tvalid_0's rmse: 0.191873\n",
      "[468]\tvalid_0's rmse: 0.191886\n",
      "[469]\tvalid_0's rmse: 0.191879\n",
      "[470]\tvalid_0's rmse: 0.191878\n",
      "[471]\tvalid_0's rmse: 0.191875\n",
      "Early stopping, best iteration is:\n",
      "[371]\tvalid_0's rmse: 0.191428\n",
      "测试集 RMSE： 0.8759737034306679\n",
      "第2次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.17144\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.85653\n",
      "[3]\tvalid_0's rmse: 2.57317\n",
      "[4]\tvalid_0's rmse: 2.31775\n",
      "[5]\tvalid_0's rmse: 2.08836\n",
      "[6]\tvalid_0's rmse: 1.88213\n",
      "[7]\tvalid_0's rmse: 1.69606\n",
      "[8]\tvalid_0's rmse: 1.52869\n",
      "[9]\tvalid_0's rmse: 1.37839\n",
      "[10]\tvalid_0's rmse: 1.24337\n",
      "[11]\tvalid_0's rmse: 1.12184\n",
      "[12]\tvalid_0's rmse: 1.01291\n",
      "[13]\tvalid_0's rmse: 0.914923\n",
      "[14]\tvalid_0's rmse: 0.82735\n",
      "[15]\tvalid_0's rmse: 0.748349\n",
      "[16]\tvalid_0's rmse: 0.677368\n",
      "[17]\tvalid_0's rmse: 0.614171\n",
      "[18]\tvalid_0's rmse: 0.557356\n",
      "[19]\tvalid_0's rmse: 0.506643\n",
      "[20]\tvalid_0's rmse: 0.460979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[21]\tvalid_0's rmse: 0.420183\n",
      "[22]\tvalid_0's rmse: 0.383997\n",
      "[23]\tvalid_0's rmse: 0.351825\n",
      "[24]\tvalid_0's rmse: 0.323332\n",
      "[25]\tvalid_0's rmse: 0.29812\n",
      "[26]\tvalid_0's rmse: 0.275464\n",
      "[27]\tvalid_0's rmse: 0.25561\n",
      "[28]\tvalid_0's rmse: 0.238346\n",
      "[29]\tvalid_0's rmse: 0.223039\n",
      "[30]\tvalid_0's rmse: 0.209537\n",
      "[31]\tvalid_0's rmse: 0.198195\n",
      "[32]\tvalid_0's rmse: 0.188181\n",
      "[33]\tvalid_0's rmse: 0.179489\n",
      "[34]\tvalid_0's rmse: 0.172017\n",
      "[35]\tvalid_0's rmse: 0.165703\n",
      "[36]\tvalid_0's rmse: 0.159873\n",
      "[37]\tvalid_0's rmse: 0.155445\n",
      "[38]\tvalid_0's rmse: 0.151273\n",
      "[39]\tvalid_0's rmse: 0.148285\n",
      "[40]\tvalid_0's rmse: 0.145306\n",
      "[41]\tvalid_0's rmse: 0.143203\n",
      "[42]\tvalid_0's rmse: 0.141204\n",
      "[43]\tvalid_0's rmse: 0.13927\n",
      "[44]\tvalid_0's rmse: 0.137426\n",
      "[45]\tvalid_0's rmse: 0.136271\n",
      "[46]\tvalid_0's rmse: 0.135183\n",
      "[47]\tvalid_0's rmse: 0.134499\n",
      "[48]\tvalid_0's rmse: 0.133498\n",
      "[49]\tvalid_0's rmse: 0.132774\n",
      "[50]\tvalid_0's rmse: 0.132044\n",
      "[51]\tvalid_0's rmse: 0.131452\n",
      "[52]\tvalid_0's rmse: 0.130851\n",
      "[53]\tvalid_0's rmse: 0.130539\n",
      "[54]\tvalid_0's rmse: 0.130113\n",
      "[55]\tvalid_0's rmse: 0.129522\n",
      "[56]\tvalid_0's rmse: 0.129248\n",
      "[57]\tvalid_0's rmse: 0.129113\n",
      "[58]\tvalid_0's rmse: 0.12857\n",
      "[59]\tvalid_0's rmse: 0.128232\n",
      "[60]\tvalid_0's rmse: 0.12784\n",
      "[61]\tvalid_0's rmse: 0.127601\n",
      "[62]\tvalid_0's rmse: 0.127283\n",
      "[63]\tvalid_0's rmse: 0.12682\n",
      "[64]\tvalid_0's rmse: 0.126507\n",
      "[65]\tvalid_0's rmse: 0.126625\n",
      "[66]\tvalid_0's rmse: 0.126308\n",
      "[67]\tvalid_0's rmse: 0.126181\n",
      "[68]\tvalid_0's rmse: 0.125931\n",
      "[69]\tvalid_0's rmse: 0.125801\n",
      "[70]\tvalid_0's rmse: 0.125738\n",
      "[71]\tvalid_0's rmse: 0.125539\n",
      "[72]\tvalid_0's rmse: 0.125456\n",
      "[73]\tvalid_0's rmse: 0.125312\n",
      "[74]\tvalid_0's rmse: 0.125052\n",
      "[75]\tvalid_0's rmse: 0.124819\n",
      "[76]\tvalid_0's rmse: 0.124692\n",
      "[77]\tvalid_0's rmse: 0.124633\n",
      "[78]\tvalid_0's rmse: 0.124469\n",
      "[79]\tvalid_0's rmse: 0.124412\n",
      "[80]\tvalid_0's rmse: 0.124185\n",
      "[81]\tvalid_0's rmse: 0.124271\n",
      "[82]\tvalid_0's rmse: 0.124257\n",
      "[83]\tvalid_0's rmse: 0.124242\n",
      "[84]\tvalid_0's rmse: 0.124266\n",
      "[85]\tvalid_0's rmse: 0.124188\n",
      "[86]\tvalid_0's rmse: 0.124332\n",
      "[87]\tvalid_0's rmse: 0.124297\n",
      "[88]\tvalid_0's rmse: 0.124247\n",
      "[89]\tvalid_0's rmse: 0.124081\n",
      "[90]\tvalid_0's rmse: 0.123744\n",
      "[91]\tvalid_0's rmse: 0.123687\n",
      "[92]\tvalid_0's rmse: 0.123639\n",
      "[93]\tvalid_0's rmse: 0.123793\n",
      "[94]\tvalid_0's rmse: 0.123746\n",
      "[95]\tvalid_0's rmse: 0.123772\n",
      "[96]\tvalid_0's rmse: 0.123753\n",
      "[97]\tvalid_0's rmse: 0.123616\n",
      "[98]\tvalid_0's rmse: 0.123455\n",
      "[99]\tvalid_0's rmse: 0.123382\n",
      "[100]\tvalid_0's rmse: 0.123264\n",
      "[101]\tvalid_0's rmse: 0.123171\n",
      "[102]\tvalid_0's rmse: 0.12307\n",
      "[103]\tvalid_0's rmse: 0.123068\n",
      "[104]\tvalid_0's rmse: 0.122886\n",
      "[105]\tvalid_0's rmse: 0.1229\n",
      "[106]\tvalid_0's rmse: 0.122837\n",
      "[107]\tvalid_0's rmse: 0.12289\n",
      "[108]\tvalid_0's rmse: 0.122743\n",
      "[109]\tvalid_0's rmse: 0.122743\n",
      "[110]\tvalid_0's rmse: 0.122714\n",
      "[111]\tvalid_0's rmse: 0.122699\n",
      "[112]\tvalid_0's rmse: 0.122601\n",
      "[113]\tvalid_0's rmse: 0.122486\n",
      "[114]\tvalid_0's rmse: 0.122501\n",
      "[115]\tvalid_0's rmse: 0.122645\n",
      "[116]\tvalid_0's rmse: 0.122604\n",
      "[117]\tvalid_0's rmse: 0.122654\n",
      "[118]\tvalid_0's rmse: 0.122616\n",
      "[119]\tvalid_0's rmse: 0.122549\n",
      "[120]\tvalid_0's rmse: 0.12243\n",
      "[121]\tvalid_0's rmse: 0.122402\n",
      "[122]\tvalid_0's rmse: 0.122262\n",
      "[123]\tvalid_0's rmse: 0.122195\n",
      "[124]\tvalid_0's rmse: 0.122002\n",
      "[125]\tvalid_0's rmse: 0.121999\n",
      "[126]\tvalid_0's rmse: 0.121923\n",
      "[127]\tvalid_0's rmse: 0.121964\n",
      "[128]\tvalid_0's rmse: 0.121964\n",
      "[129]\tvalid_0's rmse: 0.122096\n",
      "[130]\tvalid_0's rmse: 0.122034\n",
      "[131]\tvalid_0's rmse: 0.121895\n",
      "[132]\tvalid_0's rmse: 0.121937\n",
      "[133]\tvalid_0's rmse: 0.121899\n",
      "[134]\tvalid_0's rmse: 0.121859\n",
      "[135]\tvalid_0's rmse: 0.121814\n",
      "[136]\tvalid_0's rmse: 0.121933\n",
      "[137]\tvalid_0's rmse: 0.121875\n",
      "[138]\tvalid_0's rmse: 0.12181\n",
      "[139]\tvalid_0's rmse: 0.121664\n",
      "[140]\tvalid_0's rmse: 0.121728\n",
      "[141]\tvalid_0's rmse: 0.121683\n",
      "[142]\tvalid_0's rmse: 0.121632\n",
      "[143]\tvalid_0's rmse: 0.121571\n",
      "[144]\tvalid_0's rmse: 0.121638\n",
      "[145]\tvalid_0's rmse: 0.121537\n",
      "[146]\tvalid_0's rmse: 0.121516\n",
      "[147]\tvalid_0's rmse: 0.121497\n",
      "[148]\tvalid_0's rmse: 0.121438\n",
      "[149]\tvalid_0's rmse: 0.121342\n",
      "[150]\tvalid_0's rmse: 0.121333\n",
      "[151]\tvalid_0's rmse: 0.121265\n",
      "[152]\tvalid_0's rmse: 0.121209\n",
      "[153]\tvalid_0's rmse: 0.121157\n",
      "[154]\tvalid_0's rmse: 0.121171\n",
      "[155]\tvalid_0's rmse: 0.121256\n",
      "[156]\tvalid_0's rmse: 0.121284\n",
      "[157]\tvalid_0's rmse: 0.121238\n",
      "[158]\tvalid_0's rmse: 0.121241\n",
      "[159]\tvalid_0's rmse: 0.121198\n",
      "[160]\tvalid_0's rmse: 0.121149\n",
      "[161]\tvalid_0's rmse: 0.121106\n",
      "[162]\tvalid_0's rmse: 0.12103\n",
      "[163]\tvalid_0's rmse: 0.120943\n",
      "[164]\tvalid_0's rmse: 0.120958\n",
      "[165]\tvalid_0's rmse: 0.120883\n",
      "[166]\tvalid_0's rmse: 0.120771\n",
      "[167]\tvalid_0's rmse: 0.120778\n",
      "[168]\tvalid_0's rmse: 0.120804\n",
      "[169]\tvalid_0's rmse: 0.120876\n",
      "[170]\tvalid_0's rmse: 0.120802\n",
      "[171]\tvalid_0's rmse: 0.120846\n",
      "[172]\tvalid_0's rmse: 0.120909\n",
      "[173]\tvalid_0's rmse: 0.120888\n",
      "[174]\tvalid_0's rmse: 0.120868\n",
      "[175]\tvalid_0's rmse: 0.120797\n",
      "[176]\tvalid_0's rmse: 0.120728\n",
      "[177]\tvalid_0's rmse: 0.120735\n",
      "[178]\tvalid_0's rmse: 0.120657\n",
      "[179]\tvalid_0's rmse: 0.120713\n",
      "[180]\tvalid_0's rmse: 0.120696\n",
      "[181]\tvalid_0's rmse: 0.120714\n",
      "[182]\tvalid_0's rmse: 0.120599\n",
      "[183]\tvalid_0's rmse: 0.120615\n",
      "[184]\tvalid_0's rmse: 0.120554\n",
      "[185]\tvalid_0's rmse: 0.120537\n",
      "[186]\tvalid_0's rmse: 0.120509\n",
      "[187]\tvalid_0's rmse: 0.120496\n",
      "[188]\tvalid_0's rmse: 0.120602\n",
      "[189]\tvalid_0's rmse: 0.120506\n",
      "[190]\tvalid_0's rmse: 0.120481\n",
      "[191]\tvalid_0's rmse: 0.120404\n",
      "[192]\tvalid_0's rmse: 0.120336\n",
      "[193]\tvalid_0's rmse: 0.120371\n",
      "[194]\tvalid_0's rmse: 0.120351\n",
      "[195]\tvalid_0's rmse: 0.12039\n",
      "[196]\tvalid_0's rmse: 0.120344\n",
      "[197]\tvalid_0's rmse: 0.120385\n",
      "[198]\tvalid_0's rmse: 0.120461\n",
      "[199]\tvalid_0's rmse: 0.120452\n",
      "[200]\tvalid_0's rmse: 0.120516\n",
      "[201]\tvalid_0's rmse: 0.120563\n",
      "[202]\tvalid_0's rmse: 0.120486\n",
      "[203]\tvalid_0's rmse: 0.120427\n",
      "[204]\tvalid_0's rmse: 0.120372\n",
      "[205]\tvalid_0's rmse: 0.12033\n",
      "[206]\tvalid_0's rmse: 0.12041\n",
      "[207]\tvalid_0's rmse: 0.120358\n",
      "[208]\tvalid_0's rmse: 0.120343\n",
      "[209]\tvalid_0's rmse: 0.120341\n",
      "[210]\tvalid_0's rmse: 0.120353\n",
      "[211]\tvalid_0's rmse: 0.120334\n",
      "[212]\tvalid_0's rmse: 0.120346\n",
      "[213]\tvalid_0's rmse: 0.120291\n",
      "[214]\tvalid_0's rmse: 0.120248\n",
      "[215]\tvalid_0's rmse: 0.120221\n",
      "[216]\tvalid_0's rmse: 0.120286\n",
      "[217]\tvalid_0's rmse: 0.120311\n",
      "[218]\tvalid_0's rmse: 0.120247\n",
      "[219]\tvalid_0's rmse: 0.120281\n",
      "[220]\tvalid_0's rmse: 0.120238\n",
      "[221]\tvalid_0's rmse: 0.120221\n",
      "[222]\tvalid_0's rmse: 0.120253\n",
      "[223]\tvalid_0's rmse: 0.120221\n",
      "[224]\tvalid_0's rmse: 0.120161\n",
      "[225]\tvalid_0's rmse: 0.120135\n",
      "[226]\tvalid_0's rmse: 0.120096\n",
      "[227]\tvalid_0's rmse: 0.120027\n",
      "[228]\tvalid_0's rmse: 0.120059\n",
      "[229]\tvalid_0's rmse: 0.119995\n",
      "[230]\tvalid_0's rmse: 0.119998\n",
      "[231]\tvalid_0's rmse: 0.119924\n",
      "[232]\tvalid_0's rmse: 0.119914\n",
      "[233]\tvalid_0's rmse: 0.1199\n",
      "[234]\tvalid_0's rmse: 0.119916\n",
      "[235]\tvalid_0's rmse: 0.119856\n",
      "[236]\tvalid_0's rmse: 0.119872\n",
      "[237]\tvalid_0's rmse: 0.119841\n",
      "[238]\tvalid_0's rmse: 0.119814\n",
      "[239]\tvalid_0's rmse: 0.119814\n",
      "[240]\tvalid_0's rmse: 0.119765\n",
      "[241]\tvalid_0's rmse: 0.119849\n",
      "[242]\tvalid_0's rmse: 0.119884\n",
      "[243]\tvalid_0's rmse: 0.119931\n",
      "[244]\tvalid_0's rmse: 0.12006\n",
      "[245]\tvalid_0's rmse: 0.120051\n",
      "[246]\tvalid_0's rmse: 0.120071\n",
      "[247]\tvalid_0's rmse: 0.12011\n",
      "[248]\tvalid_0's rmse: 0.120116\n",
      "[249]\tvalid_0's rmse: 0.120103\n",
      "[250]\tvalid_0's rmse: 0.120073\n",
      "[251]\tvalid_0's rmse: 0.120066\n",
      "[252]\tvalid_0's rmse: 0.120042\n",
      "[253]\tvalid_0's rmse: 0.120068\n",
      "[254]\tvalid_0's rmse: 0.120032\n",
      "[255]\tvalid_0's rmse: 0.120025\n",
      "[256]\tvalid_0's rmse: 0.119945\n",
      "[257]\tvalid_0's rmse: 0.119917\n",
      "[258]\tvalid_0's rmse: 0.119981\n",
      "[259]\tvalid_0's rmse: 0.120077\n",
      "[260]\tvalid_0's rmse: 0.120085\n",
      "[261]\tvalid_0's rmse: 0.120074\n",
      "[262]\tvalid_0's rmse: 0.120034\n",
      "[263]\tvalid_0's rmse: 0.120025\n",
      "[264]\tvalid_0's rmse: 0.120016\n",
      "[265]\tvalid_0's rmse: 0.119965\n",
      "[266]\tvalid_0's rmse: 0.119954\n",
      "[267]\tvalid_0's rmse: 0.119899\n",
      "[268]\tvalid_0's rmse: 0.119937\n",
      "[269]\tvalid_0's rmse: 0.119935\n",
      "[270]\tvalid_0's rmse: 0.119977\n",
      "[271]\tvalid_0's rmse: 0.119995\n",
      "[272]\tvalid_0's rmse: 0.119963\n",
      "[273]\tvalid_0's rmse: 0.119911\n",
      "[274]\tvalid_0's rmse: 0.119983\n",
      "[275]\tvalid_0's rmse: 0.119988\n",
      "[276]\tvalid_0's rmse: 0.119953\n",
      "[277]\tvalid_0's rmse: 0.119969\n",
      "[278]\tvalid_0's rmse: 0.119961\n",
      "[279]\tvalid_0's rmse: 0.119935\n",
      "[280]\tvalid_0's rmse: 0.119923\n",
      "[281]\tvalid_0's rmse: 0.119913\n",
      "[282]\tvalid_0's rmse: 0.119919\n",
      "[283]\tvalid_0's rmse: 0.119946\n",
      "[284]\tvalid_0's rmse: 0.119941\n",
      "[285]\tvalid_0's rmse: 0.119997\n",
      "[286]\tvalid_0's rmse: 0.119936\n",
      "[287]\tvalid_0's rmse: 0.119976\n",
      "[288]\tvalid_0's rmse: 0.119944\n",
      "[289]\tvalid_0's rmse: 0.119949\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290]\tvalid_0's rmse: 0.119967\n",
      "[291]\tvalid_0's rmse: 0.119939\n",
      "[292]\tvalid_0's rmse: 0.119908\n",
      "[293]\tvalid_0's rmse: 0.119868\n",
      "[294]\tvalid_0's rmse: 0.119907\n",
      "[295]\tvalid_0's rmse: 0.119948\n",
      "[296]\tvalid_0's rmse: 0.119976\n",
      "[297]\tvalid_0's rmse: 0.120036\n",
      "[298]\tvalid_0's rmse: 0.120007\n",
      "[299]\tvalid_0's rmse: 0.120082\n",
      "[300]\tvalid_0's rmse: 0.120058\n",
      "[301]\tvalid_0's rmse: 0.120123\n",
      "[302]\tvalid_0's rmse: 0.120106\n",
      "[303]\tvalid_0's rmse: 0.120132\n",
      "[304]\tvalid_0's rmse: 0.12011\n",
      "[305]\tvalid_0's rmse: 0.120114\n",
      "[306]\tvalid_0's rmse: 0.120147\n",
      "[307]\tvalid_0's rmse: 0.120196\n",
      "[308]\tvalid_0's rmse: 0.120208\n",
      "[309]\tvalid_0's rmse: 0.120256\n",
      "[310]\tvalid_0's rmse: 0.120256\n",
      "[311]\tvalid_0's rmse: 0.120252\n",
      "[312]\tvalid_0's rmse: 0.120309\n",
      "[313]\tvalid_0's rmse: 0.120357\n",
      "[314]\tvalid_0's rmse: 0.120365\n",
      "[315]\tvalid_0's rmse: 0.120382\n",
      "[316]\tvalid_0's rmse: 0.120409\n",
      "[317]\tvalid_0's rmse: 0.120392\n",
      "[318]\tvalid_0's rmse: 0.120368\n",
      "[319]\tvalid_0's rmse: 0.120401\n",
      "[320]\tvalid_0's rmse: 0.120366\n",
      "[321]\tvalid_0's rmse: 0.120367\n",
      "[322]\tvalid_0's rmse: 0.120325\n",
      "[323]\tvalid_0's rmse: 0.120391\n",
      "[324]\tvalid_0's rmse: 0.120382\n",
      "[325]\tvalid_0's rmse: 0.120381\n",
      "[326]\tvalid_0's rmse: 0.120321\n",
      "[327]\tvalid_0's rmse: 0.120328\n",
      "[328]\tvalid_0's rmse: 0.120381\n",
      "[329]\tvalid_0's rmse: 0.120392\n",
      "[330]\tvalid_0's rmse: 0.120442\n",
      "[331]\tvalid_0's rmse: 0.120442\n",
      "[332]\tvalid_0's rmse: 0.120438\n",
      "[333]\tvalid_0's rmse: 0.120484\n",
      "[334]\tvalid_0's rmse: 0.120461\n",
      "[335]\tvalid_0's rmse: 0.120485\n",
      "[336]\tvalid_0's rmse: 0.120517\n",
      "[337]\tvalid_0's rmse: 0.120523\n",
      "[338]\tvalid_0's rmse: 0.120583\n",
      "[339]\tvalid_0's rmse: 0.120554\n",
      "[340]\tvalid_0's rmse: 0.120561\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's rmse: 0.119765\n",
      "测试集 RMSE： 0.9073884535468801\n",
      "第3次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.09345\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.78582\n",
      "[3]\tvalid_0's rmse: 2.50834\n",
      "[4]\tvalid_0's rmse: 2.25913\n",
      "[5]\tvalid_0's rmse: 2.03475\n",
      "[6]\tvalid_0's rmse: 1.83286\n",
      "[7]\tvalid_0's rmse: 1.6511\n",
      "[8]\tvalid_0's rmse: 1.48767\n",
      "[9]\tvalid_0's rmse: 1.34116\n",
      "[10]\tvalid_0's rmse: 1.209\n",
      "[11]\tvalid_0's rmse: 1.09048\n",
      "[12]\tvalid_0's rmse: 0.983763\n",
      "[13]\tvalid_0's rmse: 0.888101\n",
      "[14]\tvalid_0's rmse: 0.80216\n",
      "[15]\tvalid_0's rmse: 0.724927\n",
      "[16]\tvalid_0's rmse: 0.655711\n",
      "[17]\tvalid_0's rmse: 0.593574\n",
      "[18]\tvalid_0's rmse: 0.537982\n",
      "[19]\tvalid_0's rmse: 0.488221\n",
      "[20]\tvalid_0's rmse: 0.443737\n",
      "[21]\tvalid_0's rmse: 0.40432\n",
      "[22]\tvalid_0's rmse: 0.368714\n",
      "[23]\tvalid_0's rmse: 0.337172\n",
      "[24]\tvalid_0's rmse: 0.30928\n",
      "[25]\tvalid_0's rmse: 0.284323\n",
      "[26]\tvalid_0's rmse: 0.262393\n",
      "[27]\tvalid_0's rmse: 0.242998\n",
      "[28]\tvalid_0's rmse: 0.226098\n",
      "[29]\tvalid_0's rmse: 0.21117\n",
      "[30]\tvalid_0's rmse: 0.198169\n",
      "[31]\tvalid_0's rmse: 0.186408\n",
      "[32]\tvalid_0's rmse: 0.17648\n",
      "[33]\tvalid_0's rmse: 0.167972\n",
      "[34]\tvalid_0's rmse: 0.160352\n",
      "[35]\tvalid_0's rmse: 0.153639\n",
      "[36]\tvalid_0's rmse: 0.148036\n",
      "[37]\tvalid_0's rmse: 0.143383\n",
      "[38]\tvalid_0's rmse: 0.138959\n",
      "[39]\tvalid_0's rmse: 0.135543\n",
      "[40]\tvalid_0's rmse: 0.132637\n",
      "[41]\tvalid_0's rmse: 0.13016\n",
      "[42]\tvalid_0's rmse: 0.127547\n",
      "[43]\tvalid_0's rmse: 0.125342\n",
      "[44]\tvalid_0's rmse: 0.123293\n",
      "[45]\tvalid_0's rmse: 0.121877\n",
      "[46]\tvalid_0's rmse: 0.120422\n",
      "[47]\tvalid_0's rmse: 0.119112\n",
      "[48]\tvalid_0's rmse: 0.117908\n",
      "[49]\tvalid_0's rmse: 0.117092\n",
      "[50]\tvalid_0's rmse: 0.116565\n",
      "[51]\tvalid_0's rmse: 0.11584\n",
      "[52]\tvalid_0's rmse: 0.114855\n",
      "[53]\tvalid_0's rmse: 0.114368\n",
      "[54]\tvalid_0's rmse: 0.113794\n",
      "[55]\tvalid_0's rmse: 0.113282\n",
      "[56]\tvalid_0's rmse: 0.112888\n",
      "[57]\tvalid_0's rmse: 0.112533\n",
      "[58]\tvalid_0's rmse: 0.112051\n",
      "[59]\tvalid_0's rmse: 0.111437\n",
      "[60]\tvalid_0's rmse: 0.111079\n",
      "[61]\tvalid_0's rmse: 0.110274\n",
      "[62]\tvalid_0's rmse: 0.110052\n",
      "[63]\tvalid_0's rmse: 0.109617\n",
      "[64]\tvalid_0's rmse: 0.10926\n",
      "[65]\tvalid_0's rmse: 0.108811\n",
      "[66]\tvalid_0's rmse: 0.108196\n",
      "[67]\tvalid_0's rmse: 0.107806\n",
      "[68]\tvalid_0's rmse: 0.107401\n",
      "[69]\tvalid_0's rmse: 0.107018\n",
      "[70]\tvalid_0's rmse: 0.106869\n",
      "[71]\tvalid_0's rmse: 0.106544\n",
      "[72]\tvalid_0's rmse: 0.106219\n",
      "[73]\tvalid_0's rmse: 0.106256\n",
      "[74]\tvalid_0's rmse: 0.106059\n",
      "[75]\tvalid_0's rmse: 0.10594\n",
      "[76]\tvalid_0's rmse: 0.105962\n",
      "[77]\tvalid_0's rmse: 0.105875\n",
      "[78]\tvalid_0's rmse: 0.105714\n",
      "[79]\tvalid_0's rmse: 0.105537\n",
      "[80]\tvalid_0's rmse: 0.105232\n",
      "[81]\tvalid_0's rmse: 0.105228\n",
      "[82]\tvalid_0's rmse: 0.105181\n",
      "[83]\tvalid_0's rmse: 0.105002\n",
      "[84]\tvalid_0's rmse: 0.104963\n",
      "[85]\tvalid_0's rmse: 0.104728\n",
      "[86]\tvalid_0's rmse: 0.104645\n",
      "[87]\tvalid_0's rmse: 0.104439\n",
      "[88]\tvalid_0's rmse: 0.104294\n",
      "[89]\tvalid_0's rmse: 0.104178\n",
      "[90]\tvalid_0's rmse: 0.104115\n",
      "[91]\tvalid_0's rmse: 0.104043\n",
      "[92]\tvalid_0's rmse: 0.104008\n",
      "[93]\tvalid_0's rmse: 0.104039\n",
      "[94]\tvalid_0's rmse: 0.104079\n",
      "[95]\tvalid_0's rmse: 0.104063\n",
      "[96]\tvalid_0's rmse: 0.104029\n",
      "[97]\tvalid_0's rmse: 0.103972\n",
      "[98]\tvalid_0's rmse: 0.103926\n",
      "[99]\tvalid_0's rmse: 0.10397\n",
      "[100]\tvalid_0's rmse: 0.104014\n",
      "[101]\tvalid_0's rmse: 0.103989\n",
      "[102]\tvalid_0's rmse: 0.104076\n",
      "[103]\tvalid_0's rmse: 0.103844\n",
      "[104]\tvalid_0's rmse: 0.103708\n",
      "[105]\tvalid_0's rmse: 0.103686\n",
      "[106]\tvalid_0's rmse: 0.103552\n",
      "[107]\tvalid_0's rmse: 0.103495\n",
      "[108]\tvalid_0's rmse: 0.103507\n",
      "[109]\tvalid_0's rmse: 0.103563\n",
      "[110]\tvalid_0's rmse: 0.103625\n",
      "[111]\tvalid_0's rmse: 0.103607\n",
      "[112]\tvalid_0's rmse: 0.103463\n",
      "[113]\tvalid_0's rmse: 0.103457\n",
      "[114]\tvalid_0's rmse: 0.103312\n",
      "[115]\tvalid_0's rmse: 0.103162\n",
      "[116]\tvalid_0's rmse: 0.103041\n",
      "[117]\tvalid_0's rmse: 0.10306\n",
      "[118]\tvalid_0's rmse: 0.103078\n",
      "[119]\tvalid_0's rmse: 0.103\n",
      "[120]\tvalid_0's rmse: 0.103036\n",
      "[121]\tvalid_0's rmse: 0.103047\n",
      "[122]\tvalid_0's rmse: 0.103066\n",
      "[123]\tvalid_0's rmse: 0.103101\n",
      "[124]\tvalid_0's rmse: 0.10302\n",
      "[125]\tvalid_0's rmse: 0.10305\n",
      "[126]\tvalid_0's rmse: 0.102911\n",
      "[127]\tvalid_0's rmse: 0.102815\n",
      "[128]\tvalid_0's rmse: 0.102872\n",
      "[129]\tvalid_0's rmse: 0.102835\n",
      "[130]\tvalid_0's rmse: 0.102782\n",
      "[131]\tvalid_0's rmse: 0.102655\n",
      "[132]\tvalid_0's rmse: 0.102643\n",
      "[133]\tvalid_0's rmse: 0.102521\n",
      "[134]\tvalid_0's rmse: 0.102542\n",
      "[135]\tvalid_0's rmse: 0.102594\n",
      "[136]\tvalid_0's rmse: 0.10266\n",
      "[137]\tvalid_0's rmse: 0.102833\n",
      "[138]\tvalid_0's rmse: 0.102885\n",
      "[139]\tvalid_0's rmse: 0.102833\n",
      "[140]\tvalid_0's rmse: 0.102838\n",
      "[141]\tvalid_0's rmse: 0.102895\n",
      "[142]\tvalid_0's rmse: 0.102888\n",
      "[143]\tvalid_0's rmse: 0.102987\n",
      "[144]\tvalid_0's rmse: 0.102891\n",
      "[145]\tvalid_0's rmse: 0.102841\n",
      "[146]\tvalid_0's rmse: 0.102849\n",
      "[147]\tvalid_0's rmse: 0.10283\n",
      "[148]\tvalid_0's rmse: 0.102792\n",
      "[149]\tvalid_0's rmse: 0.102901\n",
      "[150]\tvalid_0's rmse: 0.102904\n",
      "[151]\tvalid_0's rmse: 0.102832\n",
      "[152]\tvalid_0's rmse: 0.102878\n",
      "[153]\tvalid_0's rmse: 0.102865\n",
      "[154]\tvalid_0's rmse: 0.102822\n",
      "[155]\tvalid_0's rmse: 0.10284\n",
      "[156]\tvalid_0's rmse: 0.102785\n",
      "[157]\tvalid_0's rmse: 0.10275\n",
      "[158]\tvalid_0's rmse: 0.102682\n",
      "[159]\tvalid_0's rmse: 0.10267\n",
      "[160]\tvalid_0's rmse: 0.102616\n",
      "[161]\tvalid_0's rmse: 0.102582\n",
      "[162]\tvalid_0's rmse: 0.102548\n",
      "[163]\tvalid_0's rmse: 0.102487\n",
      "[164]\tvalid_0's rmse: 0.10251\n",
      "[165]\tvalid_0's rmse: 0.102511\n",
      "[166]\tvalid_0's rmse: 0.102565\n",
      "[167]\tvalid_0's rmse: 0.102572\n",
      "[168]\tvalid_0's rmse: 0.102549\n",
      "[169]\tvalid_0's rmse: 0.102536\n",
      "[170]\tvalid_0's rmse: 0.102619\n",
      "[171]\tvalid_0's rmse: 0.102672\n",
      "[172]\tvalid_0's rmse: 0.102603\n",
      "[173]\tvalid_0's rmse: 0.102674\n",
      "[174]\tvalid_0's rmse: 0.102726\n",
      "[175]\tvalid_0's rmse: 0.102741\n",
      "[176]\tvalid_0's rmse: 0.102871\n",
      "[177]\tvalid_0's rmse: 0.102864\n",
      "[178]\tvalid_0's rmse: 0.102965\n",
      "[179]\tvalid_0's rmse: 0.10308\n",
      "[180]\tvalid_0's rmse: 0.103026\n",
      "[181]\tvalid_0's rmse: 0.103007\n",
      "[182]\tvalid_0's rmse: 0.103112\n",
      "[183]\tvalid_0's rmse: 0.103073\n",
      "[184]\tvalid_0's rmse: 0.10311\n",
      "[185]\tvalid_0's rmse: 0.103107\n",
      "[186]\tvalid_0's rmse: 0.103046\n",
      "[187]\tvalid_0's rmse: 0.102946\n",
      "[188]\tvalid_0's rmse: 0.102881\n",
      "[189]\tvalid_0's rmse: 0.102835\n",
      "[190]\tvalid_0's rmse: 0.102838\n",
      "[191]\tvalid_0's rmse: 0.102899\n",
      "[192]\tvalid_0's rmse: 0.102851\n",
      "[193]\tvalid_0's rmse: 0.102892\n",
      "[194]\tvalid_0's rmse: 0.102917\n",
      "[195]\tvalid_0's rmse: 0.102998\n",
      "[196]\tvalid_0's rmse: 0.102995\n",
      "[197]\tvalid_0's rmse: 0.103086\n",
      "[198]\tvalid_0's rmse: 0.103032\n",
      "[199]\tvalid_0's rmse: 0.102951\n",
      "[200]\tvalid_0's rmse: 0.102907\n",
      "[201]\tvalid_0's rmse: 0.102892\n",
      "[202]\tvalid_0's rmse: 0.102869\n",
      "[203]\tvalid_0's rmse: 0.102853\n",
      "[204]\tvalid_0's rmse: 0.102885\n",
      "[205]\tvalid_0's rmse: 0.102926\n",
      "[206]\tvalid_0's rmse: 0.102914\n",
      "[207]\tvalid_0's rmse: 0.10289\n",
      "[208]\tvalid_0's rmse: 0.102863\n",
      "[209]\tvalid_0's rmse: 0.102943\n",
      "[210]\tvalid_0's rmse: 0.103017\n",
      "[211]\tvalid_0's rmse: 0.102983\n",
      "[212]\tvalid_0's rmse: 0.103021\n",
      "[213]\tvalid_0's rmse: 0.10297\n",
      "[214]\tvalid_0's rmse: 0.102958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[215]\tvalid_0's rmse: 0.102988\n",
      "[216]\tvalid_0's rmse: 0.103074\n",
      "[217]\tvalid_0's rmse: 0.103002\n",
      "[218]\tvalid_0's rmse: 0.102961\n",
      "[219]\tvalid_0's rmse: 0.102954\n",
      "[220]\tvalid_0's rmse: 0.102989\n",
      "[221]\tvalid_0's rmse: 0.102965\n",
      "[222]\tvalid_0's rmse: 0.102973\n",
      "[223]\tvalid_0's rmse: 0.103021\n",
      "[224]\tvalid_0's rmse: 0.103059\n",
      "[225]\tvalid_0's rmse: 0.103153\n",
      "[226]\tvalid_0's rmse: 0.103208\n",
      "[227]\tvalid_0's rmse: 0.103325\n",
      "[228]\tvalid_0's rmse: 0.103281\n",
      "[229]\tvalid_0's rmse: 0.103321\n",
      "[230]\tvalid_0's rmse: 0.103359\n",
      "[231]\tvalid_0's rmse: 0.103394\n",
      "[232]\tvalid_0's rmse: 0.103453\n",
      "[233]\tvalid_0's rmse: 0.10344\n",
      "[234]\tvalid_0's rmse: 0.103474\n",
      "[235]\tvalid_0's rmse: 0.103488\n",
      "[236]\tvalid_0's rmse: 0.103491\n",
      "[237]\tvalid_0's rmse: 0.103626\n",
      "[238]\tvalid_0's rmse: 0.103628\n",
      "[239]\tvalid_0's rmse: 0.103661\n",
      "[240]\tvalid_0's rmse: 0.103664\n",
      "[241]\tvalid_0's rmse: 0.103692\n",
      "[242]\tvalid_0's rmse: 0.103707\n",
      "[243]\tvalid_0's rmse: 0.103717\n",
      "[244]\tvalid_0's rmse: 0.103723\n",
      "[245]\tvalid_0's rmse: 0.103799\n",
      "[246]\tvalid_0's rmse: 0.103895\n",
      "[247]\tvalid_0's rmse: 0.10401\n",
      "[248]\tvalid_0's rmse: 0.104035\n",
      "[249]\tvalid_0's rmse: 0.104029\n",
      "[250]\tvalid_0's rmse: 0.104105\n",
      "[251]\tvalid_0's rmse: 0.104244\n",
      "[252]\tvalid_0's rmse: 0.104291\n",
      "[253]\tvalid_0's rmse: 0.104394\n",
      "[254]\tvalid_0's rmse: 0.104334\n",
      "[255]\tvalid_0's rmse: 0.104368\n",
      "[256]\tvalid_0's rmse: 0.104361\n",
      "[257]\tvalid_0's rmse: 0.104317\n",
      "[258]\tvalid_0's rmse: 0.104297\n",
      "[259]\tvalid_0's rmse: 0.104399\n",
      "[260]\tvalid_0's rmse: 0.104377\n",
      "[261]\tvalid_0's rmse: 0.104339\n",
      "[262]\tvalid_0's rmse: 0.104364\n",
      "[263]\tvalid_0's rmse: 0.104407\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's rmse: 0.102487\n",
      "测试集 RMSE： 0.9175435961885596\n",
      "第4次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.11902\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.80806\n",
      "[3]\tvalid_0's rmse: 2.52848\n",
      "[4]\tvalid_0's rmse: 2.27641\n",
      "[5]\tvalid_0's rmse: 2.05005\n",
      "[6]\tvalid_0's rmse: 1.84697\n",
      "[7]\tvalid_0's rmse: 1.66416\n",
      "[8]\tvalid_0's rmse: 1.49927\n",
      "[9]\tvalid_0's rmse: 1.35075\n",
      "[10]\tvalid_0's rmse: 1.21812\n",
      "[11]\tvalid_0's rmse: 1.0982\n",
      "[12]\tvalid_0's rmse: 0.990523\n",
      "[13]\tvalid_0's rmse: 0.893768\n",
      "[14]\tvalid_0's rmse: 0.806879\n",
      "[15]\tvalid_0's rmse: 0.729071\n",
      "[16]\tvalid_0's rmse: 0.659146\n",
      "[17]\tvalid_0's rmse: 0.596586\n",
      "[18]\tvalid_0's rmse: 0.540438\n",
      "[19]\tvalid_0's rmse: 0.490302\n",
      "[20]\tvalid_0's rmse: 0.445386\n",
      "[21]\tvalid_0's rmse: 0.405022\n",
      "[22]\tvalid_0's rmse: 0.369296\n",
      "[23]\tvalid_0's rmse: 0.336786\n",
      "[24]\tvalid_0's rmse: 0.308142\n",
      "[25]\tvalid_0's rmse: 0.282639\n",
      "[26]\tvalid_0's rmse: 0.259996\n",
      "[27]\tvalid_0's rmse: 0.239851\n",
      "[28]\tvalid_0's rmse: 0.222309\n",
      "[29]\tvalid_0's rmse: 0.207001\n",
      "[30]\tvalid_0's rmse: 0.193809\n",
      "[31]\tvalid_0's rmse: 0.182125\n",
      "[32]\tvalid_0's rmse: 0.171877\n",
      "[33]\tvalid_0's rmse: 0.16279\n",
      "[34]\tvalid_0's rmse: 0.155142\n",
      "[35]\tvalid_0's rmse: 0.148391\n",
      "[36]\tvalid_0's rmse: 0.142813\n",
      "[37]\tvalid_0's rmse: 0.137972\n",
      "[38]\tvalid_0's rmse: 0.133663\n",
      "[39]\tvalid_0's rmse: 0.130221\n",
      "[40]\tvalid_0's rmse: 0.127128\n",
      "[41]\tvalid_0's rmse: 0.124261\n",
      "[42]\tvalid_0's rmse: 0.121855\n",
      "[43]\tvalid_0's rmse: 0.119848\n",
      "[44]\tvalid_0's rmse: 0.118029\n",
      "[45]\tvalid_0's rmse: 0.116448\n",
      "[46]\tvalid_0's rmse: 0.114752\n",
      "[47]\tvalid_0's rmse: 0.113502\n",
      "[48]\tvalid_0's rmse: 0.112328\n",
      "[49]\tvalid_0's rmse: 0.111622\n",
      "[50]\tvalid_0's rmse: 0.110714\n",
      "[51]\tvalid_0's rmse: 0.110216\n",
      "[52]\tvalid_0's rmse: 0.10979\n",
      "[53]\tvalid_0's rmse: 0.109103\n",
      "[54]\tvalid_0's rmse: 0.108745\n",
      "[55]\tvalid_0's rmse: 0.108143\n",
      "[56]\tvalid_0's rmse: 0.108128\n",
      "[57]\tvalid_0's rmse: 0.107758\n",
      "[58]\tvalid_0's rmse: 0.10714\n",
      "[59]\tvalid_0's rmse: 0.106566\n",
      "[60]\tvalid_0's rmse: 0.106129\n",
      "[61]\tvalid_0's rmse: 0.105694\n",
      "[62]\tvalid_0's rmse: 0.105223\n",
      "[63]\tvalid_0's rmse: 0.104976\n",
      "[64]\tvalid_0's rmse: 0.104609\n",
      "[65]\tvalid_0's rmse: 0.104275\n",
      "[66]\tvalid_0's rmse: 0.104016\n",
      "[67]\tvalid_0's rmse: 0.103506\n",
      "[68]\tvalid_0's rmse: 0.103399\n",
      "[69]\tvalid_0's rmse: 0.103179\n",
      "[70]\tvalid_0's rmse: 0.10299\n",
      "[71]\tvalid_0's rmse: 0.102748\n",
      "[72]\tvalid_0's rmse: 0.102609\n",
      "[73]\tvalid_0's rmse: 0.102488\n",
      "[74]\tvalid_0's rmse: 0.102345\n",
      "[75]\tvalid_0's rmse: 0.102394\n",
      "[76]\tvalid_0's rmse: 0.102308\n",
      "[77]\tvalid_0's rmse: 0.102248\n",
      "[78]\tvalid_0's rmse: 0.102145\n",
      "[79]\tvalid_0's rmse: 0.101948\n",
      "[80]\tvalid_0's rmse: 0.101881\n",
      "[81]\tvalid_0's rmse: 0.101748\n",
      "[82]\tvalid_0's rmse: 0.101672\n",
      "[83]\tvalid_0's rmse: 0.101569\n",
      "[84]\tvalid_0's rmse: 0.101538\n",
      "[85]\tvalid_0's rmse: 0.101492\n",
      "[86]\tvalid_0's rmse: 0.10144\n",
      "[87]\tvalid_0's rmse: 0.101287\n",
      "[88]\tvalid_0's rmse: 0.101248\n",
      "[89]\tvalid_0's rmse: 0.101243\n",
      "[90]\tvalid_0's rmse: 0.101246\n",
      "[91]\tvalid_0's rmse: 0.101317\n",
      "[92]\tvalid_0's rmse: 0.101233\n",
      "[93]\tvalid_0's rmse: 0.101164\n",
      "[94]\tvalid_0's rmse: 0.101097\n",
      "[95]\tvalid_0's rmse: 0.100954\n",
      "[96]\tvalid_0's rmse: 0.100889\n",
      "[97]\tvalid_0's rmse: 0.100743\n",
      "[98]\tvalid_0's rmse: 0.100821\n",
      "[99]\tvalid_0's rmse: 0.100875\n",
      "[100]\tvalid_0's rmse: 0.100768\n",
      "[101]\tvalid_0's rmse: 0.100776\n",
      "[102]\tvalid_0's rmse: 0.100674\n",
      "[103]\tvalid_0's rmse: 0.100738\n",
      "[104]\tvalid_0's rmse: 0.100713\n",
      "[105]\tvalid_0's rmse: 0.100663\n",
      "[106]\tvalid_0's rmse: 0.100652\n",
      "[107]\tvalid_0's rmse: 0.100627\n",
      "[108]\tvalid_0's rmse: 0.10057\n",
      "[109]\tvalid_0's rmse: 0.100572\n",
      "[110]\tvalid_0's rmse: 0.100501\n",
      "[111]\tvalid_0's rmse: 0.100559\n",
      "[112]\tvalid_0's rmse: 0.100282\n",
      "[113]\tvalid_0's rmse: 0.100297\n",
      "[114]\tvalid_0's rmse: 0.100269\n",
      "[115]\tvalid_0's rmse: 0.100167\n",
      "[116]\tvalid_0's rmse: 0.100182\n",
      "[117]\tvalid_0's rmse: 0.100099\n",
      "[118]\tvalid_0's rmse: 0.100137\n",
      "[119]\tvalid_0's rmse: 0.10002\n",
      "[120]\tvalid_0's rmse: 0.100014\n",
      "[121]\tvalid_0's rmse: 0.0999903\n",
      "[122]\tvalid_0's rmse: 0.100086\n",
      "[123]\tvalid_0's rmse: 0.100022\n",
      "[124]\tvalid_0's rmse: 0.100008\n",
      "[125]\tvalid_0's rmse: 0.0998428\n",
      "[126]\tvalid_0's rmse: 0.0998039\n",
      "[127]\tvalid_0's rmse: 0.0997315\n",
      "[128]\tvalid_0's rmse: 0.0997081\n",
      "[129]\tvalid_0's rmse: 0.0997387\n",
      "[130]\tvalid_0's rmse: 0.0997258\n",
      "[131]\tvalid_0's rmse: 0.0996791\n",
      "[132]\tvalid_0's rmse: 0.0996088\n",
      "[133]\tvalid_0's rmse: 0.0997667\n",
      "[134]\tvalid_0's rmse: 0.0997529\n",
      "[135]\tvalid_0's rmse: 0.0996478\n",
      "[136]\tvalid_0's rmse: 0.0997765\n",
      "[137]\tvalid_0's rmse: 0.0997216\n",
      "[138]\tvalid_0's rmse: 0.0997168\n",
      "[139]\tvalid_0's rmse: 0.0997909\n",
      "[140]\tvalid_0's rmse: 0.0997366\n",
      "[141]\tvalid_0's rmse: 0.0997902\n",
      "[142]\tvalid_0's rmse: 0.0998161\n",
      "[143]\tvalid_0's rmse: 0.0998206\n",
      "[144]\tvalid_0's rmse: 0.0998715\n",
      "[145]\tvalid_0's rmse: 0.0999831\n",
      "[146]\tvalid_0's rmse: 0.0999219\n",
      "[147]\tvalid_0's rmse: 0.0998942\n",
      "[148]\tvalid_0's rmse: 0.0998339\n",
      "[149]\tvalid_0's rmse: 0.0999037\n",
      "[150]\tvalid_0's rmse: 0.100042\n",
      "[151]\tvalid_0's rmse: 0.100058\n",
      "[152]\tvalid_0's rmse: 0.100009\n",
      "[153]\tvalid_0's rmse: 0.0999869\n",
      "[154]\tvalid_0's rmse: 0.0999092\n",
      "[155]\tvalid_0's rmse: 0.0999403\n",
      "[156]\tvalid_0's rmse: 0.0999328\n",
      "[157]\tvalid_0's rmse: 0.0999878\n",
      "[158]\tvalid_0's rmse: 0.0998912\n",
      "[159]\tvalid_0's rmse: 0.0998703\n",
      "[160]\tvalid_0's rmse: 0.099804\n",
      "[161]\tvalid_0's rmse: 0.0998617\n",
      "[162]\tvalid_0's rmse: 0.0999752\n",
      "[163]\tvalid_0's rmse: 0.0999734\n",
      "[164]\tvalid_0's rmse: 0.0999234\n",
      "[165]\tvalid_0's rmse: 0.0998732\n",
      "[166]\tvalid_0's rmse: 0.0999086\n",
      "[167]\tvalid_0's rmse: 0.099931\n",
      "[168]\tvalid_0's rmse: 0.0999685\n",
      "[169]\tvalid_0's rmse: 0.0999414\n",
      "[170]\tvalid_0's rmse: 0.0999291\n",
      "[171]\tvalid_0's rmse: 0.0999004\n",
      "[172]\tvalid_0's rmse: 0.1\n",
      "[173]\tvalid_0's rmse: 0.0999216\n",
      "[174]\tvalid_0's rmse: 0.100019\n",
      "[175]\tvalid_0's rmse: 0.100048\n",
      "[176]\tvalid_0's rmse: 0.10005\n",
      "[177]\tvalid_0's rmse: 0.0999711\n",
      "[178]\tvalid_0's rmse: 0.0999394\n",
      "[179]\tvalid_0's rmse: 0.0999072\n",
      "[180]\tvalid_0's rmse: 0.0999444\n",
      "[181]\tvalid_0's rmse: 0.099924\n",
      "[182]\tvalid_0's rmse: 0.0999516\n",
      "[183]\tvalid_0's rmse: 0.0999542\n",
      "[184]\tvalid_0's rmse: 0.0999811\n",
      "[185]\tvalid_0's rmse: 0.100061\n",
      "[186]\tvalid_0's rmse: 0.0999905\n",
      "[187]\tvalid_0's rmse: 0.100004\n",
      "[188]\tvalid_0's rmse: 0.099965\n",
      "[189]\tvalid_0's rmse: 0.0999488\n",
      "[190]\tvalid_0's rmse: 0.100012\n",
      "[191]\tvalid_0's rmse: 0.0999908\n",
      "[192]\tvalid_0's rmse: 0.100045\n",
      "[193]\tvalid_0's rmse: 0.100144\n",
      "[194]\tvalid_0's rmse: 0.100141\n",
      "[195]\tvalid_0's rmse: 0.100197\n",
      "[196]\tvalid_0's rmse: 0.100144\n",
      "[197]\tvalid_0's rmse: 0.100148\n",
      "[198]\tvalid_0's rmse: 0.100213\n",
      "[199]\tvalid_0's rmse: 0.100247\n",
      "[200]\tvalid_0's rmse: 0.100267\n",
      "[201]\tvalid_0's rmse: 0.100233\n",
      "[202]\tvalid_0's rmse: 0.100321\n",
      "[203]\tvalid_0's rmse: 0.100236\n",
      "[204]\tvalid_0's rmse: 0.100292\n",
      "[205]\tvalid_0's rmse: 0.100359\n",
      "[206]\tvalid_0's rmse: 0.100267\n",
      "[207]\tvalid_0's rmse: 0.100337\n",
      "[208]\tvalid_0's rmse: 0.10041\n",
      "[209]\tvalid_0's rmse: 0.100458\n",
      "[210]\tvalid_0's rmse: 0.100478\n",
      "[211]\tvalid_0's rmse: 0.100534\n",
      "[212]\tvalid_0's rmse: 0.100509\n",
      "[213]\tvalid_0's rmse: 0.100535\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[214]\tvalid_0's rmse: 0.100425\n",
      "[215]\tvalid_0's rmse: 0.100485\n",
      "[216]\tvalid_0's rmse: 0.100538\n",
      "[217]\tvalid_0's rmse: 0.100636\n",
      "[218]\tvalid_0's rmse: 0.10067\n",
      "[219]\tvalid_0's rmse: 0.100597\n",
      "[220]\tvalid_0's rmse: 0.100635\n",
      "[221]\tvalid_0's rmse: 0.100702\n",
      "[222]\tvalid_0's rmse: 0.100635\n",
      "[223]\tvalid_0's rmse: 0.10056\n",
      "[224]\tvalid_0's rmse: 0.100551\n",
      "[225]\tvalid_0's rmse: 0.100565\n",
      "[226]\tvalid_0's rmse: 0.100539\n",
      "[227]\tvalid_0's rmse: 0.100507\n",
      "[228]\tvalid_0's rmse: 0.100491\n",
      "[229]\tvalid_0's rmse: 0.100517\n",
      "[230]\tvalid_0's rmse: 0.100581\n",
      "[231]\tvalid_0's rmse: 0.10068\n",
      "[232]\tvalid_0's rmse: 0.100674\n",
      "Early stopping, best iteration is:\n",
      "[132]\tvalid_0's rmse: 0.0996088\n",
      "测试集 RMSE： 0.9193362495762213\n",
      "第5次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.09074\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.78282\n",
      "[3]\tvalid_0's rmse: 2.50616\n",
      "[4]\tvalid_0's rmse: 2.25733\n",
      "[5]\tvalid_0's rmse: 2.03388\n",
      "[6]\tvalid_0's rmse: 1.83232\n",
      "[7]\tvalid_0's rmse: 1.65174\n",
      "[8]\tvalid_0's rmse: 1.48912\n",
      "[9]\tvalid_0's rmse: 1.34305\n",
      "[10]\tvalid_0's rmse: 1.21223\n",
      "[11]\tvalid_0's rmse: 1.09518\n",
      "[12]\tvalid_0's rmse: 0.989783\n",
      "[13]\tvalid_0's rmse: 0.895184\n",
      "[14]\tvalid_0's rmse: 0.810572\n",
      "[15]\tvalid_0's rmse: 0.735\n",
      "[16]\tvalid_0's rmse: 0.667637\n",
      "[17]\tvalid_0's rmse: 0.606924\n",
      "[18]\tvalid_0's rmse: 0.553453\n",
      "[19]\tvalid_0's rmse: 0.505931\n",
      "[20]\tvalid_0's rmse: 0.464121\n",
      "[21]\tvalid_0's rmse: 0.427075\n",
      "[22]\tvalid_0's rmse: 0.39463\n",
      "[23]\tvalid_0's rmse: 0.366207\n",
      "[24]\tvalid_0's rmse: 0.341105\n",
      "[25]\tvalid_0's rmse: 0.319221\n",
      "[26]\tvalid_0's rmse: 0.300035\n",
      "[27]\tvalid_0's rmse: 0.283204\n",
      "[28]\tvalid_0's rmse: 0.269096\n",
      "[29]\tvalid_0's rmse: 0.257027\n",
      "[30]\tvalid_0's rmse: 0.247072\n",
      "[31]\tvalid_0's rmse: 0.238495\n",
      "[32]\tvalid_0's rmse: 0.231466\n",
      "[33]\tvalid_0's rmse: 0.225369\n",
      "[34]\tvalid_0's rmse: 0.219953\n",
      "[35]\tvalid_0's rmse: 0.215942\n",
      "[36]\tvalid_0's rmse: 0.212578\n",
      "[37]\tvalid_0's rmse: 0.209826\n",
      "[38]\tvalid_0's rmse: 0.207533\n",
      "[39]\tvalid_0's rmse: 0.2053\n",
      "[40]\tvalid_0's rmse: 0.20365\n",
      "[41]\tvalid_0's rmse: 0.202134\n",
      "[42]\tvalid_0's rmse: 0.201035\n",
      "[43]\tvalid_0's rmse: 0.199961\n",
      "[44]\tvalid_0's rmse: 0.199184\n",
      "[45]\tvalid_0's rmse: 0.198862\n",
      "[46]\tvalid_0's rmse: 0.19832\n",
      "[47]\tvalid_0's rmse: 0.197869\n",
      "[48]\tvalid_0's rmse: 0.197667\n",
      "[49]\tvalid_0's rmse: 0.197497\n",
      "[50]\tvalid_0's rmse: 0.197234\n",
      "[51]\tvalid_0's rmse: 0.196901\n",
      "[52]\tvalid_0's rmse: 0.19656\n",
      "[53]\tvalid_0's rmse: 0.196463\n",
      "[54]\tvalid_0's rmse: 0.196293\n",
      "[55]\tvalid_0's rmse: 0.196132\n",
      "[56]\tvalid_0's rmse: 0.196032\n",
      "[57]\tvalid_0's rmse: 0.195967\n",
      "[58]\tvalid_0's rmse: 0.195871\n",
      "[59]\tvalid_0's rmse: 0.195785\n",
      "[60]\tvalid_0's rmse: 0.196005\n",
      "[61]\tvalid_0's rmse: 0.196039\n",
      "[62]\tvalid_0's rmse: 0.195848\n",
      "[63]\tvalid_0's rmse: 0.195791\n",
      "[64]\tvalid_0's rmse: 0.19581\n",
      "[65]\tvalid_0's rmse: 0.195764\n",
      "[66]\tvalid_0's rmse: 0.195673\n",
      "[67]\tvalid_0's rmse: 0.195637\n",
      "[68]\tvalid_0's rmse: 0.195551\n",
      "[69]\tvalid_0's rmse: 0.195491\n",
      "[70]\tvalid_0's rmse: 0.195579\n",
      "[71]\tvalid_0's rmse: 0.195575\n",
      "[72]\tvalid_0's rmse: 0.195498\n",
      "[73]\tvalid_0's rmse: 0.195329\n",
      "[74]\tvalid_0's rmse: 0.195537\n",
      "[75]\tvalid_0's rmse: 0.195487\n",
      "[76]\tvalid_0's rmse: 0.195549\n",
      "[77]\tvalid_0's rmse: 0.195471\n",
      "[78]\tvalid_0's rmse: 0.195396\n",
      "[79]\tvalid_0's rmse: 0.195323\n",
      "[80]\tvalid_0's rmse: 0.19532\n",
      "[81]\tvalid_0's rmse: 0.19527\n",
      "[82]\tvalid_0's rmse: 0.195131\n",
      "[83]\tvalid_0's rmse: 0.19517\n",
      "[84]\tvalid_0's rmse: 0.195024\n",
      "[85]\tvalid_0's rmse: 0.195024\n",
      "[86]\tvalid_0's rmse: 0.195053\n",
      "[87]\tvalid_0's rmse: 0.194933\n",
      "[88]\tvalid_0's rmse: 0.194841\n",
      "[89]\tvalid_0's rmse: 0.194782\n",
      "[90]\tvalid_0's rmse: 0.194918\n",
      "[91]\tvalid_0's rmse: 0.19488\n",
      "[92]\tvalid_0's rmse: 0.195002\n",
      "[93]\tvalid_0's rmse: 0.194977\n",
      "[94]\tvalid_0's rmse: 0.195003\n",
      "[95]\tvalid_0's rmse: 0.195052\n",
      "[96]\tvalid_0's rmse: 0.195017\n",
      "[97]\tvalid_0's rmse: 0.195053\n",
      "[98]\tvalid_0's rmse: 0.19508\n",
      "[99]\tvalid_0's rmse: 0.19494\n",
      "[100]\tvalid_0's rmse: 0.194946\n",
      "[101]\tvalid_0's rmse: 0.194923\n",
      "[102]\tvalid_0's rmse: 0.194876\n",
      "[103]\tvalid_0's rmse: 0.19494\n",
      "[104]\tvalid_0's rmse: 0.195044\n",
      "[105]\tvalid_0's rmse: 0.195045\n",
      "[106]\tvalid_0's rmse: 0.195031\n",
      "[107]\tvalid_0's rmse: 0.195032\n",
      "[108]\tvalid_0's rmse: 0.195021\n",
      "[109]\tvalid_0's rmse: 0.195079\n",
      "[110]\tvalid_0's rmse: 0.195065\n",
      "[111]\tvalid_0's rmse: 0.195066\n",
      "[112]\tvalid_0's rmse: 0.195214\n",
      "[113]\tvalid_0's rmse: 0.195169\n",
      "[114]\tvalid_0's rmse: 0.195204\n",
      "[115]\tvalid_0's rmse: 0.195198\n",
      "[116]\tvalid_0's rmse: 0.195152\n",
      "[117]\tvalid_0's rmse: 0.195197\n",
      "[118]\tvalid_0's rmse: 0.195252\n",
      "[119]\tvalid_0's rmse: 0.195266\n",
      "[120]\tvalid_0's rmse: 0.195247\n",
      "[121]\tvalid_0's rmse: 0.195255\n",
      "[122]\tvalid_0's rmse: 0.195223\n",
      "[123]\tvalid_0's rmse: 0.195141\n",
      "[124]\tvalid_0's rmse: 0.195213\n",
      "[125]\tvalid_0's rmse: 0.19524\n",
      "[126]\tvalid_0's rmse: 0.195232\n",
      "[127]\tvalid_0's rmse: 0.195237\n",
      "[128]\tvalid_0's rmse: 0.195296\n",
      "[129]\tvalid_0's rmse: 0.195279\n",
      "[130]\tvalid_0's rmse: 0.195255\n",
      "[131]\tvalid_0's rmse: 0.195196\n",
      "[132]\tvalid_0's rmse: 0.195185\n",
      "[133]\tvalid_0's rmse: 0.195181\n",
      "[134]\tvalid_0's rmse: 0.195251\n",
      "[135]\tvalid_0's rmse: 0.195254\n",
      "[136]\tvalid_0's rmse: 0.195225\n",
      "[137]\tvalid_0's rmse: 0.195223\n",
      "[138]\tvalid_0's rmse: 0.195201\n",
      "[139]\tvalid_0's rmse: 0.195335\n",
      "[140]\tvalid_0's rmse: 0.195359\n",
      "[141]\tvalid_0's rmse: 0.195388\n",
      "[142]\tvalid_0's rmse: 0.195374\n",
      "[143]\tvalid_0's rmse: 0.195435\n",
      "[144]\tvalid_0's rmse: 0.195472\n",
      "[145]\tvalid_0's rmse: 0.195548\n",
      "[146]\tvalid_0's rmse: 0.195546\n",
      "[147]\tvalid_0's rmse: 0.195566\n",
      "[148]\tvalid_0's rmse: 0.195554\n",
      "[149]\tvalid_0's rmse: 0.195587\n",
      "[150]\tvalid_0's rmse: 0.195604\n",
      "[151]\tvalid_0's rmse: 0.195559\n",
      "[152]\tvalid_0's rmse: 0.195521\n",
      "[153]\tvalid_0's rmse: 0.195564\n",
      "[154]\tvalid_0's rmse: 0.195507\n",
      "[155]\tvalid_0's rmse: 0.195518\n",
      "[156]\tvalid_0's rmse: 0.195581\n",
      "[157]\tvalid_0's rmse: 0.195535\n",
      "[158]\tvalid_0's rmse: 0.1956\n",
      "[159]\tvalid_0's rmse: 0.195605\n",
      "[160]\tvalid_0's rmse: 0.195697\n",
      "[161]\tvalid_0's rmse: 0.195659\n",
      "[162]\tvalid_0's rmse: 0.195659\n",
      "[163]\tvalid_0's rmse: 0.195626\n",
      "[164]\tvalid_0's rmse: 0.195603\n",
      "[165]\tvalid_0's rmse: 0.195588\n",
      "[166]\tvalid_0's rmse: 0.195723\n",
      "[167]\tvalid_0's rmse: 0.195754\n",
      "[168]\tvalid_0's rmse: 0.195809\n",
      "[169]\tvalid_0's rmse: 0.195829\n",
      "[170]\tvalid_0's rmse: 0.195858\n",
      "[171]\tvalid_0's rmse: 0.195847\n",
      "[172]\tvalid_0's rmse: 0.195822\n",
      "[173]\tvalid_0's rmse: 0.195842\n",
      "[174]\tvalid_0's rmse: 0.19583\n",
      "[175]\tvalid_0's rmse: 0.195864\n",
      "[176]\tvalid_0's rmse: 0.195802\n",
      "[177]\tvalid_0's rmse: 0.195766\n",
      "[178]\tvalid_0's rmse: 0.195776\n",
      "[179]\tvalid_0's rmse: 0.195857\n",
      "[180]\tvalid_0's rmse: 0.195789\n",
      "[181]\tvalid_0's rmse: 0.195803\n",
      "[182]\tvalid_0's rmse: 0.19577\n",
      "[183]\tvalid_0's rmse: 0.195799\n",
      "[184]\tvalid_0's rmse: 0.195732\n",
      "[185]\tvalid_0's rmse: 0.195818\n",
      "[186]\tvalid_0's rmse: 0.195775\n",
      "[187]\tvalid_0's rmse: 0.195715\n",
      "[188]\tvalid_0's rmse: 0.195671\n",
      "[189]\tvalid_0's rmse: 0.195712\n",
      "Early stopping, best iteration is:\n",
      "[89]\tvalid_0's rmse: 0.194782\n",
      "测试集 RMSE： 0.874912662470435\n",
      "第6次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.09367\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.78601\n",
      "[3]\tvalid_0's rmse: 2.50884\n",
      "[4]\tvalid_0's rmse: 2.25992\n",
      "[5]\tvalid_0's rmse: 2.03532\n",
      "[6]\tvalid_0's rmse: 1.83365\n",
      "[7]\tvalid_0's rmse: 1.65206\n",
      "[8]\tvalid_0's rmse: 1.48854\n",
      "[9]\tvalid_0's rmse: 1.342\n",
      "[10]\tvalid_0's rmse: 1.20986\n",
      "[11]\tvalid_0's rmse: 1.09073\n",
      "[12]\tvalid_0's rmse: 0.984099\n",
      "[13]\tvalid_0's rmse: 0.888207\n",
      "[14]\tvalid_0's rmse: 0.802065\n",
      "[15]\tvalid_0's rmse: 0.724762\n",
      "[16]\tvalid_0's rmse: 0.655346\n",
      "[17]\tvalid_0's rmse: 0.593109\n",
      "[18]\tvalid_0's rmse: 0.537425\n",
      "[19]\tvalid_0's rmse: 0.487317\n",
      "[20]\tvalid_0's rmse: 0.442534\n",
      "[21]\tvalid_0's rmse: 0.40238\n",
      "[22]\tvalid_0's rmse: 0.366743\n",
      "[23]\tvalid_0's rmse: 0.335003\n",
      "[24]\tvalid_0's rmse: 0.306516\n",
      "[25]\tvalid_0's rmse: 0.281475\n",
      "[26]\tvalid_0's rmse: 0.259333\n",
      "[27]\tvalid_0's rmse: 0.239441\n",
      "[28]\tvalid_0's rmse: 0.222359\n",
      "[29]\tvalid_0's rmse: 0.207038\n",
      "[30]\tvalid_0's rmse: 0.193949\n",
      "[31]\tvalid_0's rmse: 0.182559\n",
      "[32]\tvalid_0's rmse: 0.172284\n",
      "[33]\tvalid_0's rmse: 0.163764\n",
      "[34]\tvalid_0's rmse: 0.156102\n",
      "[35]\tvalid_0's rmse: 0.149514\n",
      "[36]\tvalid_0's rmse: 0.143772\n",
      "[37]\tvalid_0's rmse: 0.138929\n",
      "[38]\tvalid_0's rmse: 0.135015\n",
      "[39]\tvalid_0's rmse: 0.131429\n",
      "[40]\tvalid_0's rmse: 0.128392\n",
      "[41]\tvalid_0's rmse: 0.125883\n",
      "[42]\tvalid_0's rmse: 0.123803\n",
      "[43]\tvalid_0's rmse: 0.122052\n",
      "[44]\tvalid_0's rmse: 0.120515\n",
      "[45]\tvalid_0's rmse: 0.119395\n",
      "[46]\tvalid_0's rmse: 0.118309\n",
      "[47]\tvalid_0's rmse: 0.117582\n",
      "[48]\tvalid_0's rmse: 0.11669\n",
      "[49]\tvalid_0's rmse: 0.116057\n",
      "[50]\tvalid_0's rmse: 0.115437\n",
      "[51]\tvalid_0's rmse: 0.115128\n",
      "[52]\tvalid_0's rmse: 0.114533\n",
      "[53]\tvalid_0's rmse: 0.114124\n",
      "[54]\tvalid_0's rmse: 0.113693\n",
      "[55]\tvalid_0's rmse: 0.11336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[56]\tvalid_0's rmse: 0.113125\n",
      "[57]\tvalid_0's rmse: 0.11287\n",
      "[58]\tvalid_0's rmse: 0.112796\n",
      "[59]\tvalid_0's rmse: 0.112548\n",
      "[60]\tvalid_0's rmse: 0.112255\n",
      "[61]\tvalid_0's rmse: 0.112028\n",
      "[62]\tvalid_0's rmse: 0.111872\n",
      "[63]\tvalid_0's rmse: 0.111777\n",
      "[64]\tvalid_0's rmse: 0.111979\n",
      "[65]\tvalid_0's rmse: 0.111793\n",
      "[66]\tvalid_0's rmse: 0.111565\n",
      "[67]\tvalid_0's rmse: 0.111519\n",
      "[68]\tvalid_0's rmse: 0.111279\n",
      "[69]\tvalid_0's rmse: 0.11115\n",
      "[70]\tvalid_0's rmse: 0.111004\n",
      "[71]\tvalid_0's rmse: 0.110837\n",
      "[72]\tvalid_0's rmse: 0.110879\n",
      "[73]\tvalid_0's rmse: 0.110728\n",
      "[74]\tvalid_0's rmse: 0.110534\n",
      "[75]\tvalid_0's rmse: 0.110572\n",
      "[76]\tvalid_0's rmse: 0.110451\n",
      "[77]\tvalid_0's rmse: 0.110447\n",
      "[78]\tvalid_0's rmse: 0.110492\n",
      "[79]\tvalid_0's rmse: 0.110477\n",
      "[80]\tvalid_0's rmse: 0.110386\n",
      "[81]\tvalid_0's rmse: 0.110301\n",
      "[82]\tvalid_0's rmse: 0.110274\n",
      "[83]\tvalid_0's rmse: 0.110242\n",
      "[84]\tvalid_0's rmse: 0.110104\n",
      "[85]\tvalid_0's rmse: 0.109949\n",
      "[86]\tvalid_0's rmse: 0.10994\n",
      "[87]\tvalid_0's rmse: 0.109859\n",
      "[88]\tvalid_0's rmse: 0.109905\n",
      "[89]\tvalid_0's rmse: 0.109961\n",
      "[90]\tvalid_0's rmse: 0.109933\n",
      "[91]\tvalid_0's rmse: 0.109878\n",
      "[92]\tvalid_0's rmse: 0.109829\n",
      "[93]\tvalid_0's rmse: 0.109837\n",
      "[94]\tvalid_0's rmse: 0.109857\n",
      "[95]\tvalid_0's rmse: 0.10977\n",
      "[96]\tvalid_0's rmse: 0.109646\n",
      "[97]\tvalid_0's rmse: 0.109577\n",
      "[98]\tvalid_0's rmse: 0.109562\n",
      "[99]\tvalid_0's rmse: 0.109508\n",
      "[100]\tvalid_0's rmse: 0.109498\n",
      "[101]\tvalid_0's rmse: 0.109535\n",
      "[102]\tvalid_0's rmse: 0.10959\n",
      "[103]\tvalid_0's rmse: 0.109561\n",
      "[104]\tvalid_0's rmse: 0.10949\n",
      "[105]\tvalid_0's rmse: 0.109411\n",
      "[106]\tvalid_0's rmse: 0.109387\n",
      "[107]\tvalid_0's rmse: 0.109226\n",
      "[108]\tvalid_0's rmse: 0.109359\n",
      "[109]\tvalid_0's rmse: 0.109275\n",
      "[110]\tvalid_0's rmse: 0.109222\n",
      "[111]\tvalid_0's rmse: 0.109243\n",
      "[112]\tvalid_0's rmse: 0.109287\n",
      "[113]\tvalid_0's rmse: 0.109237\n",
      "[114]\tvalid_0's rmse: 0.109163\n",
      "[115]\tvalid_0's rmse: 0.109089\n",
      "[116]\tvalid_0's rmse: 0.109033\n",
      "[117]\tvalid_0's rmse: 0.10887\n",
      "[118]\tvalid_0's rmse: 0.108884\n",
      "[119]\tvalid_0's rmse: 0.108888\n",
      "[120]\tvalid_0's rmse: 0.108889\n",
      "[121]\tvalid_0's rmse: 0.10887\n",
      "[122]\tvalid_0's rmse: 0.108932\n",
      "[123]\tvalid_0's rmse: 0.108921\n",
      "[124]\tvalid_0's rmse: 0.10891\n",
      "[125]\tvalid_0's rmse: 0.108931\n",
      "[126]\tvalid_0's rmse: 0.108954\n",
      "[127]\tvalid_0's rmse: 0.108982\n",
      "[128]\tvalid_0's rmse: 0.108947\n",
      "[129]\tvalid_0's rmse: 0.109023\n",
      "[130]\tvalid_0's rmse: 0.109071\n",
      "[131]\tvalid_0's rmse: 0.109044\n",
      "[132]\tvalid_0's rmse: 0.109013\n",
      "[133]\tvalid_0's rmse: 0.109038\n",
      "[134]\tvalid_0's rmse: 0.108983\n",
      "[135]\tvalid_0's rmse: 0.108953\n",
      "[136]\tvalid_0's rmse: 0.108989\n",
      "[137]\tvalid_0's rmse: 0.108815\n",
      "[138]\tvalid_0's rmse: 0.108713\n",
      "[139]\tvalid_0's rmse: 0.108713\n",
      "[140]\tvalid_0's rmse: 0.108734\n",
      "[141]\tvalid_0's rmse: 0.108807\n",
      "[142]\tvalid_0's rmse: 0.108772\n",
      "[143]\tvalid_0's rmse: 0.10871\n",
      "[144]\tvalid_0's rmse: 0.108794\n",
      "[145]\tvalid_0's rmse: 0.108731\n",
      "[146]\tvalid_0's rmse: 0.108718\n",
      "[147]\tvalid_0's rmse: 0.108727\n",
      "[148]\tvalid_0's rmse: 0.108698\n",
      "[149]\tvalid_0's rmse: 0.108562\n",
      "[150]\tvalid_0's rmse: 0.108637\n",
      "[151]\tvalid_0's rmse: 0.108656\n",
      "[152]\tvalid_0's rmse: 0.108652\n",
      "[153]\tvalid_0's rmse: 0.108655\n",
      "[154]\tvalid_0's rmse: 0.108647\n",
      "[155]\tvalid_0's rmse: 0.108672\n",
      "[156]\tvalid_0's rmse: 0.108728\n",
      "[157]\tvalid_0's rmse: 0.108776\n",
      "[158]\tvalid_0's rmse: 0.108662\n",
      "[159]\tvalid_0's rmse: 0.108636\n",
      "[160]\tvalid_0's rmse: 0.108669\n",
      "[161]\tvalid_0's rmse: 0.108721\n",
      "[162]\tvalid_0's rmse: 0.108784\n",
      "[163]\tvalid_0's rmse: 0.108843\n",
      "[164]\tvalid_0's rmse: 0.108838\n",
      "[165]\tvalid_0's rmse: 0.108801\n",
      "[166]\tvalid_0's rmse: 0.108869\n",
      "[167]\tvalid_0's rmse: 0.1089\n",
      "[168]\tvalid_0's rmse: 0.108896\n",
      "[169]\tvalid_0's rmse: 0.108797\n",
      "[170]\tvalid_0's rmse: 0.108693\n",
      "[171]\tvalid_0's rmse: 0.108689\n",
      "[172]\tvalid_0's rmse: 0.108689\n",
      "[173]\tvalid_0's rmse: 0.108652\n",
      "[174]\tvalid_0's rmse: 0.108654\n",
      "[175]\tvalid_0's rmse: 0.108699\n",
      "[176]\tvalid_0's rmse: 0.108741\n",
      "[177]\tvalid_0's rmse: 0.108595\n",
      "[178]\tvalid_0's rmse: 0.108465\n",
      "[179]\tvalid_0's rmse: 0.108542\n",
      "[180]\tvalid_0's rmse: 0.108576\n",
      "[181]\tvalid_0's rmse: 0.108559\n",
      "[182]\tvalid_0's rmse: 0.108635\n",
      "[183]\tvalid_0's rmse: 0.108667\n",
      "[184]\tvalid_0's rmse: 0.108672\n",
      "[185]\tvalid_0's rmse: 0.108645\n",
      "[186]\tvalid_0's rmse: 0.108681\n",
      "[187]\tvalid_0's rmse: 0.108712\n",
      "[188]\tvalid_0's rmse: 0.108678\n",
      "[189]\tvalid_0's rmse: 0.108758\n",
      "[190]\tvalid_0's rmse: 0.108726\n",
      "[191]\tvalid_0's rmse: 0.108797\n",
      "[192]\tvalid_0's rmse: 0.108876\n",
      "[193]\tvalid_0's rmse: 0.108915\n",
      "[194]\tvalid_0's rmse: 0.10892\n",
      "[195]\tvalid_0's rmse: 0.108927\n",
      "[196]\tvalid_0's rmse: 0.108811\n",
      "[197]\tvalid_0's rmse: 0.108791\n",
      "[198]\tvalid_0's rmse: 0.108828\n",
      "[199]\tvalid_0's rmse: 0.108818\n",
      "[200]\tvalid_0's rmse: 0.108773\n",
      "[201]\tvalid_0's rmse: 0.108825\n",
      "[202]\tvalid_0's rmse: 0.108823\n",
      "[203]\tvalid_0's rmse: 0.108904\n",
      "[204]\tvalid_0's rmse: 0.108926\n",
      "[205]\tvalid_0's rmse: 0.108869\n",
      "[206]\tvalid_0's rmse: 0.108878\n",
      "[207]\tvalid_0's rmse: 0.108894\n",
      "[208]\tvalid_0's rmse: 0.108905\n",
      "[209]\tvalid_0's rmse: 0.108987\n",
      "[210]\tvalid_0's rmse: 0.109032\n",
      "[211]\tvalid_0's rmse: 0.108992\n",
      "[212]\tvalid_0's rmse: 0.109091\n",
      "[213]\tvalid_0's rmse: 0.109133\n",
      "[214]\tvalid_0's rmse: 0.109167\n",
      "[215]\tvalid_0's rmse: 0.109208\n",
      "[216]\tvalid_0's rmse: 0.109299\n",
      "[217]\tvalid_0's rmse: 0.109177\n",
      "[218]\tvalid_0's rmse: 0.109239\n",
      "[219]\tvalid_0's rmse: 0.109259\n",
      "[220]\tvalid_0's rmse: 0.109253\n",
      "[221]\tvalid_0's rmse: 0.109227\n",
      "[222]\tvalid_0's rmse: 0.109188\n",
      "[223]\tvalid_0's rmse: 0.109199\n",
      "[224]\tvalid_0's rmse: 0.109202\n",
      "[225]\tvalid_0's rmse: 0.109184\n",
      "[226]\tvalid_0's rmse: 0.10919\n",
      "[227]\tvalid_0's rmse: 0.109234\n",
      "[228]\tvalid_0's rmse: 0.109238\n",
      "[229]\tvalid_0's rmse: 0.10928\n",
      "[230]\tvalid_0's rmse: 0.109345\n",
      "[231]\tvalid_0's rmse: 0.10938\n",
      "[232]\tvalid_0's rmse: 0.109442\n",
      "[233]\tvalid_0's rmse: 0.109391\n",
      "[234]\tvalid_0's rmse: 0.109405\n",
      "[235]\tvalid_0's rmse: 0.109381\n",
      "[236]\tvalid_0's rmse: 0.109312\n",
      "[237]\tvalid_0's rmse: 0.109332\n",
      "[238]\tvalid_0's rmse: 0.10931\n",
      "[239]\tvalid_0's rmse: 0.10937\n",
      "[240]\tvalid_0's rmse: 0.109419\n",
      "[241]\tvalid_0's rmse: 0.109412\n",
      "[242]\tvalid_0's rmse: 0.109354\n",
      "[243]\tvalid_0's rmse: 0.10943\n",
      "[244]\tvalid_0's rmse: 0.109431\n",
      "[245]\tvalid_0's rmse: 0.109496\n",
      "[246]\tvalid_0's rmse: 0.109525\n",
      "[247]\tvalid_0's rmse: 0.109462\n",
      "[248]\tvalid_0's rmse: 0.109423\n",
      "[249]\tvalid_0's rmse: 0.109419\n",
      "[250]\tvalid_0's rmse: 0.10932\n",
      "[251]\tvalid_0's rmse: 0.109358\n",
      "[252]\tvalid_0's rmse: 0.109332\n",
      "[253]\tvalid_0's rmse: 0.10925\n",
      "[254]\tvalid_0's rmse: 0.109256\n",
      "[255]\tvalid_0's rmse: 0.109285\n",
      "[256]\tvalid_0's rmse: 0.109274\n",
      "[257]\tvalid_0's rmse: 0.109212\n",
      "[258]\tvalid_0's rmse: 0.109217\n",
      "[259]\tvalid_0's rmse: 0.109306\n",
      "[260]\tvalid_0's rmse: 0.109372\n",
      "[261]\tvalid_0's rmse: 0.10928\n",
      "[262]\tvalid_0's rmse: 0.109341\n",
      "[263]\tvalid_0's rmse: 0.109389\n",
      "[264]\tvalid_0's rmse: 0.109395\n",
      "[265]\tvalid_0's rmse: 0.109377\n",
      "[266]\tvalid_0's rmse: 0.109387\n",
      "[267]\tvalid_0's rmse: 0.109367\n",
      "[268]\tvalid_0's rmse: 0.109354\n",
      "[269]\tvalid_0's rmse: 0.109383\n",
      "[270]\tvalid_0's rmse: 0.109472\n",
      "[271]\tvalid_0's rmse: 0.109491\n",
      "[272]\tvalid_0's rmse: 0.109443\n",
      "[273]\tvalid_0's rmse: 0.1095\n",
      "[274]\tvalid_0's rmse: 0.109426\n",
      "[275]\tvalid_0's rmse: 0.109411\n",
      "[276]\tvalid_0's rmse: 0.109438\n",
      "[277]\tvalid_0's rmse: 0.109498\n",
      "[278]\tvalid_0's rmse: 0.109489\n",
      "Early stopping, best iteration is:\n",
      "[178]\tvalid_0's rmse: 0.108465\n",
      "测试集 RMSE： 0.9139130370819927\n",
      "第7次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.12652\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.81537\n",
      "[3]\tvalid_0's rmse: 2.53545\n",
      "[4]\tvalid_0's rmse: 2.28399\n",
      "[5]\tvalid_0's rmse: 2.05708\n",
      "[6]\tvalid_0's rmse: 1.85316\n",
      "[7]\tvalid_0's rmse: 1.66982\n",
      "[8]\tvalid_0's rmse: 1.50501\n",
      "[9]\tvalid_0's rmse: 1.35684\n",
      "[10]\tvalid_0's rmse: 1.22369\n",
      "[11]\tvalid_0's rmse: 1.10344\n",
      "[12]\tvalid_0's rmse: 0.995661\n",
      "[13]\tvalid_0's rmse: 0.898674\n",
      "[14]\tvalid_0's rmse: 0.811718\n",
      "[15]\tvalid_0's rmse: 0.73355\n",
      "[16]\tvalid_0's rmse: 0.663435\n",
      "[17]\tvalid_0's rmse: 0.60075\n",
      "[18]\tvalid_0's rmse: 0.544795\n",
      "[19]\tvalid_0's rmse: 0.494235\n",
      "[20]\tvalid_0's rmse: 0.449325\n",
      "[21]\tvalid_0's rmse: 0.409076\n",
      "[22]\tvalid_0's rmse: 0.372956\n",
      "[23]\tvalid_0's rmse: 0.340762\n",
      "[24]\tvalid_0's rmse: 0.312509\n",
      "[25]\tvalid_0's rmse: 0.286946\n",
      "[26]\tvalid_0's rmse: 0.264553\n",
      "[27]\tvalid_0's rmse: 0.244662\n",
      "[28]\tvalid_0's rmse: 0.22711\n",
      "[29]\tvalid_0's rmse: 0.211799\n",
      "[30]\tvalid_0's rmse: 0.198191\n",
      "[31]\tvalid_0's rmse: 0.186446\n",
      "[32]\tvalid_0's rmse: 0.176436\n",
      "[33]\tvalid_0's rmse: 0.16774\n",
      "[34]\tvalid_0's rmse: 0.160106\n",
      "[35]\tvalid_0's rmse: 0.153654\n",
      "[36]\tvalid_0's rmse: 0.147851\n",
      "[37]\tvalid_0's rmse: 0.143038\n",
      "[38]\tvalid_0's rmse: 0.138952\n",
      "[39]\tvalid_0's rmse: 0.135542\n",
      "[40]\tvalid_0's rmse: 0.132384\n",
      "[41]\tvalid_0's rmse: 0.130011\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42]\tvalid_0's rmse: 0.127679\n",
      "[43]\tvalid_0's rmse: 0.12576\n",
      "[44]\tvalid_0's rmse: 0.124351\n",
      "[45]\tvalid_0's rmse: 0.122858\n",
      "[46]\tvalid_0's rmse: 0.121222\n",
      "[47]\tvalid_0's rmse: 0.120319\n",
      "[48]\tvalid_0's rmse: 0.119285\n",
      "[49]\tvalid_0's rmse: 0.118341\n",
      "[50]\tvalid_0's rmse: 0.117625\n",
      "[51]\tvalid_0's rmse: 0.11695\n",
      "[52]\tvalid_0's rmse: 0.116462\n",
      "[53]\tvalid_0's rmse: 0.116174\n",
      "[54]\tvalid_0's rmse: 0.115713\n",
      "[55]\tvalid_0's rmse: 0.115061\n",
      "[56]\tvalid_0's rmse: 0.114632\n",
      "[57]\tvalid_0's rmse: 0.114328\n",
      "[58]\tvalid_0's rmse: 0.113838\n",
      "[59]\tvalid_0's rmse: 0.113705\n",
      "[60]\tvalid_0's rmse: 0.113373\n",
      "[61]\tvalid_0's rmse: 0.113052\n",
      "[62]\tvalid_0's rmse: 0.112937\n",
      "[63]\tvalid_0's rmse: 0.112604\n",
      "[64]\tvalid_0's rmse: 0.112509\n",
      "[65]\tvalid_0's rmse: 0.112261\n",
      "[66]\tvalid_0's rmse: 0.112241\n",
      "[67]\tvalid_0's rmse: 0.112053\n",
      "[68]\tvalid_0's rmse: 0.111873\n",
      "[69]\tvalid_0's rmse: 0.111788\n",
      "[70]\tvalid_0's rmse: 0.111704\n",
      "[71]\tvalid_0's rmse: 0.111773\n",
      "[72]\tvalid_0's rmse: 0.111612\n",
      "[73]\tvalid_0's rmse: 0.111273\n",
      "[74]\tvalid_0's rmse: 0.111226\n",
      "[75]\tvalid_0's rmse: 0.111292\n",
      "[76]\tvalid_0's rmse: 0.111093\n",
      "[77]\tvalid_0's rmse: 0.110952\n",
      "[78]\tvalid_0's rmse: 0.110835\n",
      "[79]\tvalid_0's rmse: 0.110745\n",
      "[80]\tvalid_0's rmse: 0.110615\n",
      "[81]\tvalid_0's rmse: 0.110676\n",
      "[82]\tvalid_0's rmse: 0.110629\n",
      "[83]\tvalid_0's rmse: 0.110624\n",
      "[84]\tvalid_0's rmse: 0.110637\n",
      "[85]\tvalid_0's rmse: 0.110574\n",
      "[86]\tvalid_0's rmse: 0.110496\n",
      "[87]\tvalid_0's rmse: 0.110505\n",
      "[88]\tvalid_0's rmse: 0.110548\n",
      "[89]\tvalid_0's rmse: 0.110547\n",
      "[90]\tvalid_0's rmse: 0.110653\n",
      "[91]\tvalid_0's rmse: 0.110684\n",
      "[92]\tvalid_0's rmse: 0.110781\n",
      "[93]\tvalid_0's rmse: 0.110768\n",
      "[94]\tvalid_0's rmse: 0.110669\n",
      "[95]\tvalid_0's rmse: 0.110543\n",
      "[96]\tvalid_0's rmse: 0.110517\n",
      "[97]\tvalid_0's rmse: 0.110558\n",
      "[98]\tvalid_0's rmse: 0.110489\n",
      "[99]\tvalid_0's rmse: 0.110519\n",
      "[100]\tvalid_0's rmse: 0.110538\n",
      "[101]\tvalid_0's rmse: 0.11054\n",
      "[102]\tvalid_0's rmse: 0.110574\n",
      "[103]\tvalid_0's rmse: 0.110506\n",
      "[104]\tvalid_0's rmse: 0.110488\n",
      "[105]\tvalid_0's rmse: 0.110423\n",
      "[106]\tvalid_0's rmse: 0.1104\n",
      "[107]\tvalid_0's rmse: 0.110406\n",
      "[108]\tvalid_0's rmse: 0.110606\n",
      "[109]\tvalid_0's rmse: 0.110649\n",
      "[110]\tvalid_0's rmse: 0.110712\n",
      "[111]\tvalid_0's rmse: 0.110815\n",
      "[112]\tvalid_0's rmse: 0.110847\n",
      "[113]\tvalid_0's rmse: 0.110802\n",
      "[114]\tvalid_0's rmse: 0.1108\n",
      "[115]\tvalid_0's rmse: 0.110786\n",
      "[116]\tvalid_0's rmse: 0.11063\n",
      "[117]\tvalid_0's rmse: 0.110687\n",
      "[118]\tvalid_0's rmse: 0.110685\n",
      "[119]\tvalid_0's rmse: 0.110816\n",
      "[120]\tvalid_0's rmse: 0.110827\n",
      "[121]\tvalid_0's rmse: 0.110901\n",
      "[122]\tvalid_0's rmse: 0.110816\n",
      "[123]\tvalid_0's rmse: 0.110825\n",
      "[124]\tvalid_0's rmse: 0.110767\n",
      "[125]\tvalid_0's rmse: 0.110792\n",
      "[126]\tvalid_0's rmse: 0.110783\n",
      "[127]\tvalid_0's rmse: 0.11078\n",
      "[128]\tvalid_0's rmse: 0.110896\n",
      "[129]\tvalid_0's rmse: 0.110883\n",
      "[130]\tvalid_0's rmse: 0.110922\n",
      "[131]\tvalid_0's rmse: 0.110913\n",
      "[132]\tvalid_0's rmse: 0.111012\n",
      "[133]\tvalid_0's rmse: 0.110863\n",
      "[134]\tvalid_0's rmse: 0.11076\n",
      "[135]\tvalid_0's rmse: 0.110766\n",
      "[136]\tvalid_0's rmse: 0.110759\n",
      "[137]\tvalid_0's rmse: 0.110812\n",
      "[138]\tvalid_0's rmse: 0.110875\n",
      "[139]\tvalid_0's rmse: 0.110885\n",
      "[140]\tvalid_0's rmse: 0.111061\n",
      "[141]\tvalid_0's rmse: 0.111163\n",
      "[142]\tvalid_0's rmse: 0.111039\n",
      "[143]\tvalid_0's rmse: 0.111062\n",
      "[144]\tvalid_0's rmse: 0.111001\n",
      "[145]\tvalid_0's rmse: 0.111055\n",
      "[146]\tvalid_0's rmse: 0.110979\n",
      "[147]\tvalid_0's rmse: 0.110994\n",
      "[148]\tvalid_0's rmse: 0.110963\n",
      "[149]\tvalid_0's rmse: 0.110974\n",
      "[150]\tvalid_0's rmse: 0.111033\n",
      "[151]\tvalid_0's rmse: 0.111008\n",
      "[152]\tvalid_0's rmse: 0.110948\n",
      "[153]\tvalid_0's rmse: 0.110933\n",
      "[154]\tvalid_0's rmse: 0.111044\n",
      "[155]\tvalid_0's rmse: 0.111079\n",
      "[156]\tvalid_0's rmse: 0.111079\n",
      "[157]\tvalid_0's rmse: 0.111035\n",
      "[158]\tvalid_0's rmse: 0.11097\n",
      "[159]\tvalid_0's rmse: 0.110898\n",
      "[160]\tvalid_0's rmse: 0.110913\n",
      "[161]\tvalid_0's rmse: 0.110848\n",
      "[162]\tvalid_0's rmse: 0.110823\n",
      "[163]\tvalid_0's rmse: 0.110771\n",
      "[164]\tvalid_0's rmse: 0.110843\n",
      "[165]\tvalid_0's rmse: 0.110855\n",
      "[166]\tvalid_0's rmse: 0.11085\n",
      "[167]\tvalid_0's rmse: 0.110899\n",
      "[168]\tvalid_0's rmse: 0.110862\n",
      "[169]\tvalid_0's rmse: 0.110821\n",
      "[170]\tvalid_0's rmse: 0.110704\n",
      "[171]\tvalid_0's rmse: 0.110751\n",
      "[172]\tvalid_0's rmse: 0.110751\n",
      "[173]\tvalid_0's rmse: 0.110917\n",
      "[174]\tvalid_0's rmse: 0.110892\n",
      "[175]\tvalid_0's rmse: 0.110925\n",
      "[176]\tvalid_0's rmse: 0.110838\n",
      "[177]\tvalid_0's rmse: 0.110866\n",
      "[178]\tvalid_0's rmse: 0.110956\n",
      "[179]\tvalid_0's rmse: 0.11098\n",
      "[180]\tvalid_0's rmse: 0.111049\n",
      "[181]\tvalid_0's rmse: 0.111057\n",
      "[182]\tvalid_0's rmse: 0.111074\n",
      "[183]\tvalid_0's rmse: 0.111016\n",
      "[184]\tvalid_0's rmse: 0.111011\n",
      "[185]\tvalid_0's rmse: 0.111036\n",
      "[186]\tvalid_0's rmse: 0.111012\n",
      "[187]\tvalid_0's rmse: 0.11101\n",
      "[188]\tvalid_0's rmse: 0.111029\n",
      "[189]\tvalid_0's rmse: 0.111132\n",
      "[190]\tvalid_0's rmse: 0.111015\n",
      "[191]\tvalid_0's rmse: 0.110998\n",
      "[192]\tvalid_0's rmse: 0.110998\n",
      "[193]\tvalid_0's rmse: 0.111038\n",
      "[194]\tvalid_0's rmse: 0.111039\n",
      "[195]\tvalid_0's rmse: 0.110995\n",
      "[196]\tvalid_0's rmse: 0.110937\n",
      "[197]\tvalid_0's rmse: 0.110948\n",
      "[198]\tvalid_0's rmse: 0.110998\n",
      "[199]\tvalid_0's rmse: 0.111001\n",
      "[200]\tvalid_0's rmse: 0.111024\n",
      "[201]\tvalid_0's rmse: 0.111074\n",
      "[202]\tvalid_0's rmse: 0.111246\n",
      "[203]\tvalid_0's rmse: 0.111207\n",
      "[204]\tvalid_0's rmse: 0.111226\n",
      "[205]\tvalid_0's rmse: 0.111165\n",
      "[206]\tvalid_0's rmse: 0.111148\n",
      "Early stopping, best iteration is:\n",
      "[106]\tvalid_0's rmse: 0.1104\n",
      "测试集 RMSE： 0.9127644755078308\n",
      "第8次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.13338\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.82135\n",
      "[3]\tvalid_0's rmse: 2.54078\n",
      "[4]\tvalid_0's rmse: 2.2884\n",
      "[5]\tvalid_0's rmse: 2.06163\n",
      "[6]\tvalid_0's rmse: 1.85735\n",
      "[7]\tvalid_0's rmse: 1.67374\n",
      "[8]\tvalid_0's rmse: 1.50821\n",
      "[9]\tvalid_0's rmse: 1.35932\n",
      "[10]\tvalid_0's rmse: 1.22578\n",
      "[11]\tvalid_0's rmse: 1.10576\n",
      "[12]\tvalid_0's rmse: 0.997977\n",
      "[13]\tvalid_0's rmse: 0.901228\n",
      "[14]\tvalid_0's rmse: 0.814256\n",
      "[15]\tvalid_0's rmse: 0.736822\n",
      "[16]\tvalid_0's rmse: 0.667174\n",
      "[17]\tvalid_0's rmse: 0.604297\n",
      "[18]\tvalid_0's rmse: 0.548039\n",
      "[19]\tvalid_0's rmse: 0.498011\n",
      "[20]\tvalid_0's rmse: 0.453054\n",
      "[21]\tvalid_0's rmse: 0.413298\n",
      "[22]\tvalid_0's rmse: 0.377968\n",
      "[23]\tvalid_0's rmse: 0.346453\n",
      "[24]\tvalid_0's rmse: 0.318547\n",
      "[25]\tvalid_0's rmse: 0.293881\n",
      "[26]\tvalid_0's rmse: 0.271829\n",
      "[27]\tvalid_0's rmse: 0.252832\n",
      "[28]\tvalid_0's rmse: 0.236024\n",
      "[29]\tvalid_0's rmse: 0.221303\n",
      "[30]\tvalid_0's rmse: 0.208687\n",
      "[31]\tvalid_0's rmse: 0.197579\n",
      "[32]\tvalid_0's rmse: 0.18804\n",
      "[33]\tvalid_0's rmse: 0.179856\n",
      "[34]\tvalid_0's rmse: 0.172762\n",
      "[35]\tvalid_0's rmse: 0.166693\n",
      "[36]\tvalid_0's rmse: 0.161538\n",
      "[37]\tvalid_0's rmse: 0.157013\n",
      "[38]\tvalid_0's rmse: 0.153241\n",
      "[39]\tvalid_0's rmse: 0.149764\n",
      "[40]\tvalid_0's rmse: 0.14743\n",
      "[41]\tvalid_0's rmse: 0.14505\n",
      "[42]\tvalid_0's rmse: 0.142917\n",
      "[43]\tvalid_0's rmse: 0.141194\n",
      "[44]\tvalid_0's rmse: 0.139526\n",
      "[45]\tvalid_0's rmse: 0.138295\n",
      "[46]\tvalid_0's rmse: 0.137188\n",
      "[47]\tvalid_0's rmse: 0.136147\n",
      "[48]\tvalid_0's rmse: 0.135576\n",
      "[49]\tvalid_0's rmse: 0.134602\n",
      "[50]\tvalid_0's rmse: 0.133984\n",
      "[51]\tvalid_0's rmse: 0.13374\n",
      "[52]\tvalid_0's rmse: 0.133091\n",
      "[53]\tvalid_0's rmse: 0.13246\n",
      "[54]\tvalid_0's rmse: 0.132024\n",
      "[55]\tvalid_0's rmse: 0.131768\n",
      "[56]\tvalid_0's rmse: 0.131268\n",
      "[57]\tvalid_0's rmse: 0.13081\n",
      "[58]\tvalid_0's rmse: 0.130229\n",
      "[59]\tvalid_0's rmse: 0.130121\n",
      "[60]\tvalid_0's rmse: 0.12983\n",
      "[61]\tvalid_0's rmse: 0.129431\n",
      "[62]\tvalid_0's rmse: 0.12907\n",
      "[63]\tvalid_0's rmse: 0.128731\n",
      "[64]\tvalid_0's rmse: 0.128626\n",
      "[65]\tvalid_0's rmse: 0.128273\n",
      "[66]\tvalid_0's rmse: 0.128176\n",
      "[67]\tvalid_0's rmse: 0.127732\n",
      "[68]\tvalid_0's rmse: 0.127587\n",
      "[69]\tvalid_0's rmse: 0.127251\n",
      "[70]\tvalid_0's rmse: 0.126843\n",
      "[71]\tvalid_0's rmse: 0.126835\n",
      "[72]\tvalid_0's rmse: 0.126854\n",
      "[73]\tvalid_0's rmse: 0.12681\n",
      "[74]\tvalid_0's rmse: 0.126816\n",
      "[75]\tvalid_0's rmse: 0.126683\n",
      "[76]\tvalid_0's rmse: 0.126692\n",
      "[77]\tvalid_0's rmse: 0.126655\n",
      "[78]\tvalid_0's rmse: 0.126417\n",
      "[79]\tvalid_0's rmse: 0.126375\n",
      "[80]\tvalid_0's rmse: 0.126373\n",
      "[81]\tvalid_0's rmse: 0.126214\n",
      "[82]\tvalid_0's rmse: 0.126092\n",
      "[83]\tvalid_0's rmse: 0.125978\n",
      "[84]\tvalid_0's rmse: 0.125872\n",
      "[85]\tvalid_0's rmse: 0.125818\n",
      "[86]\tvalid_0's rmse: 0.125756\n",
      "[87]\tvalid_0's rmse: 0.125817\n",
      "[88]\tvalid_0's rmse: 0.125726\n",
      "[89]\tvalid_0's rmse: 0.125593\n",
      "[90]\tvalid_0's rmse: 0.125629\n",
      "[91]\tvalid_0's rmse: 0.125618\n",
      "[92]\tvalid_0's rmse: 0.125582\n",
      "[93]\tvalid_0's rmse: 0.125514\n",
      "[94]\tvalid_0's rmse: 0.125592\n",
      "[95]\tvalid_0's rmse: 0.125508\n",
      "[96]\tvalid_0's rmse: 0.125688\n",
      "[97]\tvalid_0's rmse: 0.125601\n",
      "[98]\tvalid_0's rmse: 0.125407\n",
      "[99]\tvalid_0's rmse: 0.12545\n",
      "[100]\tvalid_0's rmse: 0.125407\n",
      "[101]\tvalid_0's rmse: 0.125446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[102]\tvalid_0's rmse: 0.125305\n",
      "[103]\tvalid_0's rmse: 0.125234\n",
      "[104]\tvalid_0's rmse: 0.125154\n",
      "[105]\tvalid_0's rmse: 0.125182\n",
      "[106]\tvalid_0's rmse: 0.125232\n",
      "[107]\tvalid_0's rmse: 0.125102\n",
      "[108]\tvalid_0's rmse: 0.125076\n",
      "[109]\tvalid_0's rmse: 0.125105\n",
      "[110]\tvalid_0's rmse: 0.125037\n",
      "[111]\tvalid_0's rmse: 0.125131\n",
      "[112]\tvalid_0's rmse: 0.125166\n",
      "[113]\tvalid_0's rmse: 0.125159\n",
      "[114]\tvalid_0's rmse: 0.12512\n",
      "[115]\tvalid_0's rmse: 0.125124\n",
      "[116]\tvalid_0's rmse: 0.125113\n",
      "[117]\tvalid_0's rmse: 0.125051\n",
      "[118]\tvalid_0's rmse: 0.125108\n",
      "[119]\tvalid_0's rmse: 0.125056\n",
      "[120]\tvalid_0's rmse: 0.125058\n",
      "[121]\tvalid_0's rmse: 0.124992\n",
      "[122]\tvalid_0's rmse: 0.125128\n",
      "[123]\tvalid_0's rmse: 0.125101\n",
      "[124]\tvalid_0's rmse: 0.125099\n",
      "[125]\tvalid_0's rmse: 0.125033\n",
      "[126]\tvalid_0's rmse: 0.125012\n",
      "[127]\tvalid_0's rmse: 0.124947\n",
      "[128]\tvalid_0's rmse: 0.124887\n",
      "[129]\tvalid_0's rmse: 0.124894\n",
      "[130]\tvalid_0's rmse: 0.124835\n",
      "[131]\tvalid_0's rmse: 0.12479\n",
      "[132]\tvalid_0's rmse: 0.124795\n",
      "[133]\tvalid_0's rmse: 0.124752\n",
      "[134]\tvalid_0's rmse: 0.124763\n",
      "[135]\tvalid_0's rmse: 0.12471\n",
      "[136]\tvalid_0's rmse: 0.12473\n",
      "[137]\tvalid_0's rmse: 0.124787\n",
      "[138]\tvalid_0's rmse: 0.124775\n",
      "[139]\tvalid_0's rmse: 0.124756\n",
      "[140]\tvalid_0's rmse: 0.124763\n",
      "[141]\tvalid_0's rmse: 0.124616\n",
      "[142]\tvalid_0's rmse: 0.124674\n",
      "[143]\tvalid_0's rmse: 0.124625\n",
      "[144]\tvalid_0's rmse: 0.124591\n",
      "[145]\tvalid_0's rmse: 0.124534\n",
      "[146]\tvalid_0's rmse: 0.12459\n",
      "[147]\tvalid_0's rmse: 0.124504\n",
      "[148]\tvalid_0's rmse: 0.124526\n",
      "[149]\tvalid_0's rmse: 0.124585\n",
      "[150]\tvalid_0's rmse: 0.124546\n",
      "[151]\tvalid_0's rmse: 0.124625\n",
      "[152]\tvalid_0's rmse: 0.124553\n",
      "[153]\tvalid_0's rmse: 0.124573\n",
      "[154]\tvalid_0's rmse: 0.124602\n",
      "[155]\tvalid_0's rmse: 0.124559\n",
      "[156]\tvalid_0's rmse: 0.12445\n",
      "[157]\tvalid_0's rmse: 0.124432\n",
      "[158]\tvalid_0's rmse: 0.124247\n",
      "[159]\tvalid_0's rmse: 0.124311\n",
      "[160]\tvalid_0's rmse: 0.124196\n",
      "[161]\tvalid_0's rmse: 0.124192\n",
      "[162]\tvalid_0's rmse: 0.124201\n",
      "[163]\tvalid_0's rmse: 0.124133\n",
      "[164]\tvalid_0's rmse: 0.124199\n",
      "[165]\tvalid_0's rmse: 0.124211\n",
      "[166]\tvalid_0's rmse: 0.12421\n",
      "[167]\tvalid_0's rmse: 0.124233\n",
      "[168]\tvalid_0's rmse: 0.124189\n",
      "[169]\tvalid_0's rmse: 0.12433\n",
      "[170]\tvalid_0's rmse: 0.124322\n",
      "[171]\tvalid_0's rmse: 0.124251\n",
      "[172]\tvalid_0's rmse: 0.124214\n",
      "[173]\tvalid_0's rmse: 0.124254\n",
      "[174]\tvalid_0's rmse: 0.124272\n",
      "[175]\tvalid_0's rmse: 0.12426\n",
      "[176]\tvalid_0's rmse: 0.124303\n",
      "[177]\tvalid_0's rmse: 0.124304\n",
      "[178]\tvalid_0's rmse: 0.124204\n",
      "[179]\tvalid_0's rmse: 0.124262\n",
      "[180]\tvalid_0's rmse: 0.124225\n",
      "[181]\tvalid_0's rmse: 0.124225\n",
      "[182]\tvalid_0's rmse: 0.124181\n",
      "[183]\tvalid_0's rmse: 0.124218\n",
      "[184]\tvalid_0's rmse: 0.12415\n",
      "[185]\tvalid_0's rmse: 0.124134\n",
      "[186]\tvalid_0's rmse: 0.124136\n",
      "[187]\tvalid_0's rmse: 0.124165\n",
      "[188]\tvalid_0's rmse: 0.12417\n",
      "[189]\tvalid_0's rmse: 0.124133\n",
      "[190]\tvalid_0's rmse: 0.124143\n",
      "[191]\tvalid_0's rmse: 0.124145\n",
      "[192]\tvalid_0's rmse: 0.1241\n",
      "[193]\tvalid_0's rmse: 0.124024\n",
      "[194]\tvalid_0's rmse: 0.124012\n",
      "[195]\tvalid_0's rmse: 0.123996\n",
      "[196]\tvalid_0's rmse: 0.123935\n",
      "[197]\tvalid_0's rmse: 0.12391\n",
      "[198]\tvalid_0's rmse: 0.123931\n",
      "[199]\tvalid_0's rmse: 0.12403\n",
      "[200]\tvalid_0's rmse: 0.124047\n",
      "[201]\tvalid_0's rmse: 0.123967\n",
      "[202]\tvalid_0's rmse: 0.123938\n",
      "[203]\tvalid_0's rmse: 0.12397\n",
      "[204]\tvalid_0's rmse: 0.123985\n",
      "[205]\tvalid_0's rmse: 0.123951\n",
      "[206]\tvalid_0's rmse: 0.123926\n",
      "[207]\tvalid_0's rmse: 0.123948\n",
      "[208]\tvalid_0's rmse: 0.124074\n",
      "[209]\tvalid_0's rmse: 0.124065\n",
      "[210]\tvalid_0's rmse: 0.124019\n",
      "[211]\tvalid_0's rmse: 0.124016\n",
      "[212]\tvalid_0's rmse: 0.124031\n",
      "[213]\tvalid_0's rmse: 0.123955\n",
      "[214]\tvalid_0's rmse: 0.123999\n",
      "[215]\tvalid_0's rmse: 0.12399\n",
      "[216]\tvalid_0's rmse: 0.124042\n",
      "[217]\tvalid_0's rmse: 0.124063\n",
      "[218]\tvalid_0's rmse: 0.124001\n",
      "[219]\tvalid_0's rmse: 0.123993\n",
      "[220]\tvalid_0's rmse: 0.124002\n",
      "[221]\tvalid_0's rmse: 0.12395\n",
      "[222]\tvalid_0's rmse: 0.123955\n",
      "[223]\tvalid_0's rmse: 0.124031\n",
      "[224]\tvalid_0's rmse: 0.123959\n",
      "[225]\tvalid_0's rmse: 0.123956\n",
      "[226]\tvalid_0's rmse: 0.124069\n",
      "[227]\tvalid_0's rmse: 0.124001\n",
      "[228]\tvalid_0's rmse: 0.124019\n",
      "[229]\tvalid_0's rmse: 0.124028\n",
      "[230]\tvalid_0's rmse: 0.124013\n",
      "[231]\tvalid_0's rmse: 0.123926\n",
      "[232]\tvalid_0's rmse: 0.123947\n",
      "[233]\tvalid_0's rmse: 0.123915\n",
      "[234]\tvalid_0's rmse: 0.123916\n",
      "[235]\tvalid_0's rmse: 0.124012\n",
      "[236]\tvalid_0's rmse: 0.124015\n",
      "[237]\tvalid_0's rmse: 0.124106\n",
      "[238]\tvalid_0's rmse: 0.124098\n",
      "[239]\tvalid_0's rmse: 0.124118\n",
      "[240]\tvalid_0's rmse: 0.124102\n",
      "[241]\tvalid_0's rmse: 0.124109\n",
      "[242]\tvalid_0's rmse: 0.124092\n",
      "[243]\tvalid_0's rmse: 0.124056\n",
      "[244]\tvalid_0's rmse: 0.124048\n",
      "[245]\tvalid_0's rmse: 0.124024\n",
      "[246]\tvalid_0's rmse: 0.124021\n",
      "[247]\tvalid_0's rmse: 0.124064\n",
      "[248]\tvalid_0's rmse: 0.124065\n",
      "[249]\tvalid_0's rmse: 0.124044\n",
      "[250]\tvalid_0's rmse: 0.124028\n",
      "[251]\tvalid_0's rmse: 0.124015\n",
      "[252]\tvalid_0's rmse: 0.12401\n",
      "[253]\tvalid_0's rmse: 0.124034\n",
      "[254]\tvalid_0's rmse: 0.12396\n",
      "[255]\tvalid_0's rmse: 0.123996\n",
      "[256]\tvalid_0's rmse: 0.123945\n",
      "[257]\tvalid_0's rmse: 0.123957\n",
      "[258]\tvalid_0's rmse: 0.123936\n",
      "[259]\tvalid_0's rmse: 0.123984\n",
      "[260]\tvalid_0's rmse: 0.124059\n",
      "[261]\tvalid_0's rmse: 0.124188\n",
      "[262]\tvalid_0's rmse: 0.124185\n",
      "[263]\tvalid_0's rmse: 0.124138\n",
      "[264]\tvalid_0's rmse: 0.124098\n",
      "[265]\tvalid_0's rmse: 0.124116\n",
      "[266]\tvalid_0's rmse: 0.124193\n",
      "[267]\tvalid_0's rmse: 0.124186\n",
      "[268]\tvalid_0's rmse: 0.124191\n",
      "[269]\tvalid_0's rmse: 0.124209\n",
      "[270]\tvalid_0's rmse: 0.124188\n",
      "[271]\tvalid_0's rmse: 0.124172\n",
      "[272]\tvalid_0's rmse: 0.124145\n",
      "[273]\tvalid_0's rmse: 0.124162\n",
      "[274]\tvalid_0's rmse: 0.124157\n",
      "[275]\tvalid_0's rmse: 0.124142\n",
      "[276]\tvalid_0's rmse: 0.124147\n",
      "[277]\tvalid_0's rmse: 0.124128\n",
      "[278]\tvalid_0's rmse: 0.124164\n",
      "[279]\tvalid_0's rmse: 0.124159\n",
      "[280]\tvalid_0's rmse: 0.124143\n",
      "[281]\tvalid_0's rmse: 0.124168\n",
      "[282]\tvalid_0's rmse: 0.124236\n",
      "[283]\tvalid_0's rmse: 0.124196\n",
      "[284]\tvalid_0's rmse: 0.124188\n",
      "[285]\tvalid_0's rmse: 0.124195\n",
      "[286]\tvalid_0's rmse: 0.124117\n",
      "[287]\tvalid_0's rmse: 0.124106\n",
      "[288]\tvalid_0's rmse: 0.12412\n",
      "[289]\tvalid_0's rmse: 0.124148\n",
      "[290]\tvalid_0's rmse: 0.124128\n",
      "[291]\tvalid_0's rmse: 0.124136\n",
      "[292]\tvalid_0's rmse: 0.124199\n",
      "[293]\tvalid_0's rmse: 0.124144\n",
      "[294]\tvalid_0's rmse: 0.124227\n",
      "[295]\tvalid_0's rmse: 0.12422\n",
      "[296]\tvalid_0's rmse: 0.124218\n",
      "[297]\tvalid_0's rmse: 0.124215\n",
      "Early stopping, best iteration is:\n",
      "[197]\tvalid_0's rmse: 0.12391\n",
      "测试集 RMSE： 0.9051047425702483\n",
      "第9次交叉验证\n",
      "[1]\tvalid_0's rmse: 3.09953\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[2]\tvalid_0's rmse: 2.7911\n",
      "[3]\tvalid_0's rmse: 2.51375\n",
      "[4]\tvalid_0's rmse: 2.26346\n",
      "[5]\tvalid_0's rmse: 2.03821\n",
      "[6]\tvalid_0's rmse: 1.83595\n",
      "[7]\tvalid_0's rmse: 1.65395\n",
      "[8]\tvalid_0's rmse: 1.49039\n",
      "[9]\tvalid_0's rmse: 1.34308\n",
      "[10]\tvalid_0's rmse: 1.21064\n",
      "[11]\tvalid_0's rmse: 1.09139\n",
      "[12]\tvalid_0's rmse: 0.984266\n",
      "[13]\tvalid_0's rmse: 0.888351\n",
      "[14]\tvalid_0's rmse: 0.801934\n",
      "[15]\tvalid_0's rmse: 0.724231\n",
      "[16]\tvalid_0's rmse: 0.654617\n",
      "[17]\tvalid_0's rmse: 0.592597\n",
      "[18]\tvalid_0's rmse: 0.536798\n",
      "[19]\tvalid_0's rmse: 0.486548\n",
      "[20]\tvalid_0's rmse: 0.441642\n",
      "[21]\tvalid_0's rmse: 0.401439\n",
      "[22]\tvalid_0's rmse: 0.365655\n",
      "[23]\tvalid_0's rmse: 0.33372\n",
      "[24]\tvalid_0's rmse: 0.305201\n",
      "[25]\tvalid_0's rmse: 0.280078\n",
      "[26]\tvalid_0's rmse: 0.257471\n",
      "[27]\tvalid_0's rmse: 0.237315\n",
      "[28]\tvalid_0's rmse: 0.219513\n",
      "[29]\tvalid_0's rmse: 0.204205\n",
      "[30]\tvalid_0's rmse: 0.190667\n",
      "[31]\tvalid_0's rmse: 0.178952\n",
      "[32]\tvalid_0's rmse: 0.168746\n",
      "[33]\tvalid_0's rmse: 0.159661\n",
      "[34]\tvalid_0's rmse: 0.151641\n",
      "[35]\tvalid_0's rmse: 0.145001\n",
      "[36]\tvalid_0's rmse: 0.139369\n",
      "[37]\tvalid_0's rmse: 0.134383\n",
      "[38]\tvalid_0's rmse: 0.12971\n",
      "[39]\tvalid_0's rmse: 0.125999\n",
      "[40]\tvalid_0's rmse: 0.122809\n",
      "[41]\tvalid_0's rmse: 0.120573\n",
      "[42]\tvalid_0's rmse: 0.118012\n",
      "[43]\tvalid_0's rmse: 0.115725\n",
      "[44]\tvalid_0's rmse: 0.113799\n",
      "[45]\tvalid_0's rmse: 0.112294\n",
      "[46]\tvalid_0's rmse: 0.111035\n",
      "[47]\tvalid_0's rmse: 0.109916\n",
      "[48]\tvalid_0's rmse: 0.108927\n",
      "[49]\tvalid_0's rmse: 0.108262\n",
      "[50]\tvalid_0's rmse: 0.107403\n",
      "[51]\tvalid_0's rmse: 0.10661\n",
      "[52]\tvalid_0's rmse: 0.106072\n",
      "[53]\tvalid_0's rmse: 0.105548\n",
      "[54]\tvalid_0's rmse: 0.105249\n",
      "[55]\tvalid_0's rmse: 0.105106\n",
      "[56]\tvalid_0's rmse: 0.105049\n",
      "[57]\tvalid_0's rmse: 0.104618\n",
      "[58]\tvalid_0's rmse: 0.104182\n",
      "[59]\tvalid_0's rmse: 0.104191\n",
      "[60]\tvalid_0's rmse: 0.103851\n",
      "[61]\tvalid_0's rmse: 0.103897\n",
      "[62]\tvalid_0's rmse: 0.103687\n",
      "[63]\tvalid_0's rmse: 0.103476\n",
      "[64]\tvalid_0's rmse: 0.103206\n",
      "[65]\tvalid_0's rmse: 0.103015\n",
      "[66]\tvalid_0's rmse: 0.102891\n",
      "[67]\tvalid_0's rmse: 0.102714\n",
      "[68]\tvalid_0's rmse: 0.102413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69]\tvalid_0's rmse: 0.102119\n",
      "[70]\tvalid_0's rmse: 0.101869\n",
      "[71]\tvalid_0's rmse: 0.101609\n",
      "[72]\tvalid_0's rmse: 0.101551\n",
      "[73]\tvalid_0's rmse: 0.101381\n",
      "[74]\tvalid_0's rmse: 0.101225\n",
      "[75]\tvalid_0's rmse: 0.101051\n",
      "[76]\tvalid_0's rmse: 0.101126\n",
      "[77]\tvalid_0's rmse: 0.10104\n",
      "[78]\tvalid_0's rmse: 0.101216\n",
      "[79]\tvalid_0's rmse: 0.101095\n",
      "[80]\tvalid_0's rmse: 0.100967\n",
      "[81]\tvalid_0's rmse: 0.100922\n",
      "[82]\tvalid_0's rmse: 0.100802\n",
      "[83]\tvalid_0's rmse: 0.100734\n",
      "[84]\tvalid_0's rmse: 0.100882\n",
      "[85]\tvalid_0's rmse: 0.100697\n",
      "[86]\tvalid_0's rmse: 0.100567\n",
      "[87]\tvalid_0's rmse: 0.100606\n",
      "[88]\tvalid_0's rmse: 0.100493\n",
      "[89]\tvalid_0's rmse: 0.1008\n",
      "[90]\tvalid_0's rmse: 0.100687\n",
      "[91]\tvalid_0's rmse: 0.100661\n",
      "[92]\tvalid_0's rmse: 0.100585\n",
      "[93]\tvalid_0's rmse: 0.10053\n",
      "[94]\tvalid_0's rmse: 0.100542\n",
      "[95]\tvalid_0's rmse: 0.100377\n",
      "[96]\tvalid_0's rmse: 0.100383\n",
      "[97]\tvalid_0's rmse: 0.100283\n",
      "[98]\tvalid_0's rmse: 0.100377\n",
      "[99]\tvalid_0's rmse: 0.100185\n",
      "[100]\tvalid_0's rmse: 0.100108\n",
      "[101]\tvalid_0's rmse: 0.100001\n",
      "[102]\tvalid_0's rmse: 0.0999196\n",
      "[103]\tvalid_0's rmse: 0.0998252\n",
      "[104]\tvalid_0's rmse: 0.0996773\n",
      "[105]\tvalid_0's rmse: 0.0997573\n",
      "[106]\tvalid_0's rmse: 0.0996709\n",
      "[107]\tvalid_0's rmse: 0.0997919\n",
      "[108]\tvalid_0's rmse: 0.0997775\n",
      "[109]\tvalid_0's rmse: 0.0997359\n",
      "[110]\tvalid_0's rmse: 0.0996565\n",
      "[111]\tvalid_0's rmse: 0.0995673\n",
      "[112]\tvalid_0's rmse: 0.0995136\n",
      "[113]\tvalid_0's rmse: 0.0994668\n",
      "[114]\tvalid_0's rmse: 0.0996257\n",
      "[115]\tvalid_0's rmse: 0.0998828\n",
      "[116]\tvalid_0's rmse: 0.0999343\n",
      "[117]\tvalid_0's rmse: 0.0999261\n",
      "[118]\tvalid_0's rmse: 0.099768\n",
      "[119]\tvalid_0's rmse: 0.0996607\n",
      "[120]\tvalid_0's rmse: 0.0995153\n",
      "[121]\tvalid_0's rmse: 0.0994814\n",
      "[122]\tvalid_0's rmse: 0.0994681\n",
      "[123]\tvalid_0's rmse: 0.0993503\n",
      "[124]\tvalid_0's rmse: 0.0992912\n",
      "[125]\tvalid_0's rmse: 0.0992744\n",
      "[126]\tvalid_0's rmse: 0.0992973\n",
      "[127]\tvalid_0's rmse: 0.0992286\n",
      "[128]\tvalid_0's rmse: 0.099266\n",
      "[129]\tvalid_0's rmse: 0.0992301\n",
      "[130]\tvalid_0's rmse: 0.0991654\n",
      "[131]\tvalid_0's rmse: 0.099034\n",
      "[132]\tvalid_0's rmse: 0.0990777\n",
      "[133]\tvalid_0's rmse: 0.0989557\n",
      "[134]\tvalid_0's rmse: 0.098867\n",
      "[135]\tvalid_0's rmse: 0.0988673\n",
      "[136]\tvalid_0's rmse: 0.0989297\n",
      "[137]\tvalid_0's rmse: 0.0989725\n",
      "[138]\tvalid_0's rmse: 0.09893\n",
      "[139]\tvalid_0's rmse: 0.09888\n",
      "[140]\tvalid_0's rmse: 0.0989009\n",
      "[141]\tvalid_0's rmse: 0.0988565\n",
      "[142]\tvalid_0's rmse: 0.0988323\n",
      "[143]\tvalid_0's rmse: 0.0987358\n",
      "[144]\tvalid_0's rmse: 0.0986663\n",
      "[145]\tvalid_0's rmse: 0.0987259\n",
      "[146]\tvalid_0's rmse: 0.0988661\n",
      "[147]\tvalid_0's rmse: 0.0988689\n",
      "[148]\tvalid_0's rmse: 0.0989035\n",
      "[149]\tvalid_0's rmse: 0.0990485\n",
      "[150]\tvalid_0's rmse: 0.0990332\n",
      "[151]\tvalid_0's rmse: 0.0990603\n",
      "[152]\tvalid_0's rmse: 0.0990937\n",
      "[153]\tvalid_0's rmse: 0.0990609\n",
      "[154]\tvalid_0's rmse: 0.0989561\n",
      "[155]\tvalid_0's rmse: 0.098976\n",
      "[156]\tvalid_0's rmse: 0.0989207\n",
      "[157]\tvalid_0's rmse: 0.0989766\n",
      "[158]\tvalid_0's rmse: 0.0989834\n",
      "[159]\tvalid_0's rmse: 0.0989037\n",
      "[160]\tvalid_0's rmse: 0.0988127\n",
      "[161]\tvalid_0's rmse: 0.098782\n",
      "[162]\tvalid_0's rmse: 0.0987352\n",
      "[163]\tvalid_0's rmse: 0.0986875\n",
      "[164]\tvalid_0's rmse: 0.0986918\n",
      "[165]\tvalid_0's rmse: 0.0986345\n",
      "[166]\tvalid_0's rmse: 0.098697\n",
      "[167]\tvalid_0's rmse: 0.0987348\n",
      "[168]\tvalid_0's rmse: 0.0987386\n",
      "[169]\tvalid_0's rmse: 0.0987229\n",
      "[170]\tvalid_0's rmse: 0.0987131\n",
      "[171]\tvalid_0's rmse: 0.0987042\n",
      "[172]\tvalid_0's rmse: 0.0986943\n",
      "[173]\tvalid_0's rmse: 0.0986615\n",
      "[174]\tvalid_0's rmse: 0.0986478\n",
      "[175]\tvalid_0's rmse: 0.0986343\n",
      "[176]\tvalid_0's rmse: 0.0986133\n",
      "[177]\tvalid_0's rmse: 0.09857\n",
      "[178]\tvalid_0's rmse: 0.0985389\n",
      "[179]\tvalid_0's rmse: 0.0984279\n",
      "[180]\tvalid_0's rmse: 0.0984558\n",
      "[181]\tvalid_0's rmse: 0.0985831\n",
      "[182]\tvalid_0's rmse: 0.0987557\n",
      "[183]\tvalid_0's rmse: 0.0987825\n",
      "[184]\tvalid_0's rmse: 0.0990751\n",
      "[185]\tvalid_0's rmse: 0.0990001\n",
      "[186]\tvalid_0's rmse: 0.0992299\n",
      "[187]\tvalid_0's rmse: 0.0993112\n",
      "[188]\tvalid_0's rmse: 0.0993565\n",
      "[189]\tvalid_0's rmse: 0.0993224\n",
      "[190]\tvalid_0's rmse: 0.0993492\n",
      "[191]\tvalid_0's rmse: 0.0992284\n",
      "[192]\tvalid_0's rmse: 0.0991724\n",
      "[193]\tvalid_0's rmse: 0.0991322\n",
      "[194]\tvalid_0's rmse: 0.0991727\n",
      "[195]\tvalid_0's rmse: 0.0992293\n",
      "[196]\tvalid_0's rmse: 0.0992027\n",
      "[197]\tvalid_0's rmse: 0.099192\n",
      "[198]\tvalid_0's rmse: 0.099254\n",
      "[199]\tvalid_0's rmse: 0.0992682\n",
      "[200]\tvalid_0's rmse: 0.0992272\n",
      "[201]\tvalid_0's rmse: 0.0991383\n",
      "[202]\tvalid_0's rmse: 0.0991421\n",
      "[203]\tvalid_0's rmse: 0.0991402\n",
      "[204]\tvalid_0's rmse: 0.0991359\n",
      "[205]\tvalid_0's rmse: 0.0991828\n",
      "[206]\tvalid_0's rmse: 0.0993592\n",
      "[207]\tvalid_0's rmse: 0.0994294\n",
      "[208]\tvalid_0's rmse: 0.0995523\n",
      "[209]\tvalid_0's rmse: 0.0996389\n",
      "[210]\tvalid_0's rmse: 0.0996074\n",
      "[211]\tvalid_0's rmse: 0.0995696\n",
      "[212]\tvalid_0's rmse: 0.0997881\n",
      "[213]\tvalid_0's rmse: 0.099754\n",
      "[214]\tvalid_0's rmse: 0.0997621\n",
      "[215]\tvalid_0's rmse: 0.0996973\n",
      "[216]\tvalid_0's rmse: 0.0996443\n",
      "[217]\tvalid_0's rmse: 0.0996437\n",
      "[218]\tvalid_0's rmse: 0.0995319\n",
      "[219]\tvalid_0's rmse: 0.0995714\n",
      "[220]\tvalid_0's rmse: 0.0995344\n",
      "[221]\tvalid_0's rmse: 0.0997646\n",
      "[222]\tvalid_0's rmse: 0.0997608\n",
      "[223]\tvalid_0's rmse: 0.0997827\n",
      "[224]\tvalid_0's rmse: 0.0997531\n",
      "[225]\tvalid_0's rmse: 0.0997287\n",
      "[226]\tvalid_0's rmse: 0.0996576\n",
      "[227]\tvalid_0's rmse: 0.0996328\n",
      "[228]\tvalid_0's rmse: 0.0996074\n",
      "[229]\tvalid_0's rmse: 0.0996137\n",
      "[230]\tvalid_0's rmse: 0.0996364\n",
      "[231]\tvalid_0's rmse: 0.0996173\n",
      "[232]\tvalid_0's rmse: 0.0997562\n",
      "[233]\tvalid_0's rmse: 0.0997752\n",
      "[234]\tvalid_0's rmse: 0.0998464\n",
      "[235]\tvalid_0's rmse: 0.100116\n",
      "[236]\tvalid_0's rmse: 0.100131\n",
      "[237]\tvalid_0's rmse: 0.100245\n",
      "[238]\tvalid_0's rmse: 0.10044\n",
      "[239]\tvalid_0's rmse: 0.100423\n",
      "[240]\tvalid_0's rmse: 0.10043\n",
      "[241]\tvalid_0's rmse: 0.100399\n",
      "[242]\tvalid_0's rmse: 0.100469\n",
      "[243]\tvalid_0's rmse: 0.100496\n",
      "[244]\tvalid_0's rmse: 0.100455\n",
      "[245]\tvalid_0's rmse: 0.100492\n",
      "[246]\tvalid_0's rmse: 0.100454\n",
      "[247]\tvalid_0's rmse: 0.100468\n",
      "[248]\tvalid_0's rmse: 0.100516\n",
      "[249]\tvalid_0's rmse: 0.100482\n",
      "[250]\tvalid_0's rmse: 0.10057\n",
      "[251]\tvalid_0's rmse: 0.100588\n",
      "[252]\tvalid_0's rmse: 0.100541\n",
      "[253]\tvalid_0's rmse: 0.100532\n",
      "[254]\tvalid_0's rmse: 0.100472\n",
      "[255]\tvalid_0's rmse: 0.1007\n",
      "[256]\tvalid_0's rmse: 0.100761\n",
      "[257]\tvalid_0's rmse: 0.100742\n",
      "[258]\tvalid_0's rmse: 0.100718\n",
      "[259]\tvalid_0's rmse: 0.100699\n",
      "[260]\tvalid_0's rmse: 0.100684\n",
      "[261]\tvalid_0's rmse: 0.100931\n",
      "[262]\tvalid_0's rmse: 0.10089\n",
      "[263]\tvalid_0's rmse: 0.100938\n",
      "[264]\tvalid_0's rmse: 0.1009\n",
      "[265]\tvalid_0's rmse: 0.100897\n",
      "[266]\tvalid_0's rmse: 0.100909\n",
      "[267]\tvalid_0's rmse: 0.100946\n",
      "[268]\tvalid_0's rmse: 0.101103\n",
      "[269]\tvalid_0's rmse: 0.101157\n",
      "[270]\tvalid_0's rmse: 0.10118\n",
      "[271]\tvalid_0's rmse: 0.101243\n",
      "[272]\tvalid_0's rmse: 0.101276\n",
      "[273]\tvalid_0's rmse: 0.101311\n",
      "[274]\tvalid_0's rmse: 0.101309\n",
      "[275]\tvalid_0's rmse: 0.101311\n",
      "[276]\tvalid_0's rmse: 0.10125\n",
      "[277]\tvalid_0's rmse: 0.101308\n",
      "[278]\tvalid_0's rmse: 0.101345\n",
      "[279]\tvalid_0's rmse: 0.101352\n",
      "Early stopping, best iteration is:\n",
      "[179]\tvalid_0's rmse: 0.0984279\n",
      "测试集 RMSE： 0.9200801029244168\n",
      "0.9012543100185388\n"
     ]
    }
   ],
   "source": [
    "result1 = lgb_cv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgb_cv():\n",
    "    rmse_score = 0\n",
    "    cvFold = 10\n",
    "    result = submit.copy()\n",
    "    for i in range(cvFold):\n",
    "        print('第'+str(i)+'次交叉验证')\n",
    "        X_train, X_test, y_train, y_test = train_test_split(all_X_train, all_y_train, test_size=0.2, random_state=i)\n",
    "\n",
    "        xgb_clf = xgb.XGBRegressor(max_depth=5,learning_rate=0.1, n_estimators=1500,random_state=100*i+500, n_jobs=8)\n",
    "\n",
    "        xgb_clf.fit(X_train, y_train, eval_metric='rmse')\n",
    "\n",
    "        y_pred = xgb_clf.predict(X_test)\n",
    "        score = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "        y_pred_test = xgb_clf.predict(sub_data, num_iteration=lgb_clf.best_iteration_)\n",
    "        submit['发电量'] += y_pred_test\n",
    "\n",
    "        score += 1/(1 + math.sqrt(score))\n",
    "        rmse_score += score\n",
    "\n",
    "        print('测试集 RMSE：', score)\n",
    "\n",
    "    result['发电量'] = result['发电量']/cvFold\n",
    "\n",
    "    print(rmse_score/cvFold)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8918, 231)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pip_all():\n",
    "    pipelines = []\n",
    "    pipelines.append(('ScaledLR', Pipeline([('Scaler', StandardScaler()),('LR',LinearRegression())])))\n",
    "    pipelines.append(('ScaledLASSO', Pipeline([('Scaler', StandardScaler()),('LASSO', Lasso())])))\n",
    "\n",
    "    pipelines.append(('ScaledKNN', Pipeline([('Scaler', StandardScaler()),('KNN', KNeighborsRegressor())])))\n",
    "    pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "    pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "#     X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=i)\n",
    "    results = []\n",
    "    names = []\n",
    "    for name, model in pipelines:\n",
    "        kfold = KFold(n_splits=10, random_state=21)\n",
    "        cv_results = cross_val_score(model, all_X_train, all_y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "        results.append(cv_results)\n",
    "        names.append(name)\n",
    "        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "        print(msg)\n",
    "# def pip_all():\n",
    "#     result = submit.copy()\n",
    "#     pipelines = []\n",
    "#     pipelines.append(('ScaledCART', Pipeline([('Scaler', StandardScaler()),('CART', DecisionTreeRegressor())])))\n",
    "#     pipelines.append(('ScaledGBM', Pipeline([('Scaler', StandardScaler()),('GBM', GradientBoostingRegressor())])))\n",
    "\n",
    "#     results = []\n",
    "#     names = []\n",
    "#     for name, model in pipelines:\n",
    "#         kfold = KFold(n_splits=10, random_state=21)\n",
    "#         nmse = cross_val_score(model, all_X_train, all_y_train, cv=kfold, scoring='neg_mean_squared_error')\n",
    "\n",
    "#         result['发电量'] += model.predict(test_data)\n",
    "#         avg_mse = np.average(-nmse)\n",
    "#         scores = cal_score(-nmse)\n",
    "        \n",
    "#         avg_score = np.average(scores)\n",
    "\n",
    "#         print('MSE:', -nmse)\n",
    "#         print('Score:', scores)\n",
    "#         print('Average-MSE:', avg_mse, ' - Score:', avg_score, '\\n')\n",
    "#     result['发电量'] /= 6\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'KFold' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-36c830103f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresult3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpip_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-6fb26c7e933b>\u001b[0m in \u001b[0;36mpip_all\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpipelines\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mcv_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_X_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_y_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mnames\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program/enter/lib/python3.6/site-packages/sklearn/cross_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[0;34m(estimator, X, y, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch)\u001b[0m\n\u001b[1;32m   1579\u001b[0m                                               \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1580\u001b[0m                                               fit_params)\n\u001b[0;32m-> 1581\u001b[0;31m                       for train, test in cv)\n\u001b[0m\u001b[1;32m   1582\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'KFold' object is not iterable"
     ]
    }
   ],
   "source": [
    "result3 = pip_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_430 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_431 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_432 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_433 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_434 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 29.4262 - rmse: 4.4171 - val_loss: 6.9170 - val_rmse: 2.1524\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.4233 - rmse: 1.4338 - val_loss: 1.4374 - val_rmse: 0.9034\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.6878 - rmse: 0.5804 - val_loss: 0.3373 - val_rmse: 0.3572\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1708 - rmse: 0.2704 - val_loss: 0.2121 - val_rmse: 0.2339\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0819 - rmse: 0.1813 - val_loss: 0.1683 - val_rmse: 0.1723\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0530 - rmse: 0.1441 - val_loss: 0.1400 - val_rmse: 0.1484\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0441 - rmse: 0.1310 - val_loss: 0.1292 - val_rmse: 0.1347\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0377 - rmse: 0.1176 - val_loss: 0.1228 - val_rmse: 0.1266\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0319 - rmse: 0.1065 - val_loss: 0.1167 - val_rmse: 0.1191\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0299 - rmse: 0.1013 - val_loss: 0.1129 - val_rmse: 0.1179\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0281 - rmse: 0.0978 - val_loss: 0.1182 - val_rmse: 0.1371\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0955 - val_loss: 0.1047 - val_rmse: 0.1083\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0248 - rmse: 0.0902 - val_loss: 0.1009 - val_rmse: 0.1108\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0854 - val_loss: 0.1001 - val_rmse: 0.1021\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0817 - val_loss: 0.0957 - val_rmse: 0.0994\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0823 - val_loss: 0.0925 - val_rmse: 0.0901\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0203 - rmse: 0.0773 - val_loss: 0.0898 - val_rmse: 0.0902\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0198 - rmse: 0.0770 - val_loss: 0.0887 - val_rmse: 0.0927\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0194 - rmse: 0.0750 - val_loss: 0.0840 - val_rmse: 0.0877\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0183 - rmse: 0.0743 - val_loss: 0.0835 - val_rmse: 0.0851\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0173 - rmse: 0.0709 - val_loss: 0.0863 - val_rmse: 0.1005\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0185 - rmse: 0.0770 - val_loss: 0.0830 - val_rmse: 0.0862\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0165 - rmse: 0.0685 - val_loss: 0.0827 - val_rmse: 0.0893\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0163 - rmse: 0.0703 - val_loss: 0.0806 - val_rmse: 0.0824\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0163 - rmse: 0.0692 - val_loss: 0.0781 - val_rmse: 0.0796\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0158 - rmse: 0.0684 - val_loss: 0.0805 - val_rmse: 0.0793\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0155 - rmse: 0.0675 - val_loss: 0.0799 - val_rmse: 0.0779\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0151 - rmse: 0.0679 - val_loss: 0.0780 - val_rmse: 0.0804\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0150 - rmse: 0.0669 - val_loss: 0.0781 - val_rmse: 0.0774\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0141 - rmse: 0.0650 - val_loss: 0.0768 - val_rmse: 0.0722\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0142 - rmse: 0.0639 - val_loss: 0.0767 - val_rmse: 0.0727\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0138 - rmse: 0.0629 - val_loss: 0.0782 - val_rmse: 0.0779\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0140 - rmse: 0.0643 - val_loss: 0.0769 - val_rmse: 0.0719\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0145 - rmse: 0.0676 - val_loss: 0.0754 - val_rmse: 0.0767\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0140 - rmse: 0.0648 - val_loss: 0.0749 - val_rmse: 0.0694\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0129 - rmse: 0.0615 - val_loss: 0.0758 - val_rmse: 0.0854\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0130 - rmse: 0.0616 - val_loss: 0.0911 - val_rmse: 0.1342\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0153 - rmse: 0.0718 - val_loss: 0.0752 - val_rmse: 0.0737\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0129 - rmse: 0.0615 - val_loss: 0.0816 - val_rmse: 0.1029\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0135 - rmse: 0.0658 - val_loss: 0.0762 - val_rmse: 0.0812\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0133 - rmse: 0.0652 - val_loss: 0.0762 - val_rmse: 0.0792\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0132 - rmse: 0.0643 - val_loss: 0.0741 - val_rmse: 0.0702\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0123 - rmse: 0.0595 - val_loss: 0.0733 - val_rmse: 0.0731\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0130 - rmse: 0.0626 - val_loss: 0.0750 - val_rmse: 0.0783\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0142 - rmse: 0.0650 - val_loss: 0.0716 - val_rmse: 0.0700\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0132 - rmse: 0.0600 - val_loss: 0.0906 - val_rmse: 0.1223\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0153 - rmse: 0.0691 - val_loss: 0.0784 - val_rmse: 0.0994\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0143 - rmse: 0.0655 - val_loss: 0.0807 - val_rmse: 0.1052\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0140 - rmse: 0.0661 - val_loss: 0.0788 - val_rmse: 0.0926\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0119 - rmse: 0.0600 - val_loss: 0.0717 - val_rmse: 0.0642\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0116 - rmse: 0.0580 - val_loss: 0.0724 - val_rmse: 0.0689\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0112 - rmse: 0.0554 - val_loss: 0.0706 - val_rmse: 0.0648\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0113 - rmse: 0.0572 - val_loss: 0.0727 - val_rmse: 0.0713\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0117 - rmse: 0.0581 - val_loss: 0.0748 - val_rmse: 0.0822\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0131 - rmse: 0.0655 - val_loss: 0.0712 - val_rmse: 0.0699\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0120 - rmse: 0.0594 - val_loss: 0.0726 - val_rmse: 0.0726\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0119 - rmse: 0.0588 - val_loss: 0.0722 - val_rmse: 0.0761\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0137 - rmse: 0.0650 - val_loss: 0.0741 - val_rmse: 0.0864\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0126 - rmse: 0.0579 - val_loss: 0.0741 - val_rmse: 0.0804\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0128 - rmse: 0.0601 - val_loss: 0.0743 - val_rmse: 0.0791\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0122 - rmse: 0.0547 - val_loss: 0.0692 - val_rmse: 0.0728\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0131 - rmse: 0.0600 - val_loss: 0.0687 - val_rmse: 0.0683\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0154 - rmse: 0.0694 - val_loss: 0.0715 - val_rmse: 0.0735\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0131 - rmse: 0.0606 - val_loss: 0.0665 - val_rmse: 0.0607\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0114 - rmse: 0.0535 - val_loss: 0.0678 - val_rmse: 0.0638\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0118 - rmse: 0.0571 - val_loss: 0.0711 - val_rmse: 0.0844\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0134 - rmse: 0.0652 - val_loss: 0.0731 - val_rmse: 0.1033\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0143 - rmse: 0.0663 - val_loss: 0.0719 - val_rmse: 0.0818\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0134 - rmse: 0.0581 - val_loss: 0.0731 - val_rmse: 0.0878\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0122 - rmse: 0.0569 - val_loss: 0.0673 - val_rmse: 0.0613\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0120 - rmse: 0.0554 - val_loss: 0.0683 - val_rmse: 0.0693\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0120 - rmse: 0.0586 - val_loss: 0.0705 - val_rmse: 0.0829\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0112 - rmse: 0.0560 - val_loss: 0.0669 - val_rmse: 0.0602\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0115 - rmse: 0.0577 - val_loss: 0.0696 - val_rmse: 0.0816\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0114 - rmse: 0.0559 - val_loss: 0.0673 - val_rmse: 0.0657\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0105 - rmse: 0.0508 - val_loss: 0.0704 - val_rmse: 0.0731\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0109 - rmse: 0.0546 - val_loss: 0.0682 - val_rmse: 0.0614\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0110 - rmse: 0.0573 - val_loss: 0.0674 - val_rmse: 0.0654\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0107 - rmse: 0.0551 - val_loss: 0.0697 - val_rmse: 0.0696\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0107 - rmse: 0.0549 - val_loss: 0.0667 - val_rmse: 0.0635\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0106 - rmse: 0.0537 - val_loss: 0.0672 - val_rmse: 0.0648\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0106 - rmse: 0.0551 - val_loss: 0.0665 - val_rmse: 0.0659\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0099 - rmse: 0.0510 - val_loss: 0.0688 - val_rmse: 0.0690\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/100\n",
      " - 0s - loss: 0.0112 - rmse: 0.0574 - val_loss: 0.0670 - val_rmse: 0.0676\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0107 - rmse: 0.0545 - val_loss: 0.0681 - val_rmse: 0.0726\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0124 - rmse: 0.0640 - val_loss: 0.1012 - val_rmse: 0.1689\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0177 - rmse: 0.0811 - val_loss: 0.0675 - val_rmse: 0.0719\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0122 - rmse: 0.0581 - val_loss: 0.0659 - val_rmse: 0.0570\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0112 - rmse: 0.0516 - val_loss: 0.0645 - val_rmse: 0.0649\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0119 - rmse: 0.0520 - val_loss: 0.0660 - val_rmse: 0.0667\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0130 - rmse: 0.0519 - val_loss: 0.0664 - val_rmse: 0.0623\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0108 - rmse: 0.0539 - val_loss: 0.0668 - val_rmse: 0.0738\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0114 - rmse: 0.0598 - val_loss: 0.0648 - val_rmse: 0.0613\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0106 - rmse: 0.0560 - val_loss: 0.0693 - val_rmse: 0.0839\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0119 - rmse: 0.0595 - val_loss: 0.0672 - val_rmse: 0.0689\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0104 - rmse: 0.0535 - val_loss: 0.0666 - val_rmse: 0.0788\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0104 - rmse: 0.0549 - val_loss: 0.0653 - val_rmse: 0.0590\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0100 - rmse: 0.0513 - val_loss: 0.0648 - val_rmse: 0.0676\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0114 - rmse: 0.0598 - val_loss: 0.0661 - val_rmse: 0.0630\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0102 - rmse: 0.0534 - val_loss: 0.0662 - val_rmse: 0.0698\n",
      "0.06616067125251479\n",
      "测试集 rmse： 0.7954075285139683\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_435 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_436 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_437 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_438 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_439 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 29.7358 - rmse: 4.4436 - val_loss: 6.6596 - val_rmse: 2.1000\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.6410 - rmse: 1.5167 - val_loss: 1.5866 - val_rmse: 0.9827\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.7600 - rmse: 0.6328 - val_loss: 0.2468 - val_rmse: 0.3535\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1703 - rmse: 0.2685 - val_loss: 0.1145 - val_rmse: 0.2122\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1079 - rmse: 0.2043 - val_loss: 0.0824 - val_rmse: 0.1750\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0772 - rmse: 0.1637 - val_loss: 0.0680 - val_rmse: 0.1568\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0602 - rmse: 0.1395 - val_loss: 0.0527 - val_rmse: 0.1277\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0504 - rmse: 0.1242 - val_loss: 0.0499 - val_rmse: 0.1157\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0446 - rmse: 0.1169 - val_loss: 0.0525 - val_rmse: 0.1151\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0411 - rmse: 0.1092 - val_loss: 0.0532 - val_rmse: 0.1061\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0379 - rmse: 0.1013 - val_loss: 0.0547 - val_rmse: 0.1027\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0357 - rmse: 0.0973 - val_loss: 0.0620 - val_rmse: 0.1242\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0352 - rmse: 0.0975 - val_loss: 0.0605 - val_rmse: 0.1116\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0335 - rmse: 0.0931 - val_loss: 0.0587 - val_rmse: 0.1009\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0325 - rmse: 0.0912 - val_loss: 0.0576 - val_rmse: 0.0941\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0307 - rmse: 0.0855 - val_loss: 0.0601 - val_rmse: 0.0961\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0314 - rmse: 0.0886 - val_loss: 0.0640 - val_rmse: 0.1020\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0304 - rmse: 0.0862 - val_loss: 0.0599 - val_rmse: 0.0906\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0301 - rmse: 0.0847 - val_loss: 0.0619 - val_rmse: 0.0937\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0290 - rmse: 0.0814 - val_loss: 0.0647 - val_rmse: 0.0987\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0287 - rmse: 0.0804 - val_loss: 0.0642 - val_rmse: 0.0978\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0292 - rmse: 0.0812 - val_loss: 0.0605 - val_rmse: 0.0841\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0285 - rmse: 0.0784 - val_loss: 0.0643 - val_rmse: 0.0924\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0279 - rmse: 0.0765 - val_loss: 0.0630 - val_rmse: 0.0941\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0282 - rmse: 0.0761 - val_loss: 0.0621 - val_rmse: 0.0933\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0275 - rmse: 0.0790 - val_loss: 0.0593 - val_rmse: 0.0840\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0262 - rmse: 0.0725 - val_loss: 0.0611 - val_rmse: 0.0839\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0260 - rmse: 0.0721 - val_loss: 0.0653 - val_rmse: 0.1110\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0266 - rmse: 0.0759 - val_loss: 0.0608 - val_rmse: 0.0919\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0697 - val_loss: 0.0592 - val_rmse: 0.0840\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0676 - val_loss: 0.0684 - val_rmse: 0.1195\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0764 - val_loss: 0.0567 - val_rmse: 0.0834\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0260 - rmse: 0.0754 - val_loss: 0.0559 - val_rmse: 0.0776\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0244 - rmse: 0.0685 - val_loss: 0.0589 - val_rmse: 0.0869\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0715 - val_loss: 0.0537 - val_rmse: 0.0779\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0744 - val_loss: 0.0550 - val_rmse: 0.0825\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0269 - rmse: 0.0691 - val_loss: 0.0529 - val_rmse: 0.0723\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0645 - val_loss: 0.0547 - val_rmse: 0.0801\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0674 - val_loss: 0.0589 - val_rmse: 0.1012\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0727 - val_loss: 0.0671 - val_rmse: 0.1271\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0778 - val_loss: 0.0534 - val_rmse: 0.0773\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0240 - rmse: 0.0690 - val_loss: 0.0510 - val_rmse: 0.0758\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0723 - val_loss: 0.0546 - val_rmse: 0.0861\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0657 - val_loss: 0.0517 - val_rmse: 0.0774\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0640 - val_loss: 0.0598 - val_rmse: 0.1013\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0254 - rmse: 0.0741 - val_loss: 0.0496 - val_rmse: 0.0717\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0717 - val_loss: 0.0534 - val_rmse: 0.0823\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0240 - rmse: 0.0698 - val_loss: 0.0498 - val_rmse: 0.0678\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0663 - val_loss: 0.0509 - val_rmse: 0.0745\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0758 - val_loss: 0.0484 - val_rmse: 0.0712\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0622 - val_loss: 0.0542 - val_rmse: 0.0887\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0664 - val_loss: 0.0693 - val_rmse: 0.1328\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0634 - val_loss: 0.1353 - val_rmse: 0.2310\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0308 - rmse: 0.0920 - val_loss: 0.0493 - val_rmse: 0.0705\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0697 - val_loss: 0.0774 - val_rmse: 0.1547\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0712 - val_loss: 0.0538 - val_rmse: 0.0919\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0634 - val_loss: 0.0496 - val_rmse: 0.0667\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0653 - val_loss: 0.0497 - val_rmse: 0.0780\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0244 - rmse: 0.0670 - val_loss: 0.0486 - val_rmse: 0.0829\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0635 - val_loss: 0.0465 - val_rmse: 0.0621\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0591 - val_loss: 0.0494 - val_rmse: 0.0786\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0596 - val_loss: 0.0497 - val_rmse: 0.0723\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0686 - val_loss: 0.0489 - val_rmse: 0.0751\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0635 - val_loss: 0.0540 - val_rmse: 0.0984\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0618 - val_loss: 0.0488 - val_rmse: 0.0756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0642 - val_loss: 0.0470 - val_rmse: 0.0657\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0601 - val_loss: 0.0461 - val_rmse: 0.0606\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0657 - val_loss: 0.0463 - val_rmse: 0.0640\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0693 - val_loss: 0.0694 - val_rmse: 0.1351\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0257 - rmse: 0.0749 - val_loss: 0.0468 - val_rmse: 0.0659\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0571 - val_loss: 0.0487 - val_rmse: 0.0704\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0657 - val_loss: 0.0458 - val_rmse: 0.0643\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0561 - val_loss: 0.0502 - val_rmse: 0.0788\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0666 - val_loss: 0.0481 - val_rmse: 0.0763\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0774 - val_loss: 0.0452 - val_rmse: 0.0716\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0702 - val_loss: 0.0429 - val_rmse: 0.0599\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0650 - val_loss: 0.0476 - val_rmse: 0.0780\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0215 - rmse: 0.0599 - val_loss: 0.0445 - val_rmse: 0.0624\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0638 - val_loss: 0.0444 - val_rmse: 0.0592\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0572 - val_loss: 0.0476 - val_rmse: 0.0699\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0244 - rmse: 0.0731 - val_loss: 0.0465 - val_rmse: 0.0680\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0734 - val_loss: 0.0416 - val_rmse: 0.0661\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0570 - val_loss: 0.0602 - val_rmse: 0.1278\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0649 - val_loss: 0.0515 - val_rmse: 0.0984\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0240 - rmse: 0.0671 - val_loss: 0.0408 - val_rmse: 0.0563\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0621 - val_loss: 0.0451 - val_rmse: 0.0623\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0548 - val_loss: 0.0532 - val_rmse: 0.1140\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0627 - val_loss: 0.0560 - val_rmse: 0.1029\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0636 - val_loss: 0.0656 - val_rmse: 0.1318\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0661 - val_loss: 0.0441 - val_rmse: 0.0676\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0210 - rmse: 0.0592 - val_loss: 0.0424 - val_rmse: 0.0611\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0613 - val_loss: 0.0503 - val_rmse: 0.0885\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0589 - val_loss: 0.0435 - val_rmse: 0.0649\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0211 - rmse: 0.0579 - val_loss: 0.0475 - val_rmse: 0.0786\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0640 - val_loss: 0.0430 - val_rmse: 0.0624\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0682 - val_loss: 0.0444 - val_rmse: 0.0651\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0628 - val_loss: 0.0592 - val_rmse: 0.1264\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0296 - rmse: 0.0881 - val_loss: 0.0454 - val_rmse: 0.0751\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0600 - val_loss: 0.0410 - val_rmse: 0.0578\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0608 - val_loss: 0.0431 - val_rmse: 0.0571\n",
      "0.04309238899732605\n",
      "测试集 rmse： 0.8280976420635573\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_440 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_441 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_442 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_443 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_444 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 6s - loss: 31.8012 - rmse: 4.6496 - val_loss: 6.7819 - val_rmse: 2.1716\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.2464 - rmse: 1.6413 - val_loss: 1.7621 - val_rmse: 1.0614\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.9634 - rmse: 0.7226 - val_loss: 0.2914 - val_rmse: 0.3874\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1796 - rmse: 0.2694 - val_loss: 0.0962 - val_rmse: 0.2035\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0947 - rmse: 0.1770 - val_loss: 0.0578 - val_rmse: 0.1540\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0719 - rmse: 0.1488 - val_loss: 0.0447 - val_rmse: 0.1335\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0596 - rmse: 0.1335 - val_loss: 0.0367 - val_rmse: 0.1232\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0528 - rmse: 0.1218 - val_loss: 0.0314 - val_rmse: 0.1122\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0476 - rmse: 0.1149 - val_loss: 0.0283 - val_rmse: 0.1055\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0424 - rmse: 0.1044 - val_loss: 0.0265 - val_rmse: 0.1011\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0400 - rmse: 0.1001 - val_loss: 0.0306 - val_rmse: 0.1176\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0383 - rmse: 0.0989 - val_loss: 0.0227 - val_rmse: 0.0932\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0366 - rmse: 0.0956 - val_loss: 0.0216 - val_rmse: 0.0905\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0344 - rmse: 0.0893 - val_loss: 0.0203 - val_rmse: 0.0858\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0335 - rmse: 0.0892 - val_loss: 0.0225 - val_rmse: 0.0920\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0338 - rmse: 0.0920 - val_loss: 0.0199 - val_rmse: 0.0826\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0317 - rmse: 0.0840 - val_loss: 0.0197 - val_rmse: 0.0814\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0306 - rmse: 0.0822 - val_loss: 0.0206 - val_rmse: 0.0866\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0305 - rmse: 0.0803 - val_loss: 0.0201 - val_rmse: 0.0844\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0310 - rmse: 0.0851 - val_loss: 0.0276 - val_rmse: 0.1159\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0297 - rmse: 0.0805 - val_loss: 0.0190 - val_rmse: 0.0816\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0293 - rmse: 0.0798 - val_loss: 0.0209 - val_rmse: 0.0856\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0290 - rmse: 0.0802 - val_loss: 0.0260 - val_rmse: 0.1074\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0294 - rmse: 0.0816 - val_loss: 0.0202 - val_rmse: 0.0888\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0273 - rmse: 0.0752 - val_loss: 0.0192 - val_rmse: 0.0739\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0747 - val_loss: 0.0181 - val_rmse: 0.0744\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0762 - val_loss: 0.0176 - val_rmse: 0.0718\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0280 - rmse: 0.0755 - val_loss: 0.0300 - val_rmse: 0.1150\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0269 - rmse: 0.0756 - val_loss: 0.0178 - val_rmse: 0.0696\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0265 - rmse: 0.0747 - val_loss: 0.0241 - val_rmse: 0.1003\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0281 - rmse: 0.0788 - val_loss: 0.0214 - val_rmse: 0.0895\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0266 - rmse: 0.0703 - val_loss: 0.0240 - val_rmse: 0.0959\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0712 - val_loss: 0.0180 - val_rmse: 0.0737\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0705 - val_loss: 0.0175 - val_rmse: 0.0690\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0709 - val_loss: 0.0187 - val_rmse: 0.0767\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0741 - val_loss: 0.0166 - val_rmse: 0.0664\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0671 - val_loss: 0.0175 - val_rmse: 0.0650\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0653 - val_loss: 0.0187 - val_rmse: 0.0718\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0689 - val_loss: 0.0207 - val_rmse: 0.0822\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0745 - val_loss: 0.0175 - val_rmse: 0.0696\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0679 - val_loss: 0.0184 - val_rmse: 0.0753\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0623 - val_loss: 0.0203 - val_rmse: 0.0851\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0760 - val_loss: 0.0190 - val_rmse: 0.0769\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0689 - val_loss: 0.0169 - val_rmse: 0.0652\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0691 - val_loss: 0.0188 - val_rmse: 0.0760\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0658 - val_loss: 0.0188 - val_rmse: 0.0749\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0675 - val_loss: 0.0199 - val_rmse: 0.0803\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0698 - val_loss: 0.0169 - val_rmse: 0.0652\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0215 - rmse: 0.0586 - val_loss: 0.0184 - val_rmse: 0.0673\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0670 - val_loss: 0.0164 - val_rmse: 0.0600\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0681 - val_loss: 0.0163 - val_rmse: 0.0602\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0586 - val_loss: 0.0188 - val_rmse: 0.0717\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0677 - val_loss: 0.0302 - val_rmse: 0.1121\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0691 - val_loss: 0.0229 - val_rmse: 0.0960\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0692 - val_loss: 0.0162 - val_rmse: 0.0585\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0661 - val_loss: 0.0177 - val_rmse: 0.0624\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0240 - rmse: 0.0682 - val_loss: 0.0186 - val_rmse: 0.0637\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0654 - val_loss: 0.0185 - val_rmse: 0.0709\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0572 - val_loss: 0.0296 - val_rmse: 0.1153\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0659 - val_loss: 0.0190 - val_rmse: 0.0714\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0648 - val_loss: 0.0195 - val_rmse: 0.0735\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0705 - val_loss: 0.0207 - val_rmse: 0.0826\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0649 - val_loss: 0.0253 - val_rmse: 0.0960\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0711 - val_loss: 0.0183 - val_rmse: 0.0694\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0639 - val_loss: 0.0155 - val_rmse: 0.0587\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0722 - val_loss: 0.0219 - val_rmse: 0.0876\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0645 - val_loss: 0.0185 - val_rmse: 0.0766\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0657 - val_loss: 0.0172 - val_rmse: 0.0640\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0653 - val_loss: 0.0167 - val_rmse: 0.0656\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0244 - rmse: 0.0734 - val_loss: 0.0181 - val_rmse: 0.0752\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0671 - val_loss: 0.0154 - val_rmse: 0.0581\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0716 - val_loss: 0.0157 - val_rmse: 0.0586\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0204 - rmse: 0.0548 - val_loss: 0.0173 - val_rmse: 0.0674\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0676 - val_loss: 0.0243 - val_rmse: 0.0991\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0618 - val_loss: 0.0159 - val_rmse: 0.0591\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0202 - rmse: 0.0552 - val_loss: 0.0441 - val_rmse: 0.1492\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0751 - val_loss: 0.0166 - val_rmse: 0.0629\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0658 - val_loss: 0.0167 - val_rmse: 0.0616\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0600 - val_loss: 0.0201 - val_rmse: 0.0825\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0206 - rmse: 0.0591 - val_loss: 0.0170 - val_rmse: 0.0686\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0638 - val_loss: 0.0150 - val_rmse: 0.0591\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0602 - val_loss: 0.0158 - val_rmse: 0.0569\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0661 - val_loss: 0.0242 - val_rmse: 0.0936\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0707 - val_loss: 0.0246 - val_rmse: 0.0924\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0691 - val_loss: 0.0155 - val_rmse: 0.0609\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0655 - val_loss: 0.0207 - val_rmse: 0.0819\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0215 - rmse: 0.0603 - val_loss: 0.0183 - val_rmse: 0.0742\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0703 - val_loss: 0.0183 - val_rmse: 0.0701\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0613 - val_loss: 0.0183 - val_rmse: 0.0738\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0733 - val_loss: 0.0178 - val_rmse: 0.0713\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0636 - val_loss: 0.0169 - val_rmse: 0.0654\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0574 - val_loss: 0.0283 - val_rmse: 0.1033\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0663 - val_loss: 0.0166 - val_rmse: 0.0619\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0664 - val_loss: 0.0228 - val_rmse: 0.0882\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0761 - val_loss: 0.0159 - val_rmse: 0.0581\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0207 - rmse: 0.0575 - val_loss: 0.0183 - val_rmse: 0.0717\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0717 - val_loss: 0.0186 - val_rmse: 0.0712\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0597 - val_loss: 0.0181 - val_rmse: 0.0673\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0611 - val_loss: 0.0189 - val_rmse: 0.0745\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0646 - val_loss: 0.0272 - val_rmse: 0.1042\n",
      "0.027224070498020694\n",
      "测试集 rmse： 0.8583711740525826\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_445 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_446 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_447 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_448 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_449 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 30.2324 - rmse: 4.4840 - val_loss: 6.4519 - val_rmse: 2.0685\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.0711 - rmse: 1.3537 - val_loss: 1.0561 - val_rmse: 0.7801\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.4589 - rmse: 0.4593 - val_loss: 0.1412 - val_rmse: 0.2574\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1224 - rmse: 0.2149 - val_loss: 0.0839 - val_rmse: 0.1873\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0776 - rmse: 0.1645 - val_loss: 0.0661 - val_rmse: 0.1501\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0602 - rmse: 0.1378 - val_loss: 0.0565 - val_rmse: 0.1316\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0531 - rmse: 0.1291 - val_loss: 0.0552 - val_rmse: 0.1307\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0484 - rmse: 0.1205 - val_loss: 0.0520 - val_rmse: 0.1224\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0439 - rmse: 0.1116 - val_loss: 0.0501 - val_rmse: 0.1131\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0402 - rmse: 0.1029 - val_loss: 0.0466 - val_rmse: 0.1021\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0399 - rmse: 0.1037 - val_loss: 0.0496 - val_rmse: 0.1061\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0393 - rmse: 0.1030 - val_loss: 0.0478 - val_rmse: 0.1001\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0368 - rmse: 0.0959 - val_loss: 0.0492 - val_rmse: 0.1031\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0352 - rmse: 0.0926 - val_loss: 0.0472 - val_rmse: 0.1134\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0331 - rmse: 0.0869 - val_loss: 0.0494 - val_rmse: 0.1144\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0355 - rmse: 0.0977 - val_loss: 0.0451 - val_rmse: 0.0949\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0327 - rmse: 0.0876 - val_loss: 0.0441 - val_rmse: 0.0885\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0333 - rmse: 0.0905 - val_loss: 0.0504 - val_rmse: 0.1121\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0323 - rmse: 0.0882 - val_loss: 0.0412 - val_rmse: 0.0794\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0317 - rmse: 0.0845 - val_loss: 0.0444 - val_rmse: 0.0973\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0316 - rmse: 0.0828 - val_loss: 0.0422 - val_rmse: 0.0890\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0294 - rmse: 0.0788 - val_loss: 0.0452 - val_rmse: 0.0945\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0304 - rmse: 0.0817 - val_loss: 0.0422 - val_rmse: 0.0831\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0291 - rmse: 0.0813 - val_loss: 0.0438 - val_rmse: 0.0926\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0281 - rmse: 0.0778 - val_loss: 0.0418 - val_rmse: 0.0788\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0298 - rmse: 0.0820 - val_loss: 0.0428 - val_rmse: 0.0769\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0282 - rmse: 0.0780 - val_loss: 0.0542 - val_rmse: 0.1216\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0275 - rmse: 0.0776 - val_loss: 0.0435 - val_rmse: 0.0746\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0274 - rmse: 0.0780 - val_loss: 0.0398 - val_rmse: 0.0714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100\n",
      " - 0s - loss: 0.0269 - rmse: 0.0760 - val_loss: 0.0465 - val_rmse: 0.0846\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0758 - val_loss: 0.0505 - val_rmse: 0.1140\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0743 - val_loss: 0.0425 - val_rmse: 0.0761\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0734 - val_loss: 0.0416 - val_rmse: 0.0705\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0721 - val_loss: 0.0412 - val_rmse: 0.0759\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0314 - rmse: 0.0860 - val_loss: 0.0463 - val_rmse: 0.0990\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0276 - rmse: 0.0785 - val_loss: 0.0448 - val_rmse: 0.0938\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0721 - val_loss: 0.0437 - val_rmse: 0.0801\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0254 - rmse: 0.0703 - val_loss: 0.0414 - val_rmse: 0.0679\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0285 - rmse: 0.0818 - val_loss: 0.0422 - val_rmse: 0.0706\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0267 - rmse: 0.0737 - val_loss: 0.0444 - val_rmse: 0.0889\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0263 - rmse: 0.0761 - val_loss: 0.0424 - val_rmse: 0.0748\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0262 - rmse: 0.0740 - val_loss: 0.0449 - val_rmse: 0.0847\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0248 - rmse: 0.0687 - val_loss: 0.0400 - val_rmse: 0.0664\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0256 - rmse: 0.0721 - val_loss: 0.0405 - val_rmse: 0.0746\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0717 - val_loss: 0.0405 - val_rmse: 0.0664\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0256 - rmse: 0.0729 - val_loss: 0.0400 - val_rmse: 0.0708\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0676 - val_loss: 0.0470 - val_rmse: 0.1045\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0259 - rmse: 0.0749 - val_loss: 0.0384 - val_rmse: 0.0659\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0687 - val_loss: 0.0419 - val_rmse: 0.0819\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0244 - rmse: 0.0715 - val_loss: 0.0395 - val_rmse: 0.0658\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0693 - val_loss: 0.0407 - val_rmse: 0.0719\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0708 - val_loss: 0.0451 - val_rmse: 0.0919\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0256 - rmse: 0.0732 - val_loss: 0.0543 - val_rmse: 0.1247\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0264 - rmse: 0.0776 - val_loss: 0.0481 - val_rmse: 0.1088\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0730 - val_loss: 0.0420 - val_rmse: 0.0691\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0707 - val_loss: 0.0396 - val_rmse: 0.0677\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0266 - rmse: 0.0707 - val_loss: 0.0526 - val_rmse: 0.1031\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0288 - rmse: 0.0764 - val_loss: 0.0410 - val_rmse: 0.0830\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0704 - val_loss: 0.0381 - val_rmse: 0.0617\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0672 - val_loss: 0.0373 - val_rmse: 0.0646\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0687 - val_loss: 0.0385 - val_rmse: 0.0745\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0620 - val_loss: 0.0503 - val_rmse: 0.1145\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0254 - rmse: 0.0759 - val_loss: 0.0382 - val_rmse: 0.0736\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0665 - val_loss: 0.0370 - val_rmse: 0.0622\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0638 - val_loss: 0.0381 - val_rmse: 0.0722\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0648 - val_loss: 0.0391 - val_rmse: 0.0864\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0704 - val_loss: 0.0364 - val_rmse: 0.0587\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0652 - val_loss: 0.0371 - val_rmse: 0.0647\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0700 - val_loss: 0.0367 - val_rmse: 0.0617\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0630 - val_loss: 0.0392 - val_rmse: 0.0679\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0716 - val_loss: 0.0377 - val_rmse: 0.0597\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0599 - val_loss: 0.0413 - val_rmse: 0.0915\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0258 - rmse: 0.0748 - val_loss: 0.0383 - val_rmse: 0.0675\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0673 - val_loss: 0.0389 - val_rmse: 0.0783\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0650 - val_loss: 0.0364 - val_rmse: 0.0585\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0727 - val_loss: 0.0368 - val_rmse: 0.0688\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0671 - val_loss: 0.0357 - val_rmse: 0.0644\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0743 - val_loss: 0.0409 - val_rmse: 0.0923\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0700 - val_loss: 0.0394 - val_rmse: 0.0736\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0739 - val_loss: 0.0384 - val_rmse: 0.0732\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0582 - val_loss: 0.0423 - val_rmse: 0.0905\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0744 - val_loss: 0.0413 - val_rmse: 0.0886\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0705 - val_loss: 0.0367 - val_rmse: 0.0630\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0657 - val_loss: 0.0363 - val_rmse: 0.0682\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0775 - val_loss: 0.0379 - val_rmse: 0.0717\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0669 - val_loss: 0.0466 - val_rmse: 0.1084\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0640 - val_loss: 0.0402 - val_rmse: 0.0861\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0681 - val_loss: 0.0395 - val_rmse: 0.0740\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0602 - val_loss: 0.0502 - val_rmse: 0.1115\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0726 - val_loss: 0.0361 - val_rmse: 0.0629\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0604 - val_loss: 0.0379 - val_rmse: 0.0718\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0642 - val_loss: 0.0398 - val_rmse: 0.0795\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0618 - val_loss: 0.0369 - val_rmse: 0.0657\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0615 - val_loss: 0.0357 - val_rmse: 0.0571\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0617 - val_loss: 0.0391 - val_rmse: 0.0730\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0277 - rmse: 0.0753 - val_loss: 0.0502 - val_rmse: 0.1222\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0248 - rmse: 0.0721 - val_loss: 0.0376 - val_rmse: 0.0673\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0647 - val_loss: 0.0419 - val_rmse: 0.1020\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0762 - val_loss: 0.0359 - val_rmse: 0.0723\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0254 - rmse: 0.0748 - val_loss: 0.0385 - val_rmse: 0.0738\n",
      "0.03850772708442367\n",
      "测试集 rmse： 0.8359569436920495\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_450 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_451 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_452 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_453 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_454 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 30.6957 - rmse: 4.5426 - val_loss: 6.1478 - val_rmse: 2.0354\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.5972 - rmse: 1.4965 - val_loss: 1.5835 - val_rmse: 0.9938\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.8446 - rmse: 0.6720 - val_loss: 0.2847 - val_rmse: 0.3774\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1913 - rmse: 0.2853 - val_loss: 0.1045 - val_rmse: 0.2201\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1083 - rmse: 0.1983 - val_loss: 0.0648 - val_rmse: 0.1698\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0788 - rmse: 0.1615 - val_loss: 0.0441 - val_rmse: 0.1365\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0613 - rmse: 0.1351 - val_loss: 0.0343 - val_rmse: 0.1196\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0528 - rmse: 0.1214 - val_loss: 0.0329 - val_rmse: 0.1200\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0478 - rmse: 0.1143 - val_loss: 0.0300 - val_rmse: 0.1081\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0442 - rmse: 0.1053 - val_loss: 0.0263 - val_rmse: 0.0963\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0418 - rmse: 0.1010 - val_loss: 0.0247 - val_rmse: 0.0889\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100\n",
      " - 0s - loss: 0.0394 - rmse: 0.0955 - val_loss: 0.0275 - val_rmse: 0.0992\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0385 - rmse: 0.0933 - val_loss: 0.0262 - val_rmse: 0.0973\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0370 - rmse: 0.0900 - val_loss: 0.0354 - val_rmse: 0.1257\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0368 - rmse: 0.0922 - val_loss: 0.0225 - val_rmse: 0.0838\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0354 - rmse: 0.0878 - val_loss: 0.0212 - val_rmse: 0.0786\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0343 - rmse: 0.0857 - val_loss: 0.0210 - val_rmse: 0.0765\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0338 - rmse: 0.0849 - val_loss: 0.0200 - val_rmse: 0.0761\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0333 - rmse: 0.0828 - val_loss: 0.0188 - val_rmse: 0.0720\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0321 - rmse: 0.0797 - val_loss: 0.0186 - val_rmse: 0.0703\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0306 - rmse: 0.0787 - val_loss: 0.0207 - val_rmse: 0.0843\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0312 - rmse: 0.0809 - val_loss: 0.0195 - val_rmse: 0.0812\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0312 - rmse: 0.0821 - val_loss: 0.0175 - val_rmse: 0.0693\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0308 - rmse: 0.0797 - val_loss: 0.0189 - val_rmse: 0.0783\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0296 - rmse: 0.0777 - val_loss: 0.0165 - val_rmse: 0.0657\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0281 - rmse: 0.0709 - val_loss: 0.0183 - val_rmse: 0.0746\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0283 - rmse: 0.0736 - val_loss: 0.0165 - val_rmse: 0.0670\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0280 - rmse: 0.0739 - val_loss: 0.0173 - val_rmse: 0.0741\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0281 - rmse: 0.0766 - val_loss: 0.0161 - val_rmse: 0.0654\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0263 - rmse: 0.0669 - val_loss: 0.0182 - val_rmse: 0.0785\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0280 - rmse: 0.0778 - val_loss: 0.0164 - val_rmse: 0.0687\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0267 - rmse: 0.0726 - val_loss: 0.0151 - val_rmse: 0.0628\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0257 - rmse: 0.0668 - val_loss: 0.0194 - val_rmse: 0.0880\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0274 - rmse: 0.0754 - val_loss: 0.0152 - val_rmse: 0.0712\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0679 - val_loss: 0.0258 - val_rmse: 0.1035\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0743 - val_loss: 0.0133 - val_rmse: 0.0620\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0704 - val_loss: 0.0145 - val_rmse: 0.0669\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0269 - rmse: 0.0719 - val_loss: 0.0129 - val_rmse: 0.0612\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0689 - val_loss: 0.0157 - val_rmse: 0.0776\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0680 - val_loss: 0.0184 - val_rmse: 0.0879\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0632 - val_loss: 0.0169 - val_rmse: 0.0797\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0739 - val_loss: 0.0133 - val_rmse: 0.0658\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0729 - val_loss: 0.0118 - val_rmse: 0.0567\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0677 - val_loss: 0.0185 - val_rmse: 0.0886\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0244 - rmse: 0.0701 - val_loss: 0.0118 - val_rmse: 0.0568\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0593 - val_loss: 0.1073 - val_rmse: 0.2287\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0370 - rmse: 0.1029 - val_loss: 0.0156 - val_rmse: 0.0780\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0689 - val_loss: 0.0127 - val_rmse: 0.0620\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0629 - val_loss: 0.0119 - val_rmse: 0.0574\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0637 - val_loss: 0.0129 - val_rmse: 0.0642\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0673 - val_loss: 0.0124 - val_rmse: 0.0610\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0598 - val_loss: 0.0161 - val_rmse: 0.0830\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0696 - val_loss: 0.0121 - val_rmse: 0.0609\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0646 - val_loss: 0.0146 - val_rmse: 0.0734\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0264 - rmse: 0.0774 - val_loss: 0.0113 - val_rmse: 0.0545\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0658 - val_loss: 0.0121 - val_rmse: 0.0600\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0665 - val_loss: 0.0111 - val_rmse: 0.0548\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0646 - val_loss: 0.0131 - val_rmse: 0.0687\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0632 - val_loss: 0.0128 - val_rmse: 0.0675\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0643 - val_loss: 0.0126 - val_rmse: 0.0624\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0590 - val_loss: 0.0121 - val_rmse: 0.0609\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0696 - val_loss: 0.0113 - val_rmse: 0.0540\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0599 - val_loss: 0.0133 - val_rmse: 0.0662\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0628 - val_loss: 0.0165 - val_rmse: 0.0803\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0623 - val_loss: 0.0147 - val_rmse: 0.0720\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0612 - val_loss: 0.0113 - val_rmse: 0.0557\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0645 - val_loss: 0.0119 - val_rmse: 0.0599\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0710 - val_loss: 0.0128 - val_rmse: 0.0635\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0618 - val_loss: 0.0127 - val_rmse: 0.0655\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0664 - val_loss: 0.0119 - val_rmse: 0.0556\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0664 - val_loss: 0.0124 - val_rmse: 0.0643\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0684 - val_loss: 0.0143 - val_rmse: 0.0738\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0582 - val_loss: 0.0113 - val_rmse: 0.0550\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0636 - val_loss: 0.0113 - val_rmse: 0.0566\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0566 - val_loss: 0.0137 - val_rmse: 0.0705\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0663 - val_loss: 0.0309 - val_rmse: 0.1219\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0720 - val_loss: 0.0110 - val_rmse: 0.0527\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0608 - val_loss: 0.0136 - val_rmse: 0.0649\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0640 - val_loss: 0.0108 - val_rmse: 0.0537\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0649 - val_loss: 0.0139 - val_rmse: 0.0670\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0672 - val_loss: 0.0160 - val_rmse: 0.0809\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0688 - val_loss: 0.0114 - val_rmse: 0.0575\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0205 - rmse: 0.0545 - val_loss: 0.0412 - val_rmse: 0.1463\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0723 - val_loss: 0.0107 - val_rmse: 0.0492\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0643 - val_loss: 0.0128 - val_rmse: 0.0643\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0635 - val_loss: 0.0145 - val_rmse: 0.0729\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0637 - val_loss: 0.0117 - val_rmse: 0.0594\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0607 - val_loss: 0.0123 - val_rmse: 0.0625\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0646 - val_loss: 0.0108 - val_rmse: 0.0525\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0211 - rmse: 0.0578 - val_loss: 0.0116 - val_rmse: 0.0559\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0613 - val_loss: 0.0115 - val_rmse: 0.0557\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0623 - val_loss: 0.0128 - val_rmse: 0.0613\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0633 - val_loss: 0.0169 - val_rmse: 0.0773\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0615 - val_loss: 0.0162 - val_rmse: 0.0856\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0633 - val_loss: 0.0161 - val_rmse: 0.0807\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0650 - val_loss: 0.0138 - val_rmse: 0.0682\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0623 - val_loss: 0.0132 - val_rmse: 0.0698\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0215 - rmse: 0.0602 - val_loss: 0.0241 - val_rmse: 0.1181\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0681 - val_loss: 0.0112 - val_rmse: 0.0523\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0608 - val_loss: 0.0117 - val_rmse: 0.0573\n",
      "0.011684312758587697\n",
      "测试集 rmse： 0.9024505144176831\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_455 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_456 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_457 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_458 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_459 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 7s - loss: 30.0233 - rmse: 4.4818 - val_loss: 8.2199 - val_rmse: 2.3362\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.4856 - rmse: 1.6967 - val_loss: 2.0232 - val_rmse: 1.1430\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.9667 - rmse: 0.7284 - val_loss: 0.3257 - val_rmse: 0.3883\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1839 - rmse: 0.2755 - val_loss: 0.1704 - val_rmse: 0.2271\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1058 - rmse: 0.1954 - val_loss: 0.1207 - val_rmse: 0.1833\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0771 - rmse: 0.1612 - val_loss: 0.0940 - val_rmse: 0.1514\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0608 - rmse: 0.1391 - val_loss: 0.0797 - val_rmse: 0.1391\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0509 - rmse: 0.1227 - val_loss: 0.0833 - val_rmse: 0.1425\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0461 - rmse: 0.1165 - val_loss: 0.0818 - val_rmse: 0.1178\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0412 - rmse: 0.1057 - val_loss: 0.0967 - val_rmse: 0.1387\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0400 - rmse: 0.1054 - val_loss: 0.0876 - val_rmse: 0.1031\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0383 - rmse: 0.1022 - val_loss: 0.0891 - val_rmse: 0.0997\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0352 - rmse: 0.0937 - val_loss: 0.0971 - val_rmse: 0.0964\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0340 - rmse: 0.0916 - val_loss: 0.1243 - val_rmse: 0.1591\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0366 - rmse: 0.1013 - val_loss: 0.1029 - val_rmse: 0.0928\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0318 - rmse: 0.0880 - val_loss: 0.1139 - val_rmse: 0.0915\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0301 - rmse: 0.0822 - val_loss: 0.1331 - val_rmse: 0.1403\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0317 - rmse: 0.0894 - val_loss: 0.1206 - val_rmse: 0.0994\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0302 - rmse: 0.0849 - val_loss: 0.1269 - val_rmse: 0.0912\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0295 - rmse: 0.0833 - val_loss: 0.1245 - val_rmse: 0.0852\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0295 - rmse: 0.0844 - val_loss: 0.1464 - val_rmse: 0.1275\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0285 - rmse: 0.0828 - val_loss: 0.1345 - val_rmse: 0.0827\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0265 - rmse: 0.0739 - val_loss: 0.1346 - val_rmse: 0.0991\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0278 - rmse: 0.0812 - val_loss: 0.1399 - val_rmse: 0.0797\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0785 - val_loss: 0.1395 - val_rmse: 0.0979\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0259 - rmse: 0.0725 - val_loss: 0.1416 - val_rmse: 0.0803\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0716 - val_loss: 0.1504 - val_rmse: 0.0954\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0262 - rmse: 0.0755 - val_loss: 0.1522 - val_rmse: 0.0906\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0723 - val_loss: 0.1489 - val_rmse: 0.0821\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0725 - val_loss: 0.1493 - val_rmse: 0.0782\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0729 - val_loss: 0.1526 - val_rmse: 0.0852\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0722 - val_loss: 0.1630 - val_rmse: 0.1026\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0802 - val_loss: 0.1519 - val_rmse: 0.0777\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0722 - val_loss: 0.1590 - val_rmse: 0.0832\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0705 - val_loss: 0.1606 - val_rmse: 0.0747\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0240 - rmse: 0.0673 - val_loss: 0.1541 - val_rmse: 0.0805\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0726 - val_loss: 0.1560 - val_rmse: 0.0847\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0248 - rmse: 0.0720 - val_loss: 0.1504 - val_rmse: 0.0744\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0636 - val_loss: 0.1547 - val_rmse: 0.0759\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0623 - val_loss: 0.1746 - val_rmse: 0.1365\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0256 - rmse: 0.0770 - val_loss: 0.1542 - val_rmse: 0.0742\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0630 - val_loss: 0.1634 - val_rmse: 0.0815\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0269 - rmse: 0.0801 - val_loss: 0.1564 - val_rmse: 0.0859\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0678 - val_loss: 0.1549 - val_rmse: 0.0738\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0633 - val_loss: 0.1603 - val_rmse: 0.1023\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0669 - val_loss: 0.1626 - val_rmse: 0.0960\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0684 - val_loss: 0.1525 - val_rmse: 0.0717\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0677 - val_loss: 0.1571 - val_rmse: 0.0812\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0262 - rmse: 0.0760 - val_loss: 0.1534 - val_rmse: 0.0759\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0605 - val_loss: 0.1544 - val_rmse: 0.0702\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0632 - val_loss: 0.1623 - val_rmse: 0.0849\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0266 - rmse: 0.0795 - val_loss: 0.1521 - val_rmse: 0.0805\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0676 - val_loss: 0.1510 - val_rmse: 0.0680\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0659 - val_loss: 0.1497 - val_rmse: 0.0767\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0655 - val_loss: 0.1466 - val_rmse: 0.0778\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0726 - val_loss: 0.1547 - val_rmse: 0.0887\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0687 - val_loss: 0.1503 - val_rmse: 0.0820\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0667 - val_loss: 0.1446 - val_rmse: 0.0687\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0248 - rmse: 0.0741 - val_loss: 0.1496 - val_rmse: 0.0810\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0683 - val_loss: 0.1506 - val_rmse: 0.0865\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0740 - val_loss: 0.1583 - val_rmse: 0.1033\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0656 - val_loss: 0.1503 - val_rmse: 0.0823\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0714 - val_loss: 0.1359 - val_rmse: 0.0751\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0623 - val_loss: 0.1392 - val_rmse: 0.0728\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0692 - val_loss: 0.1390 - val_rmse: 0.0737\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0621 - val_loss: 0.1329 - val_rmse: 0.0745\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0651 - val_loss: 0.1398 - val_rmse: 0.1024\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0641 - val_loss: 0.1394 - val_rmse: 0.0902\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0773 - val_loss: 0.1407 - val_rmse: 0.0979\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0656 - val_loss: 0.1391 - val_rmse: 0.0721\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0639 - val_loss: 0.1366 - val_rmse: 0.0687\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0646 - val_loss: 0.1368 - val_rmse: 0.0707\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0618 - val_loss: 0.1384 - val_rmse: 0.0734\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0641 - val_loss: 0.1423 - val_rmse: 0.0756\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0583 - val_loss: 0.1455 - val_rmse: 0.1295\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0663 - val_loss: 0.1361 - val_rmse: 0.0677\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0606 - val_loss: 0.1495 - val_rmse: 0.0801\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0680 - val_loss: 0.1416 - val_rmse: 0.0911\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0640 - val_loss: 0.1379 - val_rmse: 0.0732\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0206 - rmse: 0.0562 - val_loss: 0.1369 - val_rmse: 0.0716\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0727 - val_loss: 0.1343 - val_rmse: 0.0663\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0620 - val_loss: 0.1275 - val_rmse: 0.0700\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0614 - val_loss: 0.1376 - val_rmse: 0.0722\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0577 - val_loss: 0.1308 - val_rmse: 0.0671\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0654 - val_loss: 0.1334 - val_rmse: 0.0895\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0669 - val_loss: 0.1378 - val_rmse: 0.0744\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0691 - val_loss: 0.1425 - val_rmse: 0.1066\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0614 - val_loss: 0.1383 - val_rmse: 0.0832\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0651 - val_loss: 0.1327 - val_rmse: 0.0655\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0606 - val_loss: 0.1610 - val_rmse: 0.1332\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0263 - rmse: 0.0792 - val_loss: 0.1289 - val_rmse: 0.0696\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0603 - val_loss: 0.1424 - val_rmse: 0.0935\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0636 - val_loss: 0.1276 - val_rmse: 0.0701\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0746 - val_loss: 0.1282 - val_rmse: 0.0771\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0205 - rmse: 0.0563 - val_loss: 0.1317 - val_rmse: 0.0646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0583 - val_loss: 0.1326 - val_rmse: 0.0667\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0627 - val_loss: 0.1298 - val_rmse: 0.0670\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0577 - val_loss: 0.1298 - val_rmse: 0.0661\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0587 - val_loss: 0.1365 - val_rmse: 0.0815\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0617 - val_loss: 0.1321 - val_rmse: 0.0659\n",
      "0.1321188248067944\n",
      "测试集 rmse： 0.7334166072826889\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_460 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_461 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_462 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_463 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_464 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 28.5323 - rmse: 4.3442 - val_loss: 7.4081 - val_rmse: 2.2035\n",
      "Epoch 2/100\n",
      " - 0s - loss: 4.1974 - rmse: 1.6296 - val_loss: 1.9838 - val_rmse: 1.1102\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.0382 - rmse: 0.7597 - val_loss: 0.3321 - val_rmse: 0.4030\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.2028 - rmse: 0.2880 - val_loss: 0.1382 - val_rmse: 0.2224\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1056 - rmse: 0.1936 - val_loss: 0.1048 - val_rmse: 0.1803\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0799 - rmse: 0.1640 - val_loss: 0.0868 - val_rmse: 0.1577\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0648 - rmse: 0.1418 - val_loss: 0.0732 - val_rmse: 0.1326\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0554 - rmse: 0.1261 - val_loss: 0.0683 - val_rmse: 0.1304\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0487 - rmse: 0.1141 - val_loss: 0.0592 - val_rmse: 0.1113\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0458 - rmse: 0.1082 - val_loss: 0.0557 - val_rmse: 0.1052\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0428 - rmse: 0.1027 - val_loss: 0.0523 - val_rmse: 0.0990\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0416 - rmse: 0.1004 - val_loss: 0.0517 - val_rmse: 0.1016\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0389 - rmse: 0.0956 - val_loss: 0.0463 - val_rmse: 0.0896\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0373 - rmse: 0.0904 - val_loss: 0.0460 - val_rmse: 0.0933\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0354 - rmse: 0.0874 - val_loss: 0.0498 - val_rmse: 0.1170\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0364 - rmse: 0.0941 - val_loss: 0.0412 - val_rmse: 0.0822\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0338 - rmse: 0.0847 - val_loss: 0.0402 - val_rmse: 0.0796\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0324 - rmse: 0.0824 - val_loss: 0.0373 - val_rmse: 0.0785\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0322 - rmse: 0.0807 - val_loss: 0.0444 - val_rmse: 0.1060\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0340 - rmse: 0.0848 - val_loss: 0.0370 - val_rmse: 0.0795\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0313 - rmse: 0.0827 - val_loss: 0.0361 - val_rmse: 0.0754\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0293 - rmse: 0.0779 - val_loss: 0.0373 - val_rmse: 0.0760\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0291 - rmse: 0.0780 - val_loss: 0.0353 - val_rmse: 0.0752\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0278 - rmse: 0.0749 - val_loss: 0.0394 - val_rmse: 0.0849\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0286 - rmse: 0.0790 - val_loss: 0.0363 - val_rmse: 0.0817\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0290 - rmse: 0.0791 - val_loss: 0.0361 - val_rmse: 0.0713\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0730 - val_loss: 0.0382 - val_rmse: 0.0739\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0274 - rmse: 0.0775 - val_loss: 0.0403 - val_rmse: 0.0835\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0704 - val_loss: 0.0414 - val_rmse: 0.0773\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0260 - rmse: 0.0699 - val_loss: 0.0420 - val_rmse: 0.0907\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0275 - rmse: 0.0801 - val_loss: 0.0408 - val_rmse: 0.0726\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0276 - rmse: 0.0805 - val_loss: 0.0426 - val_rmse: 0.0785\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0701 - val_loss: 0.0391 - val_rmse: 0.0732\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0254 - rmse: 0.0728 - val_loss: 0.0431 - val_rmse: 0.0703\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0691 - val_loss: 0.0460 - val_rmse: 0.0742\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0248 - rmse: 0.0700 - val_loss: 0.0448 - val_rmse: 0.0707\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0258 - rmse: 0.0704 - val_loss: 0.0454 - val_rmse: 0.0682\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0263 - rmse: 0.0717 - val_loss: 0.0463 - val_rmse: 0.0723\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0676 - val_loss: 0.0498 - val_rmse: 0.0873\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0675 - val_loss: 0.0461 - val_rmse: 0.0686\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0671 - val_loss: 0.0477 - val_rmse: 0.0674\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0737 - val_loss: 0.0489 - val_rmse: 0.0683\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0687 - val_loss: 0.0503 - val_rmse: 0.0700\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0680 - val_loss: 0.0524 - val_rmse: 0.0682\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0655 - val_loss: 0.0523 - val_rmse: 0.0699\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0593 - val_loss: 0.0616 - val_rmse: 0.0964\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0724 - val_loss: 0.0521 - val_rmse: 0.0667\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0670 - val_loss: 0.0557 - val_rmse: 0.0661\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0693 - val_loss: 0.0547 - val_rmse: 0.0695\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0210 - rmse: 0.0571 - val_loss: 0.0590 - val_rmse: 0.0742\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0680 - val_loss: 0.0560 - val_rmse: 0.0665\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0655 - val_loss: 0.0558 - val_rmse: 0.0653\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0690 - val_loss: 0.0583 - val_rmse: 0.0680\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0701 - val_loss: 0.0594 - val_rmse: 0.0782\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0739 - val_loss: 0.0625 - val_rmse: 0.0787\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0601 - val_loss: 0.0609 - val_rmse: 0.0736\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0681 - val_loss: 0.0684 - val_rmse: 0.1009\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0679 - val_loss: 0.0626 - val_rmse: 0.0798\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0624 - val_loss: 0.0633 - val_rmse: 0.0696\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0609 - val_loss: 0.0628 - val_rmse: 0.0844\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0657 - val_loss: 0.0613 - val_rmse: 0.0620\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0629 - val_loss: 0.0622 - val_rmse: 0.0742\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0684 - val_loss: 0.0659 - val_rmse: 0.0926\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0667 - val_loss: 0.0676 - val_rmse: 0.1093\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0719 - val_loss: 0.0599 - val_rmse: 0.0686\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0640 - val_loss: 0.0600 - val_rmse: 0.0653\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0691 - val_loss: 0.0638 - val_rmse: 0.0862\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0687 - val_loss: 0.0628 - val_rmse: 0.0692\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0751 - val_loss: 0.0588 - val_rmse: 0.0677\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0599 - val_loss: 0.0614 - val_rmse: 0.0671\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0715 - val_loss: 0.0584 - val_rmse: 0.0669\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0708 - val_loss: 0.0609 - val_rmse: 0.0663\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0210 - rmse: 0.0584 - val_loss: 0.0625 - val_rmse: 0.0798\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0664 - val_loss: 0.0594 - val_rmse: 0.0629\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0643 - val_loss: 0.0583 - val_rmse: 0.0603\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0643 - val_loss: 0.0618 - val_rmse: 0.0707\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0659 - val_loss: 0.0603 - val_rmse: 0.0712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/100\n",
      " - 0s - loss: 0.0205 - rmse: 0.0572 - val_loss: 0.0605 - val_rmse: 0.0628\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0652 - val_loss: 0.0588 - val_rmse: 0.0617\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0683 - val_loss: 0.0605 - val_rmse: 0.0780\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0572 - val_loss: 0.0607 - val_rmse: 0.0615\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0658 - val_loss: 0.0588 - val_rmse: 0.0608\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0258 - rmse: 0.0693 - val_loss: 0.0584 - val_rmse: 0.0666\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0674 - val_loss: 0.0632 - val_rmse: 0.0827\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0619 - val_loss: 0.0603 - val_rmse: 0.0661\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0699 - val_loss: 0.0601 - val_rmse: 0.0660\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0627 - val_loss: 0.0584 - val_rmse: 0.0673\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0622 - val_loss: 0.0634 - val_rmse: 0.0738\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0683 - val_loss: 0.0673 - val_rmse: 0.0857\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0232 - rmse: 0.0635 - val_loss: 0.0540 - val_rmse: 0.0673\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0638 - val_loss: 0.0607 - val_rmse: 0.0732\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0655 - val_loss: 0.0635 - val_rmse: 0.0759\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0653 - val_loss: 0.0602 - val_rmse: 0.0717\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0655 - val_loss: 0.0578 - val_rmse: 0.0654\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0203 - rmse: 0.0572 - val_loss: 0.0569 - val_rmse: 0.0620\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0699 - val_loss: 0.0589 - val_rmse: 0.0725\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0653 - val_loss: 0.0573 - val_rmse: 0.0613\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0580 - val_loss: 0.0679 - val_rmse: 0.0929\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0230 - rmse: 0.0709 - val_loss: 0.0613 - val_rmse: 0.0792\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0636 - val_loss: 0.0581 - val_rmse: 0.0615\n",
      "0.05807425987067889\n",
      "测试集 rmse： 0.8058108529577892\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_465 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_466 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_467 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_468 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_469 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 34.6139 - rmse: 4.9151 - val_loss: 10.1336 - val_rmse: 2.6661\n",
      "Epoch 2/100\n",
      " - 0s - loss: 5.6226 - rmse: 1.9088 - val_loss: 2.6617 - val_rmse: 1.3153\n",
      "Epoch 3/100\n",
      " - 0s - loss: 1.5698 - rmse: 0.9546 - val_loss: 0.7170 - val_rmse: 0.6418\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.3945 - rmse: 0.4327 - val_loss: 0.1867 - val_rmse: 0.2823\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.1554 - rmse: 0.2441 - val_loss: 0.1150 - val_rmse: 0.2152\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.1068 - rmse: 0.1969 - val_loss: 0.0860 - val_rmse: 0.1829\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0833 - rmse: 0.1695 - val_loss: 0.0668 - val_rmse: 0.1602\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0687 - rmse: 0.1499 - val_loss: 0.0529 - val_rmse: 0.1439\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0584 - rmse: 0.1327 - val_loss: 0.0431 - val_rmse: 0.1245\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0516 - rmse: 0.1202 - val_loss: 0.0397 - val_rmse: 0.1180\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0474 - rmse: 0.1121 - val_loss: 0.0336 - val_rmse: 0.1054\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0432 - rmse: 0.1040 - val_loss: 0.0331 - val_rmse: 0.1050\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0409 - rmse: 0.0995 - val_loss: 0.0308 - val_rmse: 0.0943\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0386 - rmse: 0.0965 - val_loss: 0.0289 - val_rmse: 0.0917\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0364 - rmse: 0.0906 - val_loss: 0.0306 - val_rmse: 0.1007\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0351 - rmse: 0.0885 - val_loss: 0.0304 - val_rmse: 0.0988\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0348 - rmse: 0.0894 - val_loss: 0.0267 - val_rmse: 0.0813\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0329 - rmse: 0.0827 - val_loss: 0.0291 - val_rmse: 0.0922\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0331 - rmse: 0.0856 - val_loss: 0.0282 - val_rmse: 0.0835\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0313 - rmse: 0.0801 - val_loss: 0.0296 - val_rmse: 0.0915\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0312 - rmse: 0.0817 - val_loss: 0.0281 - val_rmse: 0.0858\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0307 - rmse: 0.0788 - val_loss: 0.0262 - val_rmse: 0.0779\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0303 - rmse: 0.0800 - val_loss: 0.0277 - val_rmse: 0.0834\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0294 - rmse: 0.0773 - val_loss: 0.0260 - val_rmse: 0.0779\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0285 - rmse: 0.0744 - val_loss: 0.0287 - val_rmse: 0.0927\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0282 - rmse: 0.0747 - val_loss: 0.0251 - val_rmse: 0.0780\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0280 - rmse: 0.0733 - val_loss: 0.0255 - val_rmse: 0.0751\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0712 - val_loss: 0.0308 - val_rmse: 0.0950\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0282 - rmse: 0.0759 - val_loss: 0.0255 - val_rmse: 0.0789\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0278 - rmse: 0.0744 - val_loss: 0.0233 - val_rmse: 0.0693\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0273 - rmse: 0.0748 - val_loss: 0.0243 - val_rmse: 0.0749\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0744 - val_loss: 0.0245 - val_rmse: 0.0772\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0260 - rmse: 0.0705 - val_loss: 0.0254 - val_rmse: 0.0779\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0665 - val_loss: 0.0686 - val_rmse: 0.1767\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0338 - rmse: 0.0913 - val_loss: 0.0278 - val_rmse: 0.0853\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0262 - rmse: 0.0739 - val_loss: 0.0234 - val_rmse: 0.0699\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0673 - val_loss: 0.0230 - val_rmse: 0.0680\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0704 - val_loss: 0.0230 - val_rmse: 0.0650\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0251 - rmse: 0.0685 - val_loss: 0.0215 - val_rmse: 0.0631\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0663 - val_loss: 0.0280 - val_rmse: 0.0923\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0276 - rmse: 0.0807 - val_loss: 0.0229 - val_rmse: 0.0729\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0627 - val_loss: 0.0217 - val_rmse: 0.0630\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0624 - val_loss: 0.0380 - val_rmse: 0.1231\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0807 - val_loss: 0.0236 - val_rmse: 0.0774\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0724 - val_loss: 0.0221 - val_rmse: 0.0632\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0638 - val_loss: 0.0245 - val_rmse: 0.0799\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0679 - val_loss: 0.0242 - val_rmse: 0.0789\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0669 - val_loss: 0.0211 - val_rmse: 0.0638\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0596 - val_loss: 0.1191 - val_rmse: 0.2647\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0449 - rmse: 0.1136 - val_loss: 0.0216 - val_rmse: 0.0671\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0638 - val_loss: 0.0327 - val_rmse: 0.1126\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0306 - rmse: 0.0715 - val_loss: 0.0289 - val_rmse: 0.0997\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0641 - val_loss: 0.0198 - val_rmse: 0.0585\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0656 - val_loss: 0.0234 - val_rmse: 0.0777\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0635 - val_loss: 0.0196 - val_rmse: 0.0629\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0239 - rmse: 0.0698 - val_loss: 0.0211 - val_rmse: 0.0661\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0614 - val_loss: 0.0200 - val_rmse: 0.0603\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0618 - val_loss: 0.0201 - val_rmse: 0.0652\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0641 - val_loss: 0.0232 - val_rmse: 0.0809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0695 - val_loss: 0.0231 - val_rmse: 0.0795\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0612 - val_loss: 0.0250 - val_rmse: 0.0863\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0231 - rmse: 0.0671 - val_loss: 0.0338 - val_rmse: 0.1134\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0676 - val_loss: 0.0208 - val_rmse: 0.0661\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0580 - val_loss: 0.0205 - val_rmse: 0.0648\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0658 - val_loss: 0.0206 - val_rmse: 0.0631\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0661 - val_loss: 0.0193 - val_rmse: 0.0603\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0589 - val_loss: 0.0296 - val_rmse: 0.1035\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0655 - val_loss: 0.0221 - val_rmse: 0.0749\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0683 - val_loss: 0.0247 - val_rmse: 0.0808\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0679 - val_loss: 0.0219 - val_rmse: 0.0741\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0629 - val_loss: 0.0204 - val_rmse: 0.0568\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0605 - val_loss: 0.0185 - val_rmse: 0.0583\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0212 - rmse: 0.0589 - val_loss: 0.0192 - val_rmse: 0.0583\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0647 - val_loss: 0.0197 - val_rmse: 0.0643\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0214 - rmse: 0.0608 - val_loss: 0.0323 - val_rmse: 0.1138\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0211 - rmse: 0.0597 - val_loss: 0.0236 - val_rmse: 0.0861\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0626 - val_loss: 0.0185 - val_rmse: 0.0576\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0202 - rmse: 0.0551 - val_loss: 0.0185 - val_rmse: 0.0588\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0635 - val_loss: 0.0183 - val_rmse: 0.0609\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0208 - rmse: 0.0595 - val_loss: 0.0209 - val_rmse: 0.0730\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0208 - rmse: 0.0589 - val_loss: 0.0189 - val_rmse: 0.0623\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0209 - rmse: 0.0598 - val_loss: 0.0182 - val_rmse: 0.0551\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0208 - rmse: 0.0584 - val_loss: 0.0198 - val_rmse: 0.0733\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0651 - val_loss: 0.0184 - val_rmse: 0.0557\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0211 - rmse: 0.0582 - val_loss: 0.0187 - val_rmse: 0.0618\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0718 - val_loss: 0.0218 - val_rmse: 0.0770\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0707 - val_loss: 0.0172 - val_rmse: 0.0581\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0206 - rmse: 0.0538 - val_loss: 0.0239 - val_rmse: 0.0813\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0658 - val_loss: 0.0165 - val_rmse: 0.0533\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0564 - val_loss: 0.0277 - val_rmse: 0.1032\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0612 - val_loss: 0.0188 - val_rmse: 0.0702\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0650 - val_loss: 0.0196 - val_rmse: 0.0639\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0613 - val_loss: 0.0170 - val_rmse: 0.0542\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0213 - rmse: 0.0583 - val_loss: 0.0231 - val_rmse: 0.0821\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0679 - val_loss: 0.0185 - val_rmse: 0.0620\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0612 - val_loss: 0.0182 - val_rmse: 0.0663\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0216 - rmse: 0.0634 - val_loss: 0.0169 - val_rmse: 0.0544\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0205 - rmse: 0.0553 - val_loss: 0.0281 - val_rmse: 0.1028\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0210 - rmse: 0.0597 - val_loss: 0.0187 - val_rmse: 0.0655\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0202 - rmse: 0.0555 - val_loss: 0.0200 - val_rmse: 0.0733\n",
      "0.019997634734825628\n",
      "测试集 rmse： 0.876107075775116\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_470 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_471 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_472 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_473 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_474 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 7s - loss: 25.7997 - rmse: 4.0512 - val_loss: 5.1135 - val_rmse: 1.8478\n",
      "Epoch 2/100\n",
      " - 0s - loss: 2.6608 - rmse: 1.2668 - val_loss: 1.0005 - val_rmse: 0.7728\n",
      "Epoch 3/100\n",
      " - 0s - loss: 0.4503 - rmse: 0.4508 - val_loss: 0.1986 - val_rmse: 0.2809\n",
      "Epoch 4/100\n",
      " - 0s - loss: 0.1500 - rmse: 0.2302 - val_loss: 0.1258 - val_rmse: 0.2056\n",
      "Epoch 5/100\n",
      " - 0s - loss: 0.0915 - rmse: 0.1822 - val_loss: 0.0936 - val_rmse: 0.1613\n",
      "Epoch 6/100\n",
      " - 0s - loss: 0.0683 - rmse: 0.1501 - val_loss: 0.0715 - val_rmse: 0.1349\n",
      "Epoch 7/100\n",
      " - 0s - loss: 0.0556 - rmse: 0.1292 - val_loss: 0.0619 - val_rmse: 0.1215\n",
      "Epoch 8/100\n",
      " - 0s - loss: 0.0498 - rmse: 0.1201 - val_loss: 0.0557 - val_rmse: 0.1108\n",
      "Epoch 9/100\n",
      " - 0s - loss: 0.0446 - rmse: 0.1071 - val_loss: 0.0526 - val_rmse: 0.1013\n",
      "Epoch 10/100\n",
      " - 0s - loss: 0.0425 - rmse: 0.1035 - val_loss: 0.0546 - val_rmse: 0.1175\n",
      "Epoch 11/100\n",
      " - 0s - loss: 0.0410 - rmse: 0.1015 - val_loss: 0.0471 - val_rmse: 0.0927\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0388 - rmse: 0.0953 - val_loss: 0.0492 - val_rmse: 0.0979\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0375 - rmse: 0.0924 - val_loss: 0.0494 - val_rmse: 0.1011\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0378 - rmse: 0.0948 - val_loss: 0.0479 - val_rmse: 0.0993\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0365 - rmse: 0.0926 - val_loss: 0.0445 - val_rmse: 0.0916\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0355 - rmse: 0.0882 - val_loss: 0.0521 - val_rmse: 0.1123\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0343 - rmse: 0.0862 - val_loss: 0.0454 - val_rmse: 0.0789\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0327 - rmse: 0.0817 - val_loss: 0.0427 - val_rmse: 0.0798\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0328 - rmse: 0.0810 - val_loss: 0.0431 - val_rmse: 0.0805\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0317 - rmse: 0.0794 - val_loss: 0.0408 - val_rmse: 0.0755\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0323 - rmse: 0.0822 - val_loss: 0.0414 - val_rmse: 0.0801\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0319 - rmse: 0.0807 - val_loss: 0.0429 - val_rmse: 0.0922\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0307 - rmse: 0.0779 - val_loss: 0.0432 - val_rmse: 0.0860\n",
      "Epoch 24/100\n",
      " - 0s - loss: 0.0292 - rmse: 0.0723 - val_loss: 0.0516 - val_rmse: 0.1135\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0323 - rmse: 0.0858 - val_loss: 0.0380 - val_rmse: 0.0806\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0299 - rmse: 0.0763 - val_loss: 0.0387 - val_rmse: 0.0688\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0296 - rmse: 0.0720 - val_loss: 0.0419 - val_rmse: 0.0846\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0299 - rmse: 0.0749 - val_loss: 0.0380 - val_rmse: 0.0750\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0293 - rmse: 0.0764 - val_loss: 0.0394 - val_rmse: 0.0734\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0284 - rmse: 0.0748 - val_loss: 0.0392 - val_rmse: 0.0704\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0288 - rmse: 0.0759 - val_loss: 0.0382 - val_rmse: 0.0757\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0278 - rmse: 0.0717 - val_loss: 0.0472 - val_rmse: 0.1029\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0303 - rmse: 0.0823 - val_loss: 0.0476 - val_rmse: 0.1058\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0300 - rmse: 0.0823 - val_loss: 0.0411 - val_rmse: 0.0844\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0276 - rmse: 0.0718 - val_loss: 0.0503 - val_rmse: 0.1142\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0296 - rmse: 0.0808 - val_loss: 0.0368 - val_rmse: 0.0674\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0657 - val_loss: 0.0469 - val_rmse: 0.1128\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0276 - rmse: 0.0773 - val_loss: 0.0358 - val_rmse: 0.0667\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0259 - rmse: 0.0703 - val_loss: 0.0344 - val_rmse: 0.0676\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0265 - rmse: 0.0720 - val_loss: 0.0360 - val_rmse: 0.0669\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0749 - val_loss: 0.0394 - val_rmse: 0.0833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0675 - val_loss: 0.0385 - val_rmse: 0.0799\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0263 - rmse: 0.0748 - val_loss: 0.0358 - val_rmse: 0.0594\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0260 - rmse: 0.0720 - val_loss: 0.0426 - val_rmse: 0.0896\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0258 - rmse: 0.0709 - val_loss: 0.0368 - val_rmse: 0.0654\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0636 - val_loss: 0.0647 - val_rmse: 0.1559\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0285 - rmse: 0.0843 - val_loss: 0.0368 - val_rmse: 0.0592\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0685 - val_loss: 0.0425 - val_rmse: 0.0818\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0290 - rmse: 0.0865 - val_loss: 0.0449 - val_rmse: 0.0850\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0761 - val_loss: 0.0404 - val_rmse: 0.0725\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0292 - rmse: 0.0727 - val_loss: 0.0409 - val_rmse: 0.0688\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0281 - rmse: 0.0682 - val_loss: 0.0418 - val_rmse: 0.0691\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0254 - rmse: 0.0705 - val_loss: 0.0436 - val_rmse: 0.0639\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0667 - val_loss: 0.0426 - val_rmse: 0.0634\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0668 - val_loss: 0.0453 - val_rmse: 0.0763\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0659 - val_loss: 0.0440 - val_rmse: 0.0731\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0683 - val_loss: 0.0446 - val_rmse: 0.0697\n",
      "Epoch 58/100\n",
      " - 1s - loss: 0.0221 - rmse: 0.0597 - val_loss: 0.0529 - val_rmse: 0.0995\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0262 - rmse: 0.0773 - val_loss: 0.0441 - val_rmse: 0.0708\n",
      "Epoch 60/100\n",
      " - 1s - loss: 0.0233 - rmse: 0.0675 - val_loss: 0.0406 - val_rmse: 0.0577\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0675 - val_loss: 0.0431 - val_rmse: 0.0637\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0236 - rmse: 0.0676 - val_loss: 0.0430 - val_rmse: 0.0586\n",
      "Epoch 63/100\n",
      " - 1s - loss: 0.0220 - rmse: 0.0597 - val_loss: 0.0478 - val_rmse: 0.0782\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0260 - rmse: 0.0780 - val_loss: 0.0443 - val_rmse: 0.0679\n",
      "Epoch 65/100\n",
      " - 1s - loss: 0.0230 - rmse: 0.0664 - val_loss: 0.0491 - val_rmse: 0.0879\n",
      "Epoch 66/100\n",
      " - 1s - loss: 0.0230 - rmse: 0.0657 - val_loss: 0.0443 - val_rmse: 0.0555\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0630 - val_loss: 0.0434 - val_rmse: 0.0629\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0639 - val_loss: 0.0430 - val_rmse: 0.0652\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0624 - val_loss: 0.0479 - val_rmse: 0.0961\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0264 - rmse: 0.0803 - val_loss: 0.0456 - val_rmse: 0.0740\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0760 - val_loss: 0.0517 - val_rmse: 0.0985\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0633 - val_loss: 0.0414 - val_rmse: 0.0652\n",
      "Epoch 73/100\n",
      " - 1s - loss: 0.0233 - rmse: 0.0651 - val_loss: 0.0425 - val_rmse: 0.0622\n",
      "Epoch 74/100\n",
      " - 1s - loss: 0.0278 - rmse: 0.0805 - val_loss: 0.0478 - val_rmse: 0.0842\n",
      "Epoch 75/100\n",
      " - 1s - loss: 0.0241 - rmse: 0.0686 - val_loss: 0.0442 - val_rmse: 0.0659\n",
      "Epoch 76/100\n",
      " - 1s - loss: 0.0227 - rmse: 0.0651 - val_loss: 0.0459 - val_rmse: 0.0824\n",
      "Epoch 77/100\n",
      " - 1s - loss: 0.0233 - rmse: 0.0674 - val_loss: 0.0445 - val_rmse: 0.0770\n",
      "Epoch 78/100\n",
      " - 1s - loss: 0.0278 - rmse: 0.0843 - val_loss: 0.0412 - val_rmse: 0.0562\n",
      "Epoch 79/100\n",
      " - 1s - loss: 0.0259 - rmse: 0.0664 - val_loss: 0.0453 - val_rmse: 0.0823\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0710 - val_loss: 0.0412 - val_rmse: 0.0711\n",
      "Epoch 81/100\n",
      " - 1s - loss: 0.0216 - rmse: 0.0583 - val_loss: 0.0448 - val_rmse: 0.0744\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0288 - rmse: 0.0860 - val_loss: 0.0412 - val_rmse: 0.0520\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0233 - rmse: 0.0667 - val_loss: 0.0409 - val_rmse: 0.0671\n",
      "Epoch 84/100\n",
      " - 1s - loss: 0.0223 - rmse: 0.0635 - val_loss: 0.0398 - val_rmse: 0.0625\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0217 - rmse: 0.0600 - val_loss: 0.0401 - val_rmse: 0.0607\n",
      "Epoch 86/100\n",
      " - 1s - loss: 0.0229 - rmse: 0.0643 - val_loss: 0.0393 - val_rmse: 0.0660\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0653 - val_loss: 0.0399 - val_rmse: 0.0574\n",
      "Epoch 88/100\n",
      " - 1s - loss: 0.0214 - rmse: 0.0605 - val_loss: 0.0381 - val_rmse: 0.0579\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0671 - val_loss: 0.0483 - val_rmse: 0.0975\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0651 - val_loss: 0.0384 - val_rmse: 0.0645\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0616 - val_loss: 0.0489 - val_rmse: 0.1056\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0627 - val_loss: 0.0497 - val_rmse: 0.1024\n",
      "Epoch 93/100\n",
      " - 1s - loss: 0.0224 - rmse: 0.0648 - val_loss: 0.0491 - val_rmse: 0.1060\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0753 - val_loss: 0.0377 - val_rmse: 0.0548\n",
      "Epoch 95/100\n",
      " - 1s - loss: 0.0229 - rmse: 0.0661 - val_loss: 0.0395 - val_rmse: 0.0629\n",
      "Epoch 96/100\n",
      " - 1s - loss: 0.0213 - rmse: 0.0593 - val_loss: 0.0396 - val_rmse: 0.0697\n",
      "Epoch 97/100\n",
      " - 1s - loss: 0.0230 - rmse: 0.0670 - val_loss: 0.0385 - val_rmse: 0.0564\n",
      "Epoch 98/100\n",
      " - 1s - loss: 0.0241 - rmse: 0.0707 - val_loss: 0.0368 - val_rmse: 0.0580\n",
      "Epoch 99/100\n",
      " - 1s - loss: 0.0236 - rmse: 0.0694 - val_loss: 0.0386 - val_rmse: 0.0627\n",
      "Epoch 100/100\n",
      " - 1s - loss: 0.0219 - rmse: 0.0631 - val_loss: 0.0382 - val_rmse: 0.0605\n",
      "0.038218449946207135\n",
      "测试集 rmse： 0.8364733173841299\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_475 (Dense)            (None, 64)                14848     \n",
      "_________________________________________________________________\n",
      "dense_476 (Dense)            (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_477 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_478 (Dense)            (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_479 (Dense)            (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 17,601\n",
      "Trainable params: 17,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 7134 samples, validate on 1784 samples\n",
      "Epoch 1/100\n",
      " - 9s - loss: 29.1546 - rmse: 4.3803 - val_loss: 7.1199 - val_rmse: 2.1736\n",
      "Epoch 2/100\n",
      " - 0s - loss: 3.8580 - rmse: 1.5639 - val_loss: 1.5591 - val_rmse: 0.9882\n",
      "Epoch 3/100\n",
      " - 1s - loss: 0.7730 - rmse: 0.6331 - val_loss: 0.2006 - val_rmse: 0.3281\n",
      "Epoch 4/100\n",
      " - 1s - loss: 0.1691 - rmse: 0.2580 - val_loss: 0.0813 - val_rmse: 0.2092\n",
      "Epoch 5/100\n",
      " - 1s - loss: 0.0988 - rmse: 0.1871 - val_loss: 0.0603 - val_rmse: 0.1803\n",
      "Epoch 6/100\n",
      " - 1s - loss: 0.0747 - rmse: 0.1591 - val_loss: 0.0380 - val_rmse: 0.1328\n",
      "Epoch 7/100\n",
      " - 1s - loss: 0.0604 - rmse: 0.1352 - val_loss: 0.0399 - val_rmse: 0.1440\n",
      "Epoch 8/100\n",
      " - 1s - loss: 0.0536 - rmse: 0.1247 - val_loss: 0.0307 - val_rmse: 0.1150\n",
      "Epoch 9/100\n",
      " - 1s - loss: 0.0506 - rmse: 0.1199 - val_loss: 0.0261 - val_rmse: 0.1023\n",
      "Epoch 10/100\n",
      " - 1s - loss: 0.0473 - rmse: 0.1136 - val_loss: 0.0249 - val_rmse: 0.0991\n",
      "Epoch 11/100\n",
      " - 1s - loss: 0.0456 - rmse: 0.1105 - val_loss: 0.0232 - val_rmse: 0.0943\n",
      "Epoch 12/100\n",
      " - 0s - loss: 0.0419 - rmse: 0.1010 - val_loss: 0.0213 - val_rmse: 0.0876\n",
      "Epoch 13/100\n",
      " - 0s - loss: 0.0402 - rmse: 0.0981 - val_loss: 0.0202 - val_rmse: 0.0844\n",
      "Epoch 14/100\n",
      " - 0s - loss: 0.0396 - rmse: 0.0960 - val_loss: 0.0210 - val_rmse: 0.0895\n",
      "Epoch 15/100\n",
      " - 0s - loss: 0.0384 - rmse: 0.0962 - val_loss: 0.0188 - val_rmse: 0.0797\n",
      "Epoch 16/100\n",
      " - 0s - loss: 0.0391 - rmse: 0.0980 - val_loss: 0.0193 - val_rmse: 0.0826\n",
      "Epoch 17/100\n",
      " - 0s - loss: 0.0353 - rmse: 0.0865 - val_loss: 0.0183 - val_rmse: 0.0800\n",
      "Epoch 18/100\n",
      " - 0s - loss: 0.0359 - rmse: 0.0894 - val_loss: 0.0183 - val_rmse: 0.0796\n",
      "Epoch 19/100\n",
      " - 0s - loss: 0.0349 - rmse: 0.0863 - val_loss: 0.0251 - val_rmse: 0.1060\n",
      "Epoch 20/100\n",
      " - 0s - loss: 0.0336 - rmse: 0.0820 - val_loss: 0.0229 - val_rmse: 0.0982\n",
      "Epoch 21/100\n",
      " - 0s - loss: 0.0336 - rmse: 0.0857 - val_loss: 0.0170 - val_rmse: 0.0747\n",
      "Epoch 22/100\n",
      " - 0s - loss: 0.0322 - rmse: 0.0805 - val_loss: 0.0161 - val_rmse: 0.0708\n",
      "Epoch 23/100\n",
      " - 0s - loss: 0.0317 - rmse: 0.0788 - val_loss: 0.0154 - val_rmse: 0.0672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100\n",
      " - 0s - loss: 0.0316 - rmse: 0.0786 - val_loss: 0.0310 - val_rmse: 0.1191\n",
      "Epoch 25/100\n",
      " - 0s - loss: 0.0338 - rmse: 0.0877 - val_loss: 0.0191 - val_rmse: 0.0849\n",
      "Epoch 26/100\n",
      " - 0s - loss: 0.0320 - rmse: 0.0810 - val_loss: 0.0148 - val_rmse: 0.0664\n",
      "Epoch 27/100\n",
      " - 0s - loss: 0.0296 - rmse: 0.0757 - val_loss: 0.0291 - val_rmse: 0.1168\n",
      "Epoch 28/100\n",
      " - 0s - loss: 0.0318 - rmse: 0.0843 - val_loss: 0.0143 - val_rmse: 0.0635\n",
      "Epoch 29/100\n",
      " - 0s - loss: 0.0304 - rmse: 0.0792 - val_loss: 0.0233 - val_rmse: 0.0998\n",
      "Epoch 30/100\n",
      " - 0s - loss: 0.0289 - rmse: 0.0766 - val_loss: 0.0146 - val_rmse: 0.0654\n",
      "Epoch 31/100\n",
      " - 0s - loss: 0.0282 - rmse: 0.0721 - val_loss: 0.0172 - val_rmse: 0.0812\n",
      "Epoch 32/100\n",
      " - 0s - loss: 0.0290 - rmse: 0.0770 - val_loss: 0.0134 - val_rmse: 0.0594\n",
      "Epoch 33/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0676 - val_loss: 0.0175 - val_rmse: 0.0826\n",
      "Epoch 34/100\n",
      " - 0s - loss: 0.0277 - rmse: 0.0739 - val_loss: 0.0148 - val_rmse: 0.0670\n",
      "Epoch 35/100\n",
      " - 0s - loss: 0.0267 - rmse: 0.0674 - val_loss: 0.0203 - val_rmse: 0.0920\n",
      "Epoch 36/100\n",
      " - 0s - loss: 0.0298 - rmse: 0.0809 - val_loss: 0.0156 - val_rmse: 0.0701\n",
      "Epoch 37/100\n",
      " - 0s - loss: 0.0282 - rmse: 0.0758 - val_loss: 0.0203 - val_rmse: 0.0912\n",
      "Epoch 38/100\n",
      " - 0s - loss: 0.0283 - rmse: 0.0756 - val_loss: 0.0142 - val_rmse: 0.0644\n",
      "Epoch 39/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0720 - val_loss: 0.0144 - val_rmse: 0.0642\n",
      "Epoch 40/100\n",
      " - 0s - loss: 0.0287 - rmse: 0.0791 - val_loss: 0.0140 - val_rmse: 0.0616\n",
      "Epoch 41/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0673 - val_loss: 0.0278 - val_rmse: 0.1101\n",
      "Epoch 42/100\n",
      " - 0s - loss: 0.0273 - rmse: 0.0756 - val_loss: 0.0131 - val_rmse: 0.0584\n",
      "Epoch 43/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0668 - val_loss: 0.0141 - val_rmse: 0.0622\n",
      "Epoch 44/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0710 - val_loss: 0.0134 - val_rmse: 0.0612\n",
      "Epoch 45/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0673 - val_loss: 0.0135 - val_rmse: 0.0616\n",
      "Epoch 46/100\n",
      " - 0s - loss: 0.0252 - rmse: 0.0674 - val_loss: 0.0170 - val_rmse: 0.0792\n",
      "Epoch 47/100\n",
      " - 0s - loss: 0.0288 - rmse: 0.0815 - val_loss: 0.0198 - val_rmse: 0.0964\n",
      "Epoch 48/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0753 - val_loss: 0.0140 - val_rmse: 0.0640\n",
      "Epoch 49/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0710 - val_loss: 0.0137 - val_rmse: 0.0608\n",
      "Epoch 50/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0745 - val_loss: 0.0147 - val_rmse: 0.0653\n",
      "Epoch 51/100\n",
      " - 0s - loss: 0.0249 - rmse: 0.0681 - val_loss: 0.0131 - val_rmse: 0.0590\n",
      "Epoch 52/100\n",
      " - 0s - loss: 0.0273 - rmse: 0.0735 - val_loss: 0.0131 - val_rmse: 0.0595\n",
      "Epoch 53/100\n",
      " - 0s - loss: 0.0269 - rmse: 0.0700 - val_loss: 0.0170 - val_rmse: 0.0764\n",
      "Epoch 54/100\n",
      " - 0s - loss: 0.0323 - rmse: 0.0885 - val_loss: 0.0132 - val_rmse: 0.0599\n",
      "Epoch 55/100\n",
      " - 0s - loss: 0.0246 - rmse: 0.0653 - val_loss: 0.0290 - val_rmse: 0.1127\n",
      "Epoch 56/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0751 - val_loss: 0.0165 - val_rmse: 0.0787\n",
      "Epoch 57/100\n",
      " - 0s - loss: 0.0272 - rmse: 0.0760 - val_loss: 0.0175 - val_rmse: 0.0793\n",
      "Epoch 58/100\n",
      " - 0s - loss: 0.0270 - rmse: 0.0755 - val_loss: 0.0144 - val_rmse: 0.0664\n",
      "Epoch 59/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0622 - val_loss: 0.0213 - val_rmse: 0.0921\n",
      "Epoch 60/100\n",
      " - 0s - loss: 0.0241 - rmse: 0.0657 - val_loss: 0.0235 - val_rmse: 0.1040\n",
      "Epoch 61/100\n",
      " - 0s - loss: 0.0261 - rmse: 0.0761 - val_loss: 0.0155 - val_rmse: 0.0709\n",
      "Epoch 62/100\n",
      " - 0s - loss: 0.0271 - rmse: 0.0771 - val_loss: 0.0142 - val_rmse: 0.0672\n",
      "Epoch 63/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0595 - val_loss: 0.0225 - val_rmse: 0.1027\n",
      "Epoch 64/100\n",
      " - 0s - loss: 0.0382 - rmse: 0.0794 - val_loss: 0.0270 - val_rmse: 0.1114\n",
      "Epoch 65/100\n",
      " - 0s - loss: 0.0264 - rmse: 0.0741 - val_loss: 0.0131 - val_rmse: 0.0568\n",
      "Epoch 66/100\n",
      " - 0s - loss: 0.0234 - rmse: 0.0638 - val_loss: 0.0119 - val_rmse: 0.0512\n",
      "Epoch 67/100\n",
      " - 0s - loss: 0.0247 - rmse: 0.0686 - val_loss: 0.0140 - val_rmse: 0.0642\n",
      "Epoch 68/100\n",
      " - 0s - loss: 0.0265 - rmse: 0.0756 - val_loss: 0.0146 - val_rmse: 0.0691\n",
      "Epoch 69/100\n",
      " - 0s - loss: 0.0245 - rmse: 0.0700 - val_loss: 0.0145 - val_rmse: 0.0673\n",
      "Epoch 70/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0681 - val_loss: 0.0140 - val_rmse: 0.0650\n",
      "Epoch 71/100\n",
      " - 0s - loss: 0.0255 - rmse: 0.0742 - val_loss: 0.0160 - val_rmse: 0.0746\n",
      "Epoch 72/100\n",
      " - 0s - loss: 0.0229 - rmse: 0.0635 - val_loss: 0.0129 - val_rmse: 0.0597\n",
      "Epoch 73/100\n",
      " - 0s - loss: 0.0226 - rmse: 0.0625 - val_loss: 0.0206 - val_rmse: 0.0917\n",
      "Epoch 74/100\n",
      " - 0s - loss: 0.0238 - rmse: 0.0683 - val_loss: 0.0171 - val_rmse: 0.0832\n",
      "Epoch 75/100\n",
      " - 0s - loss: 0.0224 - rmse: 0.0635 - val_loss: 0.0152 - val_rmse: 0.0720\n",
      "Epoch 76/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0622 - val_loss: 0.0125 - val_rmse: 0.0554\n",
      "Epoch 77/100\n",
      " - 0s - loss: 0.0250 - rmse: 0.0716 - val_loss: 0.0222 - val_rmse: 0.0983\n",
      "Epoch 78/100\n",
      " - 0s - loss: 0.0219 - rmse: 0.0636 - val_loss: 0.0146 - val_rmse: 0.0684\n",
      "Epoch 79/100\n",
      " - 0s - loss: 0.0242 - rmse: 0.0704 - val_loss: 0.0116 - val_rmse: 0.0509\n",
      "Epoch 80/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0638 - val_loss: 0.0124 - val_rmse: 0.0550\n",
      "Epoch 81/100\n",
      " - 0s - loss: 0.0223 - rmse: 0.0607 - val_loss: 0.0128 - val_rmse: 0.0580\n",
      "Epoch 82/100\n",
      " - 0s - loss: 0.0227 - rmse: 0.0658 - val_loss: 0.0115 - val_rmse: 0.0514\n",
      "Epoch 83/100\n",
      " - 0s - loss: 0.0243 - rmse: 0.0590 - val_loss: 0.0213 - val_rmse: 0.0999\n",
      "Epoch 84/100\n",
      " - 0s - loss: 0.0228 - rmse: 0.0682 - val_loss: 0.0140 - val_rmse: 0.0652\n",
      "Epoch 85/100\n",
      " - 0s - loss: 0.0237 - rmse: 0.0691 - val_loss: 0.0126 - val_rmse: 0.0586\n",
      "Epoch 86/100\n",
      " - 0s - loss: 0.0208 - rmse: 0.0572 - val_loss: 0.0145 - val_rmse: 0.0699\n",
      "Epoch 87/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0624 - val_loss: 0.0132 - val_rmse: 0.0633\n",
      "Epoch 88/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0629 - val_loss: 0.0121 - val_rmse: 0.0540\n",
      "Epoch 89/100\n",
      " - 0s - loss: 0.0218 - rmse: 0.0610 - val_loss: 0.0199 - val_rmse: 0.0937\n",
      "Epoch 90/100\n",
      " - 0s - loss: 0.0222 - rmse: 0.0622 - val_loss: 0.0217 - val_rmse: 0.0977\n",
      "Epoch 91/100\n",
      " - 0s - loss: 0.0253 - rmse: 0.0770 - val_loss: 0.0119 - val_rmse: 0.0575\n",
      "Epoch 92/100\n",
      " - 0s - loss: 0.0220 - rmse: 0.0620 - val_loss: 0.0181 - val_rmse: 0.0877\n",
      "Epoch 93/100\n",
      " - 0s - loss: 0.0210 - rmse: 0.0573 - val_loss: 0.0188 - val_rmse: 0.0914\n",
      "Epoch 94/100\n",
      " - 0s - loss: 0.0266 - rmse: 0.0802 - val_loss: 0.0146 - val_rmse: 0.0708\n",
      "Epoch 95/100\n",
      " - 0s - loss: 0.0221 - rmse: 0.0650 - val_loss: 0.0125 - val_rmse: 0.0637\n",
      "Epoch 96/100\n",
      " - 0s - loss: 0.0268 - rmse: 0.0815 - val_loss: 0.0111 - val_rmse: 0.0497\n",
      "Epoch 97/100\n",
      " - 0s - loss: 0.0225 - rmse: 0.0636 - val_loss: 0.0193 - val_rmse: 0.0943\n",
      "Epoch 98/100\n",
      " - 0s - loss: 0.0203 - rmse: 0.0556 - val_loss: 0.0313 - val_rmse: 0.1164\n",
      "Epoch 99/100\n",
      " - 0s - loss: 0.0207 - rmse: 0.0600 - val_loss: 0.0101 - val_rmse: 0.0461\n",
      "Epoch 100/100\n",
      " - 0s - loss: 0.0235 - rmse: 0.0669 - val_loss: 0.0138 - val_rmse: 0.0638\n",
      "0.01376068862736935\n",
      "测试集 rmse： 0.8950099922193167\n",
      "0.8367101648358881\n"
     ]
    }
   ],
   "source": [
    "from keras import backend\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return backend.sqrt(backend.mean(backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def pred_nn():\n",
    "    scaler = StandardScaler()\n",
    "    train_x = all_X_train.copy()\n",
    "    train_y = all_y_train.copy()\n",
    "    train_x = scaler.fit_transform(train_x)\n",
    "    \n",
    "#     train_x = train_x.fillna(0)\n",
    "#     test_x = test_x.fillna(0)\n",
    "    test_x = sub_data.copy()\n",
    "    test_x = scaler.transform(test_x)\n",
    "    \n",
    "    result = submit.copy()\n",
    "    rmse_score = 0\n",
    "    \n",
    "    for i in range(10):\n",
    "        X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=i)\n",
    "\n",
    "        X_train = X_train\n",
    "\n",
    "        row,col = X_train.shape\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(layers.Dense(32,activation='relu',input_shape=(col,)))\n",
    "#         model.add(layers.normalization.BatchNormalization())\n",
    "#         model.add(layers.core.Dropout(0.1))\n",
    "        model.add(layers.Dense(32,kernel_initializer='normal', activation='relu'))\n",
    "#         model.add(layers.core.Dropout(0.1))\n",
    "        model.add(layers.Dense(16,kernel_initializer='normal', activation='relu'))\n",
    "        model.add(layers.core.Dropout(0.1))\n",
    "        model.add(layers.Dense(8,kernel_initializer='normal', activation='relu'))\n",
    "        model.add(layers.core.Dropout(0.1))\n",
    "        \n",
    "        model.add(layers.Dense(1, kernel_initializer='normal'))\n",
    "\n",
    "        model.summary()\n",
    "        model.compile(optimizer=optimizers.Adadelta(),\n",
    "                      loss='mse',\n",
    "                      metrics=[rmse])\n",
    "\n",
    "        # callbacks_list = [\n",
    "        #     keras.callbacks.TensorBoard(\n",
    "        #         log_dir=\"LOF_drop_model_1\",\n",
    "        #         write_graph=True),\n",
    "        #     keras.callbacks.ModelCheckpoint(\n",
    "        #         filepath=\"LOF_drop_model_1.h5\",\n",
    "        #         monitor=\"val_acc\",\n",
    "        #         save_best_only=True\n",
    "        #     )\n",
    "        # ],validation_data=(X_test, y_test)validation_split=0.2,\n",
    "\n",
    "        model.fit(X_train, y_train, epochs=100, validation_data=(X_test, y_test), batch_size=100, verbose=2, shuffle=True)\n",
    "\n",
    "        y_prob_test = model.predict(X_test)\n",
    "\n",
    "        mse = mean_squared_error(y_test, y_prob_test)\n",
    "        print(mse)\n",
    "        score = 1/(1 + math.sqrt(mse))\n",
    "        rmse_score += score\n",
    "\n",
    "        print('测试集 rmse：',score)\n",
    "        y_prob = model.predict(test_x)\n",
    "#         print(y_prob)\n",
    "\n",
    "        result['发电量'] += y_prob[:, 0]\n",
    "    result['发电量'] = result['发电量']/10\n",
    "    \n",
    "    print(rmse_score/10)\n",
    "#     return result\n",
    "pred_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_lgb_CV():\n",
    "\n",
    "    auc_score = 0\n",
    "    test_id['prob'] = 0\n",
    "    for num in range(4):\n",
    "        print('第'+str(num)+'次交叉验证')\n",
    "        kf = KFold(n_splits = 10, random_state=100*num + 10, shuffle=True)\n",
    "        for train_ix, val_ix in kf.split(train):\n",
    "            \n",
    "            train_y = new_train.loc[train_ix,:]['label']\n",
    "            train_x = new_train.loc[train_ix,:].drop(['label','user_id'],axis=1)\n",
    "\n",
    "            X_train, X_test, y_train, y_test = train_test_split(train_x, train_y, test_size=0.2, random_state=num*20)\n",
    "            lgb_clf = lgb.LGBMClassifier(boosting_type='gbdt', num_leaves=31, max_depth=-1,\n",
    "                                learning_rate=0.01, n_estimators=2000, max_bin=225,\n",
    "                                min_child_weight=0.01, min_child_samples=20, subsample=0.7, subsample_freq=1,\n",
    "                                colsample_bytree=0.7, reg_alpha=0.0, reg_lambda=1, random_state=100*num+500, n_jobs=-1,\n",
    "                                )\n",
    "            lgb_clf.fit(X_train, y_train, eval_metric='auc', eval_set=(X_test, y_test), early_stopping_rounds=200)\n",
    "\n",
    "            auc_score += list(lgb_clf.best_score_.values())[0]['auc']\n",
    "            y_prob = lgb_clf.predict_proba(new_test_x, num_iteration=lgb_clf.best_iteration_)\n",
    "            test_id['prob'] += y_prob[:,1]\n",
    "            \n",
    "    test_id['prob'] = test_id['prob']/20\n",
    "    test_id.to_csv('lgb_linxi_080901.csv', index=False, header=False)\n",
    "\n",
    "    print(auc_score/20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>发电量</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.379993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>1.310647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13</td>\n",
       "      <td>2.148519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17</td>\n",
       "      <td>3.399214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>3.637901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>21</td>\n",
       "      <td>4.142491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23</td>\n",
       "      <td>4.267773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>25</td>\n",
       "      <td>4.791955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26</td>\n",
       "      <td>4.956942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>28</td>\n",
       "      <td>5.234955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>29</td>\n",
       "      <td>5.470411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>31</td>\n",
       "      <td>5.703498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32</td>\n",
       "      <td>5.905455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>33</td>\n",
       "      <td>6.091899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>36</td>\n",
       "      <td>6.411125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>37</td>\n",
       "      <td>6.661334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>38</td>\n",
       "      <td>6.824453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>40</td>\n",
       "      <td>7.067681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>41</td>\n",
       "      <td>7.142091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>46</td>\n",
       "      <td>7.526951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>48</td>\n",
       "      <td>7.729929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>50</td>\n",
       "      <td>7.976639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>51</td>\n",
       "      <td>8.032927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>53</td>\n",
       "      <td>8.194557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>54</td>\n",
       "      <td>8.302605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>55</td>\n",
       "      <td>8.357479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>56</td>\n",
       "      <td>8.416552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>59</td>\n",
       "      <td>8.757026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>62</td>\n",
       "      <td>8.871500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>69</td>\n",
       "      <td>9.265511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>70</td>\n",
       "      <td>9.311921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>72</td>\n",
       "      <td>9.412241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>81</td>\n",
       "      <td>9.721062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>82</td>\n",
       "      <td>9.749021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>83</td>\n",
       "      <td>9.731382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>85</td>\n",
       "      <td>9.821445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>86</td>\n",
       "      <td>9.859484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>89</td>\n",
       "      <td>9.831853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>95</td>\n",
       "      <td>9.828770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>96</td>\n",
       "      <td>9.824718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>98</td>\n",
       "      <td>9.806362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>100</td>\n",
       "      <td>9.760446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>102</td>\n",
       "      <td>9.740889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>103</td>\n",
       "      <td>9.745694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>104</td>\n",
       "      <td>9.623574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>107</td>\n",
       "      <td>9.551254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>108</td>\n",
       "      <td>9.519708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>109</td>\n",
       "      <td>9.442320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>110</td>\n",
       "      <td>9.303630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>111</td>\n",
       "      <td>9.257096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>112</td>\n",
       "      <td>9.230848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>116</td>\n",
       "      <td>9.151679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>118</td>\n",
       "      <td>9.072388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>120</td>\n",
       "      <td>8.900671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>121</td>\n",
       "      <td>8.826471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>128</td>\n",
       "      <td>8.315136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>129</td>\n",
       "      <td>8.257411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>130</td>\n",
       "      <td>8.128208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>132</td>\n",
       "      <td>8.145365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>135</td>\n",
       "      <td>8.152431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>137</td>\n",
       "      <td>7.660225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>138</td>\n",
       "      <td>7.593943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>139</td>\n",
       "      <td>7.554793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>140</td>\n",
       "      <td>7.513186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>142</td>\n",
       "      <td>7.308234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>143</td>\n",
       "      <td>7.237862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>144</td>\n",
       "      <td>7.091561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>146</td>\n",
       "      <td>6.912179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>148</td>\n",
       "      <td>6.710944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>151</td>\n",
       "      <td>6.390097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>152</td>\n",
       "      <td>6.282590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>153</td>\n",
       "      <td>6.182649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>154</td>\n",
       "      <td>5.775247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>155</td>\n",
       "      <td>5.612332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>159</td>\n",
       "      <td>5.069758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>161</td>\n",
       "      <td>4.655860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>162</td>\n",
       "      <td>4.483458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>163</td>\n",
       "      <td>4.180024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>165</td>\n",
       "      <td>3.757454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>167</td>\n",
       "      <td>3.497209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>168</td>\n",
       "      <td>3.316127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>177</td>\n",
       "      <td>0.245132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>196</td>\n",
       "      <td>0.084184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>197</td>\n",
       "      <td>0.112284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>198</td>\n",
       "      <td>0.235357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>200</td>\n",
       "      <td>0.850739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>201</td>\n",
       "      <td>1.420708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>203</td>\n",
       "      <td>1.711279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>207</td>\n",
       "      <td>1.839759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>209</td>\n",
       "      <td>1.741819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>210</td>\n",
       "      <td>1.344549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>211</td>\n",
       "      <td>1.336544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>212</td>\n",
       "      <td>1.385680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>213</td>\n",
       "      <td>1.441498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>217</td>\n",
       "      <td>1.874259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>220</td>\n",
       "      <td>2.036645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>221</td>\n",
       "      <td>1.827380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>225</td>\n",
       "      <td>2.106790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>230</td>\n",
       "      <td>1.828234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>235</td>\n",
       "      <td>2.047818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>237</td>\n",
       "      <td>1.951232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>241</td>\n",
       "      <td>2.056104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>242</td>\n",
       "      <td>2.093547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>243</td>\n",
       "      <td>2.326066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>244</td>\n",
       "      <td>2.710135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>246</td>\n",
       "      <td>2.365079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>250</td>\n",
       "      <td>2.443911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>251</td>\n",
       "      <td>2.430711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>252</td>\n",
       "      <td>2.365230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>255</td>\n",
       "      <td>2.428011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>256</td>\n",
       "      <td>2.872815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>260</td>\n",
       "      <td>2.544200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>263</td>\n",
       "      <td>2.127838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>264</td>\n",
       "      <td>2.148207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>265</td>\n",
       "      <td>2.172792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>266</td>\n",
       "      <td>2.139095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>268</td>\n",
       "      <td>2.251518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>271</td>\n",
       "      <td>2.436688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>272</td>\n",
       "      <td>2.342272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>276</td>\n",
       "      <td>2.509514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>277</td>\n",
       "      <td>2.612916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>278</td>\n",
       "      <td>2.322196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>279</td>\n",
       "      <td>2.337134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>282</td>\n",
       "      <td>2.703911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>284</td>\n",
       "      <td>3.378006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>289</td>\n",
       "      <td>3.837984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>291</td>\n",
       "      <td>2.364417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>292</td>\n",
       "      <td>2.305254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>298</td>\n",
       "      <td>2.105013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>300</td>\n",
       "      <td>2.318167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>301</td>\n",
       "      <td>2.709054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>304</td>\n",
       "      <td>2.650498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>305</td>\n",
       "      <td>2.563649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>307</td>\n",
       "      <td>3.512110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>308</td>\n",
       "      <td>8.984616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>310</td>\n",
       "      <td>10.589460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>312</td>\n",
       "      <td>9.656072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>313</td>\n",
       "      <td>9.297822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>315</td>\n",
       "      <td>9.057930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>320</td>\n",
       "      <td>8.602610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>324</td>\n",
       "      <td>7.636226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>326</td>\n",
       "      <td>8.403513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>327</td>\n",
       "      <td>8.302084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>329</td>\n",
       "      <td>7.257668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>332</td>\n",
       "      <td>7.616857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>336</td>\n",
       "      <td>6.844958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>337</td>\n",
       "      <td>7.138525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>340</td>\n",
       "      <td>6.641526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>342</td>\n",
       "      <td>6.231653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>343</td>\n",
       "      <td>6.104300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>344</td>\n",
       "      <td>5.945129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>345</td>\n",
       "      <td>5.634553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>347</td>\n",
       "      <td>5.217542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>349</td>\n",
       "      <td>4.958911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>350</td>\n",
       "      <td>4.806634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>351</td>\n",
       "      <td>4.522315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>355</td>\n",
       "      <td>4.060092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>356</td>\n",
       "      <td>3.933775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>358</td>\n",
       "      <td>3.342509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>359</td>\n",
       "      <td>3.197473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>362</td>\n",
       "      <td>2.587741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>363</td>\n",
       "      <td>2.180752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>366</td>\n",
       "      <td>0.263964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>392</td>\n",
       "      <td>2.222198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>394</td>\n",
       "      <td>2.633207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>397</td>\n",
       "      <td>3.764758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>398</td>\n",
       "      <td>3.968334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>403</td>\n",
       "      <td>5.085667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>406</td>\n",
       "      <td>5.432998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>407</td>\n",
       "      <td>5.568894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>408</td>\n",
       "      <td>5.747398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>410</td>\n",
       "      <td>6.142214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>415</td>\n",
       "      <td>6.656901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>416</td>\n",
       "      <td>6.821351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>417</td>\n",
       "      <td>6.890529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>418</td>\n",
       "      <td>6.951494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>419</td>\n",
       "      <td>7.136147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>420</td>\n",
       "      <td>7.249680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178</th>\n",
       "      <td>421</td>\n",
       "      <td>7.327195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>428</td>\n",
       "      <td>8.018916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>430</td>\n",
       "      <td>8.192014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>431</td>\n",
       "      <td>8.295383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>432</td>\n",
       "      <td>8.337752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>437</td>\n",
       "      <td>8.702291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>439</td>\n",
       "      <td>8.773325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>441</td>\n",
       "      <td>8.878181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>442</td>\n",
       "      <td>8.937398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>443</td>\n",
       "      <td>9.017538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>444</td>\n",
       "      <td>9.050070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>445</td>\n",
       "      <td>9.072912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>448</td>\n",
       "      <td>9.226300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>449</td>\n",
       "      <td>9.268350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>451</td>\n",
       "      <td>9.344818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>453</td>\n",
       "      <td>9.481102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>457</td>\n",
       "      <td>9.608223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>458</td>\n",
       "      <td>9.583856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>459</td>\n",
       "      <td>9.590272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>461</td>\n",
       "      <td>9.594298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>464</td>\n",
       "      <td>9.660141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>465</td>\n",
       "      <td>9.672235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>468</td>\n",
       "      <td>9.697869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>470</td>\n",
       "      <td>9.674090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>471</td>\n",
       "      <td>9.666163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>472</td>\n",
       "      <td>9.660069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>473</td>\n",
       "      <td>9.627389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>205</th>\n",
       "      <td>474</td>\n",
       "      <td>9.674869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>475</td>\n",
       "      <td>9.645995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>477</td>\n",
       "      <td>9.569489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>478</td>\n",
       "      <td>9.565350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>481</td>\n",
       "      <td>9.519655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>482</td>\n",
       "      <td>9.484282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>483</td>\n",
       "      <td>9.475821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>484</td>\n",
       "      <td>9.447386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>486</td>\n",
       "      <td>9.340163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>487</td>\n",
       "      <td>9.337240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>489</td>\n",
       "      <td>9.249775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>492</td>\n",
       "      <td>9.112210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>493</td>\n",
       "      <td>9.038697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>496</td>\n",
       "      <td>8.893991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>499</td>\n",
       "      <td>8.687007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>501</td>\n",
       "      <td>8.633474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>507</td>\n",
       "      <td>8.168039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>514</td>\n",
       "      <td>7.577892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>516</td>\n",
       "      <td>7.357052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>519</td>\n",
       "      <td>7.188211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>520</td>\n",
       "      <td>7.239564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>525</td>\n",
       "      <td>6.596260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>227</th>\n",
       "      <td>529</td>\n",
       "      <td>6.059739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>530</td>\n",
       "      <td>5.918167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>229</th>\n",
       "      <td>531</td>\n",
       "      <td>5.763529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>533</td>\n",
       "      <td>5.505698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>535</td>\n",
       "      <td>5.146322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>538</td>\n",
       "      <td>4.804108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>539</td>\n",
       "      <td>4.742778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>541</td>\n",
       "      <td>4.204543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>543</td>\n",
       "      <td>4.386807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>544</td>\n",
       "      <td>3.999766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>545</td>\n",
       "      <td>3.847994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>549</td>\n",
       "      <td>2.953631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>552</td>\n",
       "      <td>2.254573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240</th>\n",
       "      <td>553</td>\n",
       "      <td>1.854805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>554</td>\n",
       "      <td>1.044211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>555</td>\n",
       "      <td>0.344488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>577</td>\n",
       "      <td>0.070216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>578</td>\n",
       "      <td>0.074909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>579</td>\n",
       "      <td>0.110094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>580</td>\n",
       "      <td>0.143077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>581</td>\n",
       "      <td>0.207728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>588</td>\n",
       "      <td>0.297385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>590</td>\n",
       "      <td>0.361818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8159</th>\n",
       "      <td>17360</td>\n",
       "      <td>1.005729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8160</th>\n",
       "      <td>17361</td>\n",
       "      <td>1.215726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8161</th>\n",
       "      <td>17366</td>\n",
       "      <td>0.699498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8162</th>\n",
       "      <td>17368</td>\n",
       "      <td>0.433803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8163</th>\n",
       "      <td>17369</td>\n",
       "      <td>0.433777</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8164</th>\n",
       "      <td>17370</td>\n",
       "      <td>0.329513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8165</th>\n",
       "      <td>17372</td>\n",
       "      <td>0.462134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8166</th>\n",
       "      <td>17373</td>\n",
       "      <td>0.919635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8167</th>\n",
       "      <td>17375</td>\n",
       "      <td>1.365458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8168</th>\n",
       "      <td>17376</td>\n",
       "      <td>2.068390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8169</th>\n",
       "      <td>17377</td>\n",
       "      <td>1.617104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8170</th>\n",
       "      <td>17379</td>\n",
       "      <td>1.573545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8171</th>\n",
       "      <td>17380</td>\n",
       "      <td>1.691766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>17381</td>\n",
       "      <td>1.486004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8173</th>\n",
       "      <td>17384</td>\n",
       "      <td>1.407870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8174</th>\n",
       "      <td>17385</td>\n",
       "      <td>1.235580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8175</th>\n",
       "      <td>17388</td>\n",
       "      <td>1.695406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>17389</td>\n",
       "      <td>1.533878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8177</th>\n",
       "      <td>17392</td>\n",
       "      <td>2.608200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8178</th>\n",
       "      <td>17395</td>\n",
       "      <td>2.615252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8179</th>\n",
       "      <td>17397</td>\n",
       "      <td>1.599463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8180</th>\n",
       "      <td>17398</td>\n",
       "      <td>1.883683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>17399</td>\n",
       "      <td>1.844434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8182</th>\n",
       "      <td>17402</td>\n",
       "      <td>2.596383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8183</th>\n",
       "      <td>17405</td>\n",
       "      <td>1.919704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8184</th>\n",
       "      <td>17406</td>\n",
       "      <td>1.636135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8185</th>\n",
       "      <td>17407</td>\n",
       "      <td>2.012003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>17408</td>\n",
       "      <td>2.134157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>17410</td>\n",
       "      <td>1.887697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>17413</td>\n",
       "      <td>1.578231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>17414</td>\n",
       "      <td>1.777858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>17415</td>\n",
       "      <td>1.699387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>17419</td>\n",
       "      <td>1.174967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8192</th>\n",
       "      <td>17420</td>\n",
       "      <td>1.176794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8193</th>\n",
       "      <td>17421</td>\n",
       "      <td>1.023949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8194</th>\n",
       "      <td>17423</td>\n",
       "      <td>1.268186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8195</th>\n",
       "      <td>17430</td>\n",
       "      <td>2.558648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8196</th>\n",
       "      <td>17434</td>\n",
       "      <td>2.205204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8197</th>\n",
       "      <td>17435</td>\n",
       "      <td>2.260396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8198</th>\n",
       "      <td>17436</td>\n",
       "      <td>2.188673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8199</th>\n",
       "      <td>17437</td>\n",
       "      <td>1.935821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8200</th>\n",
       "      <td>17438</td>\n",
       "      <td>1.919375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8201</th>\n",
       "      <td>17442</td>\n",
       "      <td>1.455720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8202</th>\n",
       "      <td>17448</td>\n",
       "      <td>0.966590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8203</th>\n",
       "      <td>17451</td>\n",
       "      <td>1.148498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8204</th>\n",
       "      <td>17452</td>\n",
       "      <td>1.033140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8205</th>\n",
       "      <td>17454</td>\n",
       "      <td>0.681068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8206</th>\n",
       "      <td>17457</td>\n",
       "      <td>0.870090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8207</th>\n",
       "      <td>17460</td>\n",
       "      <td>0.767593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8208</th>\n",
       "      <td>17463</td>\n",
       "      <td>0.758075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8209</th>\n",
       "      <td>17464</td>\n",
       "      <td>0.647016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8210</th>\n",
       "      <td>17465</td>\n",
       "      <td>0.620085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8211</th>\n",
       "      <td>17467</td>\n",
       "      <td>0.683314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8212</th>\n",
       "      <td>17468</td>\n",
       "      <td>0.643761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8213</th>\n",
       "      <td>17469</td>\n",
       "      <td>0.873540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8214</th>\n",
       "      <td>17470</td>\n",
       "      <td>0.789292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8215</th>\n",
       "      <td>17471</td>\n",
       "      <td>0.991403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8216</th>\n",
       "      <td>17472</td>\n",
       "      <td>0.509876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8217</th>\n",
       "      <td>17475</td>\n",
       "      <td>0.463583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8218</th>\n",
       "      <td>17482</td>\n",
       "      <td>0.727152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8219</th>\n",
       "      <td>17483</td>\n",
       "      <td>0.549597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8220</th>\n",
       "      <td>17485</td>\n",
       "      <td>0.597505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8221</th>\n",
       "      <td>17486</td>\n",
       "      <td>0.472386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8222</th>\n",
       "      <td>17487</td>\n",
       "      <td>0.395305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8223</th>\n",
       "      <td>17488</td>\n",
       "      <td>0.395478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8224</th>\n",
       "      <td>17489</td>\n",
       "      <td>0.537458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8225</th>\n",
       "      <td>17493</td>\n",
       "      <td>0.453321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8226</th>\n",
       "      <td>17494</td>\n",
       "      <td>0.543678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8227</th>\n",
       "      <td>17495</td>\n",
       "      <td>0.563146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8228</th>\n",
       "      <td>17497</td>\n",
       "      <td>0.234414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8229</th>\n",
       "      <td>17498</td>\n",
       "      <td>0.451726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8230</th>\n",
       "      <td>17499</td>\n",
       "      <td>0.459720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8231</th>\n",
       "      <td>17501</td>\n",
       "      <td>0.212075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8232</th>\n",
       "      <td>17502</td>\n",
       "      <td>0.278195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8233</th>\n",
       "      <td>17505</td>\n",
       "      <td>0.276902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8234</th>\n",
       "      <td>17510</td>\n",
       "      <td>0.340757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8235</th>\n",
       "      <td>17511</td>\n",
       "      <td>0.325454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8236</th>\n",
       "      <td>17513</td>\n",
       "      <td>0.209315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8237</th>\n",
       "      <td>17516</td>\n",
       "      <td>0.235686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8238</th>\n",
       "      <td>17521</td>\n",
       "      <td>0.316776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8239</th>\n",
       "      <td>17523</td>\n",
       "      <td>0.247224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8240</th>\n",
       "      <td>17525</td>\n",
       "      <td>0.203133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8241</th>\n",
       "      <td>17526</td>\n",
       "      <td>0.190588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8242</th>\n",
       "      <td>17527</td>\n",
       "      <td>0.379993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8243</th>\n",
       "      <td>17528</td>\n",
       "      <td>0.122575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8244</th>\n",
       "      <td>17529</td>\n",
       "      <td>0.290566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8245</th>\n",
       "      <td>17531</td>\n",
       "      <td>0.178410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8246</th>\n",
       "      <td>17532</td>\n",
       "      <td>0.251240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8247</th>\n",
       "      <td>17536</td>\n",
       "      <td>0.099781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8248</th>\n",
       "      <td>17537</td>\n",
       "      <td>0.219136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8249</th>\n",
       "      <td>17539</td>\n",
       "      <td>0.169437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8250</th>\n",
       "      <td>17548</td>\n",
       "      <td>3.728633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8251</th>\n",
       "      <td>17550</td>\n",
       "      <td>4.137992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8252</th>\n",
       "      <td>17553</td>\n",
       "      <td>4.554413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8253</th>\n",
       "      <td>17554</td>\n",
       "      <td>4.770646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8254</th>\n",
       "      <td>17556</td>\n",
       "      <td>5.101757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8255</th>\n",
       "      <td>17559</td>\n",
       "      <td>5.434134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8256</th>\n",
       "      <td>17561</td>\n",
       "      <td>5.748299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8257</th>\n",
       "      <td>17566</td>\n",
       "      <td>6.144857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8258</th>\n",
       "      <td>17567</td>\n",
       "      <td>6.233299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8259</th>\n",
       "      <td>17568</td>\n",
       "      <td>6.470123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8260</th>\n",
       "      <td>17571</td>\n",
       "      <td>6.696871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8261</th>\n",
       "      <td>17573</td>\n",
       "      <td>7.035668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8262</th>\n",
       "      <td>17577</td>\n",
       "      <td>7.541470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8263</th>\n",
       "      <td>17579</td>\n",
       "      <td>7.836238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8264</th>\n",
       "      <td>17580</td>\n",
       "      <td>7.862036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8265</th>\n",
       "      <td>17581</td>\n",
       "      <td>8.034094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8266</th>\n",
       "      <td>17582</td>\n",
       "      <td>8.050190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8267</th>\n",
       "      <td>17585</td>\n",
       "      <td>8.183679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8268</th>\n",
       "      <td>17587</td>\n",
       "      <td>8.635865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8269</th>\n",
       "      <td>17590</td>\n",
       "      <td>8.859462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8270</th>\n",
       "      <td>17592</td>\n",
       "      <td>8.945020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8271</th>\n",
       "      <td>17593</td>\n",
       "      <td>8.877725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8272</th>\n",
       "      <td>17595</td>\n",
       "      <td>9.289232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8273</th>\n",
       "      <td>17599</td>\n",
       "      <td>9.600449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8274</th>\n",
       "      <td>17600</td>\n",
       "      <td>9.596727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8275</th>\n",
       "      <td>17601</td>\n",
       "      <td>9.672787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8276</th>\n",
       "      <td>17602</td>\n",
       "      <td>9.779665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8277</th>\n",
       "      <td>17604</td>\n",
       "      <td>9.645980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8278</th>\n",
       "      <td>17606</td>\n",
       "      <td>9.970831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8279</th>\n",
       "      <td>17607</td>\n",
       "      <td>10.007892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8280</th>\n",
       "      <td>17609</td>\n",
       "      <td>10.171494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8281</th>\n",
       "      <td>17610</td>\n",
       "      <td>10.146743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8282</th>\n",
       "      <td>17611</td>\n",
       "      <td>10.199361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8283</th>\n",
       "      <td>17613</td>\n",
       "      <td>10.325161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8284</th>\n",
       "      <td>17615</td>\n",
       "      <td>10.339414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8285</th>\n",
       "      <td>17617</td>\n",
       "      <td>10.461532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8286</th>\n",
       "      <td>17619</td>\n",
       "      <td>10.548253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8287</th>\n",
       "      <td>17620</td>\n",
       "      <td>10.593583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8288</th>\n",
       "      <td>17621</td>\n",
       "      <td>10.648318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8289</th>\n",
       "      <td>17623</td>\n",
       "      <td>10.653581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8290</th>\n",
       "      <td>17627</td>\n",
       "      <td>10.645057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8291</th>\n",
       "      <td>17628</td>\n",
       "      <td>10.624866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8292</th>\n",
       "      <td>17634</td>\n",
       "      <td>10.776289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8293</th>\n",
       "      <td>17636</td>\n",
       "      <td>10.956407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8294</th>\n",
       "      <td>17637</td>\n",
       "      <td>11.007053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8295</th>\n",
       "      <td>17639</td>\n",
       "      <td>10.940367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8296</th>\n",
       "      <td>17641</td>\n",
       "      <td>10.887003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8297</th>\n",
       "      <td>17642</td>\n",
       "      <td>10.847633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8298</th>\n",
       "      <td>17650</td>\n",
       "      <td>10.825756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8299</th>\n",
       "      <td>17651</td>\n",
       "      <td>10.861789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>17653</td>\n",
       "      <td>10.846521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>17655</td>\n",
       "      <td>10.802960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>17661</td>\n",
       "      <td>10.658774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303</th>\n",
       "      <td>17663</td>\n",
       "      <td>10.580125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>17664</td>\n",
       "      <td>10.445233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8305</th>\n",
       "      <td>17665</td>\n",
       "      <td>10.488573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8306</th>\n",
       "      <td>17666</td>\n",
       "      <td>10.354250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8307</th>\n",
       "      <td>17667</td>\n",
       "      <td>10.109492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8308</th>\n",
       "      <td>17669</td>\n",
       "      <td>10.216032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8309</th>\n",
       "      <td>17670</td>\n",
       "      <td>10.222556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8310</th>\n",
       "      <td>17671</td>\n",
       "      <td>10.116428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8311</th>\n",
       "      <td>17673</td>\n",
       "      <td>10.074059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8312</th>\n",
       "      <td>17674</td>\n",
       "      <td>9.864921</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8313</th>\n",
       "      <td>17676</td>\n",
       "      <td>9.627611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8314</th>\n",
       "      <td>17677</td>\n",
       "      <td>9.612117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8315</th>\n",
       "      <td>17678</td>\n",
       "      <td>9.587343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8316</th>\n",
       "      <td>17681</td>\n",
       "      <td>9.480658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8317</th>\n",
       "      <td>17682</td>\n",
       "      <td>9.351024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8318</th>\n",
       "      <td>17685</td>\n",
       "      <td>9.055952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8319</th>\n",
       "      <td>17689</td>\n",
       "      <td>8.645376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8320</th>\n",
       "      <td>17691</td>\n",
       "      <td>8.541692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8321</th>\n",
       "      <td>17692</td>\n",
       "      <td>8.353376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8322</th>\n",
       "      <td>17693</td>\n",
       "      <td>8.310928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8323</th>\n",
       "      <td>17696</td>\n",
       "      <td>7.905647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8324</th>\n",
       "      <td>17698</td>\n",
       "      <td>7.970684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8325</th>\n",
       "      <td>17701</td>\n",
       "      <td>7.394595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>17702</td>\n",
       "      <td>7.118932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8327</th>\n",
       "      <td>17704</td>\n",
       "      <td>6.865413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8328</th>\n",
       "      <td>17706</td>\n",
       "      <td>6.542598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8329</th>\n",
       "      <td>17707</td>\n",
       "      <td>6.336975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8330</th>\n",
       "      <td>17709</td>\n",
       "      <td>6.049479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8331</th>\n",
       "      <td>17711</td>\n",
       "      <td>5.931909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8332</th>\n",
       "      <td>17713</td>\n",
       "      <td>5.662599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8333</th>\n",
       "      <td>17717</td>\n",
       "      <td>4.987594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8334</th>\n",
       "      <td>17721</td>\n",
       "      <td>4.409462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8335</th>\n",
       "      <td>17722</td>\n",
       "      <td>4.316736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8336</th>\n",
       "      <td>17725</td>\n",
       "      <td>3.835152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8337</th>\n",
       "      <td>17727</td>\n",
       "      <td>3.634722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8338</th>\n",
       "      <td>17731</td>\n",
       "      <td>3.083607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8339</th>\n",
       "      <td>17734</td>\n",
       "      <td>2.486251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8340</th>\n",
       "      <td>17737</td>\n",
       "      <td>2.031687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8341</th>\n",
       "      <td>17738</td>\n",
       "      <td>1.969037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8342</th>\n",
       "      <td>17739</td>\n",
       "      <td>1.836407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8343</th>\n",
       "      <td>17743</td>\n",
       "      <td>1.443910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8344</th>\n",
       "      <td>17747</td>\n",
       "      <td>3.384284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8345</th>\n",
       "      <td>17748</td>\n",
       "      <td>3.505647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8346</th>\n",
       "      <td>17749</td>\n",
       "      <td>3.634318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8347</th>\n",
       "      <td>17750</td>\n",
       "      <td>3.695816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8348</th>\n",
       "      <td>17752</td>\n",
       "      <td>3.940835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8349</th>\n",
       "      <td>17753</td>\n",
       "      <td>4.138375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8350</th>\n",
       "      <td>17755</td>\n",
       "      <td>4.331132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8351</th>\n",
       "      <td>17762</td>\n",
       "      <td>5.314666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8352</th>\n",
       "      <td>17763</td>\n",
       "      <td>5.444903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8353</th>\n",
       "      <td>17771</td>\n",
       "      <td>6.274039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8354</th>\n",
       "      <td>17775</td>\n",
       "      <td>6.925983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8355</th>\n",
       "      <td>17777</td>\n",
       "      <td>7.161341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8356</th>\n",
       "      <td>17778</td>\n",
       "      <td>7.139262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8357</th>\n",
       "      <td>17779</td>\n",
       "      <td>7.331574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8358</th>\n",
       "      <td>17780</td>\n",
       "      <td>7.415808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8359</th>\n",
       "      <td>17781</td>\n",
       "      <td>7.547741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8360</th>\n",
       "      <td>17783</td>\n",
       "      <td>7.759238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8361</th>\n",
       "      <td>17784</td>\n",
       "      <td>7.977719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8362</th>\n",
       "      <td>17788</td>\n",
       "      <td>8.195721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8363</th>\n",
       "      <td>17789</td>\n",
       "      <td>8.261116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8364</th>\n",
       "      <td>17791</td>\n",
       "      <td>8.557284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8365</th>\n",
       "      <td>17793</td>\n",
       "      <td>8.748883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8366</th>\n",
       "      <td>17794</td>\n",
       "      <td>8.862028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8367</th>\n",
       "      <td>17795</td>\n",
       "      <td>8.904456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8368</th>\n",
       "      <td>17798</td>\n",
       "      <td>9.026064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8369</th>\n",
       "      <td>17799</td>\n",
       "      <td>8.987906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8370</th>\n",
       "      <td>17800</td>\n",
       "      <td>9.014552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>17801</td>\n",
       "      <td>9.294529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>17802</td>\n",
       "      <td>9.461701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>17803</td>\n",
       "      <td>9.482594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>17805</td>\n",
       "      <td>9.483438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>17812</td>\n",
       "      <td>9.719305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8376</th>\n",
       "      <td>17813</td>\n",
       "      <td>9.676747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8377</th>\n",
       "      <td>17815</td>\n",
       "      <td>9.651715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8378</th>\n",
       "      <td>17817</td>\n",
       "      <td>9.807773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8379</th>\n",
       "      <td>17818</td>\n",
       "      <td>10.015129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8380</th>\n",
       "      <td>17820</td>\n",
       "      <td>9.993725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8381</th>\n",
       "      <td>17822</td>\n",
       "      <td>10.173583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8382</th>\n",
       "      <td>17824</td>\n",
       "      <td>10.339922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8383</th>\n",
       "      <td>17825</td>\n",
       "      <td>10.211312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8384</th>\n",
       "      <td>17832</td>\n",
       "      <td>10.197531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8385</th>\n",
       "      <td>17833</td>\n",
       "      <td>10.425416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8386</th>\n",
       "      <td>17835</td>\n",
       "      <td>10.264581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8387</th>\n",
       "      <td>17838</td>\n",
       "      <td>10.422482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8388</th>\n",
       "      <td>17839</td>\n",
       "      <td>10.428012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8389</th>\n",
       "      <td>17840</td>\n",
       "      <td>10.532358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8390</th>\n",
       "      <td>17844</td>\n",
       "      <td>10.238992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8391</th>\n",
       "      <td>17845</td>\n",
       "      <td>10.513745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8392</th>\n",
       "      <td>17850</td>\n",
       "      <td>8.129732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8393</th>\n",
       "      <td>17852</td>\n",
       "      <td>7.943417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8394</th>\n",
       "      <td>17853</td>\n",
       "      <td>8.910582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8395</th>\n",
       "      <td>17855</td>\n",
       "      <td>8.508670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8396</th>\n",
       "      <td>17856</td>\n",
       "      <td>7.704780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8397</th>\n",
       "      <td>17857</td>\n",
       "      <td>8.167097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8398</th>\n",
       "      <td>17859</td>\n",
       "      <td>9.796024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8399</th>\n",
       "      <td>17861</td>\n",
       "      <td>10.383762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8400</th>\n",
       "      <td>17863</td>\n",
       "      <td>10.143166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8401</th>\n",
       "      <td>17864</td>\n",
       "      <td>8.866513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8402</th>\n",
       "      <td>17865</td>\n",
       "      <td>9.873160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>17868</td>\n",
       "      <td>8.248922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8404</th>\n",
       "      <td>17869</td>\n",
       "      <td>8.223208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8405</th>\n",
       "      <td>17870</td>\n",
       "      <td>10.046501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8406</th>\n",
       "      <td>17871</td>\n",
       "      <td>9.897205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>17872</td>\n",
       "      <td>9.820846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8408</th>\n",
       "      <td>17875</td>\n",
       "      <td>9.122500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8409 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID        发电量\n",
       "0         1   0.379993\n",
       "1         9   1.310647\n",
       "2        13   2.148519\n",
       "3        17   3.399214\n",
       "4        18   3.637901\n",
       "5        21   4.142491\n",
       "6        23   4.267773\n",
       "7        25   4.791955\n",
       "8        26   4.956942\n",
       "9        28   5.234955\n",
       "10       29   5.470411\n",
       "11       31   5.703498\n",
       "12       32   5.905455\n",
       "13       33   6.091899\n",
       "14       36   6.411125\n",
       "15       37   6.661334\n",
       "16       38   6.824453\n",
       "17       40   7.067681\n",
       "18       41   7.142091\n",
       "19       46   7.526951\n",
       "20       48   7.729929\n",
       "21       50   7.976639\n",
       "22       51   8.032927\n",
       "23       53   8.194557\n",
       "24       54   8.302605\n",
       "25       55   8.357479\n",
       "26       56   8.416552\n",
       "27       59   8.757026\n",
       "28       62   8.871500\n",
       "29       69   9.265511\n",
       "30       70   9.311921\n",
       "31       72   9.412241\n",
       "32       81   9.721062\n",
       "33       82   9.749021\n",
       "34       83   9.731382\n",
       "35       85   9.821445\n",
       "36       86   9.859484\n",
       "37       89   9.831853\n",
       "38       95   9.828770\n",
       "39       96   9.824718\n",
       "40       98   9.806362\n",
       "41      100   9.760446\n",
       "42      102   9.740889\n",
       "43      103   9.745694\n",
       "44      104   9.623574\n",
       "45      107   9.551254\n",
       "46      108   9.519708\n",
       "47      109   9.442320\n",
       "48      110   9.303630\n",
       "49      111   9.257096\n",
       "50      112   9.230848\n",
       "51      116   9.151679\n",
       "52      118   9.072388\n",
       "53      120   8.900671\n",
       "54      121   8.826471\n",
       "55      128   8.315136\n",
       "56      129   8.257411\n",
       "57      130   8.128208\n",
       "58      132   8.145365\n",
       "59      135   8.152431\n",
       "60      137   7.660225\n",
       "61      138   7.593943\n",
       "62      139   7.554793\n",
       "63      140   7.513186\n",
       "64      142   7.308234\n",
       "65      143   7.237862\n",
       "66      144   7.091561\n",
       "67      146   6.912179\n",
       "68      148   6.710944\n",
       "69      151   6.390097\n",
       "70      152   6.282590\n",
       "71      153   6.182649\n",
       "72      154   5.775247\n",
       "73      155   5.612332\n",
       "74      159   5.069758\n",
       "75      161   4.655860\n",
       "76      162   4.483458\n",
       "77      163   4.180024\n",
       "78      165   3.757454\n",
       "79      167   3.497209\n",
       "80      168   3.316127\n",
       "81      177   0.245132\n",
       "82      196   0.084184\n",
       "83      197   0.112284\n",
       "84      198   0.235357\n",
       "85      200   0.850739\n",
       "86      201   1.420708\n",
       "87      203   1.711279\n",
       "88      207   1.839759\n",
       "89      209   1.741819\n",
       "90      210   1.344549\n",
       "91      211   1.336544\n",
       "92      212   1.385680\n",
       "93      213   1.441498\n",
       "94      217   1.874259\n",
       "95      220   2.036645\n",
       "96      221   1.827380\n",
       "97      225   2.106790\n",
       "98      230   1.828234\n",
       "99      235   2.047818\n",
       "100     237   1.951232\n",
       "101     241   2.056104\n",
       "102     242   2.093547\n",
       "103     243   2.326066\n",
       "104     244   2.710135\n",
       "105     246   2.365079\n",
       "106     250   2.443911\n",
       "107     251   2.430711\n",
       "108     252   2.365230\n",
       "109     255   2.428011\n",
       "110     256   2.872815\n",
       "111     260   2.544200\n",
       "112     263   2.127838\n",
       "113     264   2.148207\n",
       "114     265   2.172792\n",
       "115     266   2.139095\n",
       "116     268   2.251518\n",
       "117     271   2.436688\n",
       "118     272   2.342272\n",
       "119     276   2.509514\n",
       "120     277   2.612916\n",
       "121     278   2.322196\n",
       "122     279   2.337134\n",
       "123     282   2.703911\n",
       "124     284   3.378006\n",
       "125     289   3.837984\n",
       "126     291   2.364417\n",
       "127     292   2.305254\n",
       "128     298   2.105013\n",
       "129     300   2.318167\n",
       "130     301   2.709054\n",
       "131     304   2.650498\n",
       "132     305   2.563649\n",
       "133     307   3.512110\n",
       "134     308   8.984616\n",
       "135     310  10.589460\n",
       "136     312   9.656072\n",
       "137     313   9.297822\n",
       "138     315   9.057930\n",
       "139     320   8.602610\n",
       "140     324   7.636226\n",
       "141     326   8.403513\n",
       "142     327   8.302084\n",
       "143     329   7.257668\n",
       "144     332   7.616857\n",
       "145     336   6.844958\n",
       "146     337   7.138525\n",
       "147     340   6.641526\n",
       "148     342   6.231653\n",
       "149     343   6.104300\n",
       "150     344   5.945129\n",
       "151     345   5.634553\n",
       "152     347   5.217542\n",
       "153     349   4.958911\n",
       "154     350   4.806634\n",
       "155     351   4.522315\n",
       "156     355   4.060092\n",
       "157     356   3.933775\n",
       "158     358   3.342509\n",
       "159     359   3.197473\n",
       "160     362   2.587741\n",
       "161     363   2.180752\n",
       "162     366   0.263964\n",
       "163     392   2.222198\n",
       "164     394   2.633207\n",
       "165     397   3.764758\n",
       "166     398   3.968334\n",
       "167     403   5.085667\n",
       "168     406   5.432998\n",
       "169     407   5.568894\n",
       "170     408   5.747398\n",
       "171     410   6.142214\n",
       "172     415   6.656901\n",
       "173     416   6.821351\n",
       "174     417   6.890529\n",
       "175     418   6.951494\n",
       "176     419   7.136147\n",
       "177     420   7.249680\n",
       "178     421   7.327195\n",
       "179     428   8.018916\n",
       "180     430   8.192014\n",
       "181     431   8.295383\n",
       "182     432   8.337752\n",
       "183     437   8.702291\n",
       "184     439   8.773325\n",
       "185     441   8.878181\n",
       "186     442   8.937398\n",
       "187     443   9.017538\n",
       "188     444   9.050070\n",
       "189     445   9.072912\n",
       "190     448   9.226300\n",
       "191     449   9.268350\n",
       "192     451   9.344818\n",
       "193     453   9.481102\n",
       "194     457   9.608223\n",
       "195     458   9.583856\n",
       "196     459   9.590272\n",
       "197     461   9.594298\n",
       "198     464   9.660141\n",
       "199     465   9.672235\n",
       "200     468   9.697869\n",
       "201     470   9.674090\n",
       "202     471   9.666163\n",
       "203     472   9.660069\n",
       "204     473   9.627389\n",
       "205     474   9.674869\n",
       "206     475   9.645995\n",
       "207     477   9.569489\n",
       "208     478   9.565350\n",
       "209     481   9.519655\n",
       "210     482   9.484282\n",
       "211     483   9.475821\n",
       "212     484   9.447386\n",
       "213     486   9.340163\n",
       "214     487   9.337240\n",
       "215     489   9.249775\n",
       "216     492   9.112210\n",
       "217     493   9.038697\n",
       "218     496   8.893991\n",
       "219     499   8.687007\n",
       "220     501   8.633474\n",
       "221     507   8.168039\n",
       "222     514   7.577892\n",
       "223     516   7.357052\n",
       "224     519   7.188211\n",
       "225     520   7.239564\n",
       "226     525   6.596260\n",
       "227     529   6.059739\n",
       "228     530   5.918167\n",
       "229     531   5.763529\n",
       "230     533   5.505698\n",
       "231     535   5.146322\n",
       "232     538   4.804108\n",
       "233     539   4.742778\n",
       "234     541   4.204543\n",
       "235     543   4.386807\n",
       "236     544   3.999766\n",
       "237     545   3.847994\n",
       "238     549   2.953631\n",
       "239     552   2.254573\n",
       "240     553   1.854805\n",
       "241     554   1.044211\n",
       "242     555   0.344488\n",
       "243     577   0.070216\n",
       "244     578   0.074909\n",
       "245     579   0.110094\n",
       "246     580   0.143077\n",
       "247     581   0.207728\n",
       "248     588   0.297385\n",
       "249     590   0.361818\n",
       "...     ...        ...\n",
       "8159  17360   1.005729\n",
       "8160  17361   1.215726\n",
       "8161  17366   0.699498\n",
       "8162  17368   0.433803\n",
       "8163  17369   0.433777\n",
       "8164  17370   0.329513\n",
       "8165  17372   0.462134\n",
       "8166  17373   0.919635\n",
       "8167  17375   1.365458\n",
       "8168  17376   2.068390\n",
       "8169  17377   1.617104\n",
       "8170  17379   1.573545\n",
       "8171  17380   1.691766\n",
       "8172  17381   1.486004\n",
       "8173  17384   1.407870\n",
       "8174  17385   1.235580\n",
       "8175  17388   1.695406\n",
       "8176  17389   1.533878\n",
       "8177  17392   2.608200\n",
       "8178  17395   2.615252\n",
       "8179  17397   1.599463\n",
       "8180  17398   1.883683\n",
       "8181  17399   1.844434\n",
       "8182  17402   2.596383\n",
       "8183  17405   1.919704\n",
       "8184  17406   1.636135\n",
       "8185  17407   2.012003\n",
       "8186  17408   2.134157\n",
       "8187  17410   1.887697\n",
       "8188  17413   1.578231\n",
       "8189  17414   1.777858\n",
       "8190  17415   1.699387\n",
       "8191  17419   1.174967\n",
       "8192  17420   1.176794\n",
       "8193  17421   1.023949\n",
       "8194  17423   1.268186\n",
       "8195  17430   2.558648\n",
       "8196  17434   2.205204\n",
       "8197  17435   2.260396\n",
       "8198  17436   2.188673\n",
       "8199  17437   1.935821\n",
       "8200  17438   1.919375\n",
       "8201  17442   1.455720\n",
       "8202  17448   0.966590\n",
       "8203  17451   1.148498\n",
       "8204  17452   1.033140\n",
       "8205  17454   0.681068\n",
       "8206  17457   0.870090\n",
       "8207  17460   0.767593\n",
       "8208  17463   0.758075\n",
       "8209  17464   0.647016\n",
       "8210  17465   0.620085\n",
       "8211  17467   0.683314\n",
       "8212  17468   0.643761\n",
       "8213  17469   0.873540\n",
       "8214  17470   0.789292\n",
       "8215  17471   0.991403\n",
       "8216  17472   0.509876\n",
       "8217  17475   0.463583\n",
       "8218  17482   0.727152\n",
       "8219  17483   0.549597\n",
       "8220  17485   0.597505\n",
       "8221  17486   0.472386\n",
       "8222  17487   0.395305\n",
       "8223  17488   0.395478\n",
       "8224  17489   0.537458\n",
       "8225  17493   0.453321\n",
       "8226  17494   0.543678\n",
       "8227  17495   0.563146\n",
       "8228  17497   0.234414\n",
       "8229  17498   0.451726\n",
       "8230  17499   0.459720\n",
       "8231  17501   0.212075\n",
       "8232  17502   0.278195\n",
       "8233  17505   0.276902\n",
       "8234  17510   0.340757\n",
       "8235  17511   0.325454\n",
       "8236  17513   0.209315\n",
       "8237  17516   0.235686\n",
       "8238  17521   0.316776\n",
       "8239  17523   0.247224\n",
       "8240  17525   0.203133\n",
       "8241  17526   0.190588\n",
       "8242  17527   0.379993\n",
       "8243  17528   0.122575\n",
       "8244  17529   0.290566\n",
       "8245  17531   0.178410\n",
       "8246  17532   0.251240\n",
       "8247  17536   0.099781\n",
       "8248  17537   0.219136\n",
       "8249  17539   0.169437\n",
       "8250  17548   3.728633\n",
       "8251  17550   4.137992\n",
       "8252  17553   4.554413\n",
       "8253  17554   4.770646\n",
       "8254  17556   5.101757\n",
       "8255  17559   5.434134\n",
       "8256  17561   5.748299\n",
       "8257  17566   6.144857\n",
       "8258  17567   6.233299\n",
       "8259  17568   6.470123\n",
       "8260  17571   6.696871\n",
       "8261  17573   7.035668\n",
       "8262  17577   7.541470\n",
       "8263  17579   7.836238\n",
       "8264  17580   7.862036\n",
       "8265  17581   8.034094\n",
       "8266  17582   8.050190\n",
       "8267  17585   8.183679\n",
       "8268  17587   8.635865\n",
       "8269  17590   8.859462\n",
       "8270  17592   8.945020\n",
       "8271  17593   8.877725\n",
       "8272  17595   9.289232\n",
       "8273  17599   9.600449\n",
       "8274  17600   9.596727\n",
       "8275  17601   9.672787\n",
       "8276  17602   9.779665\n",
       "8277  17604   9.645980\n",
       "8278  17606   9.970831\n",
       "8279  17607  10.007892\n",
       "8280  17609  10.171494\n",
       "8281  17610  10.146743\n",
       "8282  17611  10.199361\n",
       "8283  17613  10.325161\n",
       "8284  17615  10.339414\n",
       "8285  17617  10.461532\n",
       "8286  17619  10.548253\n",
       "8287  17620  10.593583\n",
       "8288  17621  10.648318\n",
       "8289  17623  10.653581\n",
       "8290  17627  10.645057\n",
       "8291  17628  10.624866\n",
       "8292  17634  10.776289\n",
       "8293  17636  10.956407\n",
       "8294  17637  11.007053\n",
       "8295  17639  10.940367\n",
       "8296  17641  10.887003\n",
       "8297  17642  10.847633\n",
       "8298  17650  10.825756\n",
       "8299  17651  10.861789\n",
       "8300  17653  10.846521\n",
       "8301  17655  10.802960\n",
       "8302  17661  10.658774\n",
       "8303  17663  10.580125\n",
       "8304  17664  10.445233\n",
       "8305  17665  10.488573\n",
       "8306  17666  10.354250\n",
       "8307  17667  10.109492\n",
       "8308  17669  10.216032\n",
       "8309  17670  10.222556\n",
       "8310  17671  10.116428\n",
       "8311  17673  10.074059\n",
       "8312  17674   9.864921\n",
       "8313  17676   9.627611\n",
       "8314  17677   9.612117\n",
       "8315  17678   9.587343\n",
       "8316  17681   9.480658\n",
       "8317  17682   9.351024\n",
       "8318  17685   9.055952\n",
       "8319  17689   8.645376\n",
       "8320  17691   8.541692\n",
       "8321  17692   8.353376\n",
       "8322  17693   8.310928\n",
       "8323  17696   7.905647\n",
       "8324  17698   7.970684\n",
       "8325  17701   7.394595\n",
       "8326  17702   7.118932\n",
       "8327  17704   6.865413\n",
       "8328  17706   6.542598\n",
       "8329  17707   6.336975\n",
       "8330  17709   6.049479\n",
       "8331  17711   5.931909\n",
       "8332  17713   5.662599\n",
       "8333  17717   4.987594\n",
       "8334  17721   4.409462\n",
       "8335  17722   4.316736\n",
       "8336  17725   3.835152\n",
       "8337  17727   3.634722\n",
       "8338  17731   3.083607\n",
       "8339  17734   2.486251\n",
       "8340  17737   2.031687\n",
       "8341  17738   1.969037\n",
       "8342  17739   1.836407\n",
       "8343  17743   1.443910\n",
       "8344  17747   3.384284\n",
       "8345  17748   3.505647\n",
       "8346  17749   3.634318\n",
       "8347  17750   3.695816\n",
       "8348  17752   3.940835\n",
       "8349  17753   4.138375\n",
       "8350  17755   4.331132\n",
       "8351  17762   5.314666\n",
       "8352  17763   5.444903\n",
       "8353  17771   6.274039\n",
       "8354  17775   6.925983\n",
       "8355  17777   7.161341\n",
       "8356  17778   7.139262\n",
       "8357  17779   7.331574\n",
       "8358  17780   7.415808\n",
       "8359  17781   7.547741\n",
       "8360  17783   7.759238\n",
       "8361  17784   7.977719\n",
       "8362  17788   8.195721\n",
       "8363  17789   8.261116\n",
       "8364  17791   8.557284\n",
       "8365  17793   8.748883\n",
       "8366  17794   8.862028\n",
       "8367  17795   8.904456\n",
       "8368  17798   9.026064\n",
       "8369  17799   8.987906\n",
       "8370  17800   9.014552\n",
       "8371  17801   9.294529\n",
       "8372  17802   9.461701\n",
       "8373  17803   9.482594\n",
       "8374  17805   9.483438\n",
       "8375  17812   9.719305\n",
       "8376  17813   9.676747\n",
       "8377  17815   9.651715\n",
       "8378  17817   9.807773\n",
       "8379  17818  10.015129\n",
       "8380  17820   9.993725\n",
       "8381  17822  10.173583\n",
       "8382  17824  10.339922\n",
       "8383  17825  10.211312\n",
       "8384  17832  10.197531\n",
       "8385  17833  10.425416\n",
       "8386  17835  10.264581\n",
       "8387  17838  10.422482\n",
       "8388  17839  10.428012\n",
       "8389  17840  10.532358\n",
       "8390  17844  10.238992\n",
       "8391  17845  10.513745\n",
       "8392  17850   8.129732\n",
       "8393  17852   7.943417\n",
       "8394  17853   8.910582\n",
       "8395  17855   8.508670\n",
       "8396  17856   7.704780\n",
       "8397  17857   8.167097\n",
       "8398  17859   9.796024\n",
       "8399  17861  10.383762\n",
       "8400  17863  10.143166\n",
       "8401  17864   8.866513\n",
       "8402  17865   9.873160\n",
       "8403  17868   8.248922\n",
       "8404  17869   8.223208\n",
       "8405  17870  10.046501\n",
       "8406  17871   9.897205\n",
       "8407  17872   9.820846\n",
       "8408  17875   9.122500\n",
       "\n",
       "[8409 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = result1[result1['ID'].isin(special_missing_ID)].index\n",
    "result1.loc[index, '发电量'] = 0.379993053\n",
    "\n",
    "result1.to_csv('sub_linxi_081602.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
